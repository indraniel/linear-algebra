% Chapter 1, Topic from _Linear Algebra_ Jim Hefferon
%  http://joshua.smcvt.edu/linearalgebra
%  2001-Jun-09
\topic{Accuracy of Computations}
\index{accuracy! of Gauss's Method|(}
\index{Gauss's Method! accuracy|(}
Gauss's Method lends itself to computerization.
The code below illustrates.
It operates on an $\nbyn{n}$ matrix named~\lstinline[style=inline]!a!, 
doing row combinations using the first row, then
the second row, etc.
\begin{lstlisting}
for(row=1; row<=n-1; row++){
  for(row_below=row+1; row_below<=n; row_below++){
    multiplier=a[row_below,row]/a[row,row];
    for(col=row; col<=n; col++){
      a[row_below,col]-=multiplier*a[row,col];
    }
  }
}
\end{lstlisting}
This is in the C~language.\index{C language}
The 
\lstinline[style=inline]!for(row=1; row<=n-1; row++){ .. }!
loop initializes \lstinline[style=inline]!row! at~$1$ and then iterates while
\lstinline[style=inline]!row! is less than or equal to $n-1$, each time through
incrementing \lstinline[style=inline]!row! by one with the 
\lstinline[style=inline]!++! operation.
The other non-obvious language 
construct is that the \lstinline[style=inline]!-=! in the innermost
loop has the effect of
\lstinline[style=inline]!a[row_below,col]=-1*multiplier*a[row,col]+a[row_below,col]!.

While that code is a first take on mechanizing Gauss's Method,
it is naive.
For one thing, it assumes that 
the entry in the
\lstinline[style=inline]!row,row! position is nonzero.
So one way that it needs to be extended is to cover
the case where finding a zero in that location leads to a row swap or to the
conclusion that the matrix is singular.

We could add some \lstinline[style=inline]!if! statements to cover those cases 
but we will instead consider another way in which this code is naive.
It is prone to pitfalls arising from the computer's reliance on 
floating point arithmetic.

For example, above we have seen that we must handle a singular system as
a separate case.
But systems that are nearly singular also require care.
Consider this one (the extra digits are in the ninth significant place).
\begin{equation*}
   \begin{linsys}{2}
                   x &+ &2y &= &3\hfill\hbox{}  \\
     1.000\,000\,01x &+ &2y &= &3.000\,000\,01
   \end{linsys}
   \tag{$*$}
\end{equation*}
By eye we easily spot the solution $x=1$, $y=1$.
A computer has more trouble.
If it represents real numbers to eight significant places, called 
\definend{single precision}\index{single precision}, 
then it will represent the second
equation internally as $1.000\,000\,0x+2y=3.000\,000\,0$, losing the
digits in the ninth place.
Instead of reporting the correct solution, this computer will think that 
the two equations are equal and it will report that the system is 
singular.

For some intuition about how the computer could come up with 
something that far off, consider this graph of the system.
\begin{center}
  \includegraphics{ch1.33}
\end{center}
We cannot tell the two lines apart;
this system is nearly singular in the sense that
the two lines are nearly the same line.
This gives the system~($*$) the property that a small change in an
equation can cause a large change in the solution.
For instance, changing the $3.000\,000\,01$ to $3.000\,000\,03$ 
changes the intersection point from $(1,1)$ to $(3,0)$.
The solution
changes radically depending on the ninth digit, which explains why
an eight-place computer has trouble.
A problem that is very sensitive to inaccuracy or uncertainties in
the input values is \definend{ill-conditioned}.\index{ill-conditioned problem}

The above example gives one way in which a system can be
difficult to solve on a computer.
It has the advantage that the picture of nearly-equal lines gives a memorable 
insight into one way for numerical difficulties to happen.
Unfortunately this insight isn't useful when we wish
to solve some large system.
We typically will not understand the geometry of an arbitrary large
system.

There are other ways that a computer's results may be
unreliable, besides that the angle between some
of the linear surfaces is small.
For example, consider this system (from \cite{Hamming}).
\begin{equation*}
  \begin{linsys}{2}
     0.001x  &+  &y  &=  &1  \\
          x  &-  &y  &=  &0
  \end{linsys}
\tag*{($**$)}\end{equation*}
The second equation
gives $x=y$, so $x=y=1/1.001$ and 
thus both variables have values that are just less than $1$.
A computer using two digits represents the system internally in this way
(we will do this example in two-digit floating point 
arithmetic for clarity but inventing a similar one with 
eight or more digits is easy).
\begin{equation*}
  \begin{linsys}{2}
    (1.0\times 10^{-3})\cdot x  &+  &(1.0\times 10^{0})\cdot y  &=  &1.0\times 10^{0}  \\
    (1.0\times 10^{0})\cdot x   &-  &(1.0\times 10^{0})\cdot y  &=  &0.0\times 10^{0}
  \end{linsys}
\end{equation*}
The row reduction step $-1000\rho_1+\rho_2$ produces 
a second equation $-1001y=-1000$, which this computer rounds to two places as 
$(-1.0\times 10^{3})y=-1.0\times 10^{3}$.
The computer decides from the second equation that $y=1$ 
and with that it concludes from the first equation that $x=0$.
The~$y$ value is close but the~$x$ is bad\Dash
the ratio of the actual answer to the computer's answer is 
infinite.\index{accuracy!rounding error}
In short, another cause of 
unreliable output is the computer's reliance on floating point arithmetic
when the system-solving code leads to using leading entries that are small. 

An experienced programmer may respond by using
\definend{double precision},\index{double precision} %
which retains sixteen significant digits, 
or perhaps using some even larger size.
This will indeed solve many problems.
However, double precision has greater memory requirements
and besides we can obviously tweak the above to give the
same trouble in the seventeenth digit, so double precision
isn't a panacea.
We need a strategy to minimize numerical
trouble
as well as some guidance about how far we can trust the reported 
solutions.

A basic improvement on the naive code above 
is to not determine the factor to use for row combinations
by simply taking the entry in the \lstinline[style=inline]!row,row! position,
but rather to look at all of the entries in the \lstinline[style=inline]!row!
column below the \lstinline[style=inline]!row,row! entry
and take one that is likely to give reliable results
because it is not too small.
This is \definend{partial pivoting}.%
\index{partial pivoting}\index{pivoting!full} 

For example, to solve the troublesome system~($**$) above
we start by looking at both equations for a best entry to use, 
and take the~$1$ in
the second equation as more likely to give good results.
The combination step of $-.001\rho_2+\rho_1$ gives a first equation of 
$1.001y=1$, which the computer will represent as 
$(1.0\times 10^{0})y=1.0\times 10^{0}$, leading to the conclusion that 
$y=1$ and, after back-substitution, that $x=1$, 
both of which are close to right.  
We can adapt the code from above to do this.
\begin{lstlisting}
for(row=1; row<=n-1; row++){
/* find the largest entry in this column (in row max) */
  max=row;
  for(row_below=row+1; row_below<=n; row_below++){
    if (abs(a[row_below,row]) > abs(a[max,row]));
      max = row_below;
  }
/* swap rows to move that best entry up */
  for(col=row; col<=n; col++){
    temp=a[row,col];
    a[row,col]=a[max,col];
    a[max,col]=temp;
  }
/* proceed as before */
  for(row_below=row+1; row_below<=n; row_below++){
    multiplier=a[row_below,row]/a[row,row];
     for(col=row; col<=n; col++){
       a[row_below,col]-=multiplier*a[row,col];
    }
  }
}
\end{lstlisting}

A full analysis of the best way to implement Gauss's Method 
is beyond the scope of this book (see \cite{Wilkinson65}),
but the method recommended by most experts 
first finds the best entry
among the candidates and then scales it to a number that is less
likely to give trouble.
This is 
\definend{scaled partial pivoting}.\index{scaled partial pivoting}\index{pivoting!partial!scaled}

In addition to returning a result that is likely to be reliable,
most well-done code will return a 
\definend{conditioning number}%
\index{conditioning number}\index{matrix!conditioning number}
that describes the factor by which uncertainties in the input numbers
could be magnified to become inaccuracies in the results returned 
(see \cite{Rice}).

The lesson is that
just because Gauss's Method always works in theory, and just
because computer code correctly implements that method,
doesn't mean that the answer is reliable.
In practice, always use a package
where experts have worked hard to counter what can go wrong.

\begin{exercises}
  \item 
    Using two decimal places, add $253$ and $2/3$.
    \begin{answer}
      Scientific notation because is convenient for expressing 
      the two-place restriction:
      $.25\times 10^{2}+.67\times 10^{0}=.25\times 10^{2}$.
      Notice that the $2/3$ has no effect.
    \end{answer}
  \item 
    This intersect-the-lines problem contrasts with the example
    discussed above.
    \begin{center}
      \vcenteredhbox{\includegraphics{ch1.34}}
      \qquad
      $\displaystyle \begin{linsys}{2}
            x &+ &2y &= &3  \\
            3x &- &2y &= &1
      \end{linsys}$
    \end{center}
    Illustrate that in this system 
    some small change in the numbers will produce only a
    small change in the solution by changing the constant in the
    bottom equation to $1.008$ and solving.
    Compare it to the solution of the unchanged system.
    \begin{answer}
      The reduction
      \begin{equation*}
        \grstep{-3\rho_1+\rho_2}
        \begin{linsys}{2}
          x  &+  &2y   &=  &3  \\
             &   &-8y  &=  &-7.992
        \end{linsys}
      \end{equation*}
      gives \( (x,y)=(1.002,0.999) \).
      So for this system a small change in the constant produces only a small
      change in the solution.
    \end{answer}
  \item 
    Consider this system (\cite{Rice}).
    \begin{equation*}
      \begin{linsys}{2}
        0.000\,3x  &+  &1.556y  &=  &1.569 \\
        0.345\,4x  &-  &2.346y  &=  &1.018
      \end{linsys}
    \end{equation*}
    \begin{exparts*}
      \partsitem Solve it.
      \partsitem Solve it by
         rounding at each step to four digits. 
    \end{exparts*}
    \begin{answer}
      \begin{exparts}
        \partsitem The fully accurate solution is that $x=10$ and $y=0$.
        \partsitem The four-digit reduction
          \begin{equation*}
            \grstep{-(.3454/.0003)\rho_1+\rho_2}
            \begin{amat}{2}
              .0003  &1.556  &1.569  \\
              0      &1789   &-1805
            \end{amat}
          \end{equation*}
          gives the conclusion that $x=10460$ and~$y=-1.009$. 
          Of course, this is wildly different than the correct answer.
      \end{exparts}
    \end{answer}
  \item 
    Rounding inside the computer often has an effect on the result.
    Assume that your machine has eight significant digits.
    \begin{exparts}
      \partsitem Show that the machine will compute 
         $(2/3)+((2/3)-(1/3))$ as unequal to $((2/3)+(2/3))-(1/3)$.
         Thus, computer arithmetic is not associative.
      \partsitem Compare the computer's version of $(1/3)x+y=0$
        and $(2/3)x+2y=0$.
        Is twice the first equation the same as the second?
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem For the first one, first, $(2/3)-(1/3)$ is 
          $.666\,666\,67-.333\,333\,33=.333\,333\,34$
          and so 
          $(2/3)+((2/3)-(1/3))=.666\,666\,67+.333\,333\,34=1.000\,000\,0$.

          For the other one, first 
          $((2/3)+(2/3))=.666\,666\,67+.666\,666\,67=1.333\,333\,3$
          and so 
          $((2/3)+(2/3))-(1/3)=1.333\,333\,3-.333\,333\,33=.999\,999\,97$.
        \partsitem The first equation is 
          $.333\,333\,33\cdot x+1.000\,000\,0\cdot y=0$
          while the second is 
          $.666\,666\,67\cdot x+2.000\,000\,0\cdot y=0$.
      \end{exparts}
    \end{answer}
  \item 
    Ill-conditioning is not only dependent on the matrix of
    coefficients.
    This example \cite{Hamming} shows that it can arise from an
    interaction between the left and right sides of the system.
    Let $\varepsilon$ be a small real.
    \begin{equation*}
      \begin{linsys}{3}
        3x  &+  &2y           &+  &z            &=  &6\hfill   \\
        2x  &+  &2\varepsilon y  &+  &2\varepsilon z  
                                          &=  &2+4\varepsilon\hfill \\
         x  &+  &2\varepsilon y  &-  &\varepsilon z   
                                          &=  &1+\varepsilon\hfill
      \end{linsys}
    \end{equation*}
    \begin{exparts}
      \partsitem Solve the system by hand.
        Notice that the $\varepsilon$'s divide out only because there is
        an exact cancellation of the integer parts on the right side
        as well as on the left. 
      \partsitem Solve the system by hand, rounding to two decimal
        places, and with $\varepsilon=0.001$.
%      \partsitem If you have access to a computer package for solving
%        linear systems, see if you can find an $\varepsilon$ small enough
%        to get your package to give incorrect results.
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem This calculation
          \begin{align*}
            &\grstep[-(1/3)\rho_1+\rho_3]{-(2/3)\rho_1+\rho_2}
            \begin{amat}{3}
              3  &2                   &1                   &6               \\
              0  &-(4/3)+2\varepsilon &-(2/3)+2\varepsilon &-2+4\varepsilon \\
              0  &-(2/3)+2\varepsilon &-(1/3)-\varepsilon  &-1+\varepsilon 
            \end{amat}                                                    \\
            &\grstep{-(1/2)\rho_2+\rho_3}
            \begin{amat}{3}
              3  &2                   &1                   &6               \\
              0  &-(4/3)+2\varepsilon &-(2/3)+2\varepsilon &-2+4\varepsilon \\
              0  &\varepsilon         &-2\varepsilon       &-\varepsilon 
            \end{amat}
          \end{align*}
          gives a third equation of $y-2z=-1$.
          Substituting into the second equation gives 
          $((-10/3)+6\varepsilon)\cdot z=(-10/3)+6\varepsilon$ 
          so $z=1$ and thus $y=1$.
          With those, the first equation says that $x=1$. 
        \partsitem 
          As above, scientific notation is convenient to express the
          restriction on the numbe of digits.

          The solution with two digits retained
          is $z=2.1$, $y=2.6$, and $x=-.43$.
          \begin{multline*}
            \begin{amat}{3}
              .30\times 10^{1}  &.20\times 10^{1}  &.10\times 10^{1} 
                 &.60\times 10^{1}        \\
              .10\times 10^{1}  &.20\times 10^{-3} &.20\times 10^{-3} 
                 &.20\times 10^{1}        \\
              .30\times 10^{1}  &.20\times 10^{-3}  &-.10\times 10^{-3} 
                 &.10\times 10^{1}        
            \end{amat}                                           \\
            \begin{aligned}
            &\grstep[-(1/3)\rho_1+\rho_3]{-(2/3)\rho_1+\rho_2}
            \begin{amat}{3}
              .30\times 10^{1}  &.20\times 10^{1}  &.10\times 10^{1} 
                 &.60\times 10^{1}        \\
              0                 &-.13\times 10^{1} &-.67\times 10^{0} 
                 &-.20\times 10^{1}        \\
              0                 &-.67\times 10^{0}  &-.33\times 10^{0} 
                 &-.10\times 10^{1}        
            \end{amat}                                          \\
            &\grstep{-(.67/1.3)\rho_2+\rho_3}
            \begin{amat}{3}
              .30\times 10^{1}  &.20\times 10^{1}  &.10\times 10^{1} 
                 &.60\times 10^{1}        \\
              0                 &-.13\times 10^{1} &-.67\times 10^{0} 
                 &-.20\times 10^{1}        \\
              0                 &0                  &.15\times 10^{-2} 
                 &.31\times 10^{-2}        
            \end{amat}
            \end{aligned}
          \end{multline*}
      \end{exparts}
    \end{answer}
\end{exercises}
\index{accuracy! of Gauss's Method|)}
\index{Gauss's Method! accuracy|)}

\endinput




