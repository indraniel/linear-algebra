% Chapter 2, Section 1 _Linear Algebra_ Jim Hefferon
%  http://joshua.smcvt.edu/linearalgebra
%  2001-Jun-10
\chapter{Vector Spaces}
The first chapter finished with a fair
understanding
of how Gauss's Method solves a linear system.
It systematically takes linear combinations of the rows.
Here we move to a general study of linear 
combinations.

We need a setting.
At times in the first chapter we've combined vectors from $\Re^2$, 
at other times vectors from $\Re^3$,
and at other times vectors from higher-dimensional spaces. 
So our first impulse might be 
to work in $\Re^n$, leaving $n$ unspecified.
This would have the advantage that any of the results 
would hold for $\Re^2$ and for $\Re^3$ and for many other spaces,
simultaneously.

But if having the results apply to many spaces at once is
advantageous then sticking only to $\Re^n$'s is overly restrictive. 
We'd like our results to apply to combinations of row vectors,
as in the final section of the first chapter.
We've even seen some spaces that are not simply a collection of all of the
same-sized column vectors or row vectors.
For instance, we've seen a homogeneous system's solution
set that is a plane inside of $\Re^3$.
This set is a closed system in that 
a linear combination of these solutions is also a solution. 
But it does not contain all of the three-tall column vectors, 
only some of them.

We want the results about linear combinations to apply anywhere that linear
combinations make sense. 
We shall call any such set a \definend{vector space}.
Our results, instead of being phrased as
``Whenever we have a collection in which we can sensibly take linear 
combinations \ldots'', will be stated 
``In any vector space \ldots''

Such a statement describes at once what
happens in many spaces.
To understand the advantages of moving from studying a single space 
to studying a class of spaces, consider this analogy.
Imagine that the government made laws one person at a time:
``Leslie Jones can't jay walk.''
That would be bad; 
statements have the virtue of economy when they apply to many cases at once.
Or suppose that they said, ``Kim Ke must stop when passing 
an accident.''
Contrast that with, ``Any doctor must stop when passing 
an accident.''
More general statements, in some ways, are clearer.













\section{Definition of Vector Space}
\index{vector space|(}
We shall study structures with two operations,
an addition and a scalar multiplication, that are subject to some
simple conditions.
We will reflect more on the conditions later
but on first reading notice how reasonable they are.
For instance, surely any operation that can be called an addition
(e.g., column vector addition, row vector addition, or
real number addition) will satisfy conditions (1) through~(5) below.



\vspace*{1ex}
\subsection{Definition and Examples}

\begin{definition}
\label{def:VecSpace}
%<*df:VectorSpace>
A \definend{vector space}\index{vector space!definition}
(over \( \Re \)) consists of a set \( V \) along with
two operations 
`+'\index{vector!sum}\index{sum!vector}\index{addition of vectors} 
and `\( \cdot \)'\index{scalar multiple!vector}\index{vector!scalar multiple} 
subject to the conditions
that for all vectors \( \vec{v},\vec{w},\vec{u}\in V \)
and all \definend{scalars}\index{scalar}
\( r,s\in\Re \):
% \begin{tfae}
\begin{compactenum} 
\item the set $V$ is closed under
  vector addition, that is, 
  \( \vec{v}+\vec{w}\in V \)
\item vector addition is commutative,
  \( \vec{v}+\vec{w}=\vec{w}+\vec{v} \) 
\item vector addition is associative,
  \( (\vec{v}+\vec{w})+\vec{u}=\vec{v}+(\vec{w}+\vec{u}) \)
\item there is a \definend{zero vector}\index{zero vector}
    \( \zero\in V \) such that
    \( \vec{v}+\zero=\vec{v}\, \) for all \( \vec{v}\in V\/ \)
\item each \( \vec{v}\in V \) has an
    \definend{additive inverse}\index{additive inverse}\index{inverse!additive}
    \( \vec{w}\in V \) such that \( \vec{w}+\vec{v}=\zero \)
\item  the set $V$ is closed under
    scalar multiplication, that is, 
   \( r\cdot\vec{v}\in V \)
\item addition of scalars distributes over scalar multiplication,
 \( (r+s)\cdot\vec{v}=r\cdot\vec{v}+s\cdot\vec{v} \)
\item scalar multiplication distributes over vector addition,
  \( r\cdot(\vec{v}+\vec{w})=r\cdot\vec{v}+r\cdot\vec{w} \)
\item ordinary multipication of scalars associates with 
  scalar multiplication, \( (rs)\cdot\vec{v} =r\cdot(s\cdot\vec{v}) \)
\item multiplication by the scalar~$1$ is the 
  identity operation, \( 1\cdot\vec{v}=\vec{v} \).
\end{compactenum}
% \end{tfae}  
%</df:VectorSpace>
\end{definition}

\vspace{-4.5ex}  % why is the extra space there?  Bug in shading code?
\begin{remark}
The definition involves two kinds of addition and two kinds of multiplication,
and so may at first seem confused.
For instance, in condition~(7)
the `$+$' on the left is addition of two real numbers
while the `$+$' on the right is addition of two vectors in
\( V\/ \).
These expressions aren't ambiguous because of context; for example,
\( r \) and \( s \)
are real numbers so `\( r+s \)' can only mean real number addition.
In the same way, item~(9)'s left side `$rs$' is ordinary real number 
multiplication, while its right side `$s\cdot\vec{v}$' is 
the scalar multipliction defined for this
vector space.
\end{remark}

\vspace{-0.5ex}  % why is the extra space there?  Bug in shading code?
The best way to understand the definition is to
go through the examples below and for each,
check all ten conditions.
The first example includes that check, written out at length.
Use it as a model for the others.
Especially important are the \definend{closure}\index{vector space!closure}
conditions, (1) and~(6).
They specify that the addition and scalar multiplication operations
are always sensible\Dash they are defined for every pair of vectors 
and every scalar and vector,
and the result of the operation is a member of the set
(see \nearbyexample{PlaneThruOriginSubsp}). 

\begin{example}  \label{ex:RealVecSpaces}
The set
\( \Re^2 \) is a vector space if the operations `\( + \)' and `\( \cdot \)'
have their usual meaning.
\begin{equation*}
  \colvec{x_1 \\ x_2}
  +
  \colvec{y_1 \\ y_2}
  =
  \colvec{x_1+y_1 \\ x_2+y_2}
  \qquad
  r\cdot
  \colvec{x_1 \\ x_2}
  =
  \colvec{rx_1 \\ rx_2}
\end{equation*}
We shall check all of the conditions.

There are five conditions in the paragraph having to do with addition.
For (1), closure of addition, observe that for any \( v_1,v_2,w_1,w_2\in\Re \)
the result of the vector sum
\begin{equation*}
  \colvec{v_1 \\ v_2}
  +\colvec{w_1 \\ w_2}
  =\colvec{v_1+w_1 \\ v_2+w_2}
\end{equation*}
is a column array with two real entries, and so is in \( \Re^2 \).
For (2), that addition of vectors commutes, 
take all entries to be real numbers and compute
\begin{equation*}
  \colvec{v_1 \\ v_2}
  +\colvec{w_1 \\ w_2}
  =\colvec{v_1+w_1 \\ v_2+w_2}
  =\colvec{w_1+v_1 \\ w_2+v_2}
  =\colvec{w_1 \\ w_2}
  +\colvec{v_1 \\ v_2}
\end{equation*}
(the second equality follows from the fact that the components of the
vectors are real numbers, and the addition of real numbers is commutative).
Condition~(3), associativity of vector addition, is similar.
\begin{align*}
  (\colvec{v_1 \\ v_2}
  +\colvec{w_1 \\ w_2})
  +\colvec{u_1 \\ u_2}
  &=\colvec{(v_1+w_1)+u_1 \\ (v_2+w_2)+u_2}  \\
  &=\colvec{v_1+(w_1+u_1) \\ v_2+(w_2+u_2)}  \\
  &=\colvec{v_1 \\ v_2}
  +(\colvec{w_1 \\ w_2}
  +\colvec{u_1 \\ u_2})
\end{align*}
For the fourth condition we must produce a zero element\Dash the
vector of zeroes is it.
\begin{equation*}
  \colvec{v_1 \\ v_2}
  +\colvec{0 \\ 0}
  =\colvec{v_1 \\ v_2}
\end{equation*}
For (5), to produce an additive inverse, note that for any $v_1,v_2\in\Re$
we have
\begin{equation*}
  \colvec{-v_1 \\ -v_2}
  +\colvec{v_1 \\ v_2}
  =\colvec{0 \\ 0}
\end{equation*}
so the first vector is the desired additive inverse of the second.

The checks for the five conditions having to do with scalar multiplication
are similar.
For (6), closure under scalar multiplication,
where $r, v_1, v_2 \in \Re$,
\begin{equation*}
  r\cdot\colvec{v_1 \\ v_2}
  =\colvec{rv_1 \\ rv_2}
\end{equation*}
is a column array with two real entries, and so is in \( \Re^2 \).
Next, this checks (7).
\begin{equation*}
  (r+s)\cdot\colvec{v_1 \\ v_2}
  =\colvec{(r+s)v_1 \\ (r+s)v_2}
  =\colvec{rv_1+sv_1 \\ rv_2+sv_2}
  =r\cdot\colvec{v_1 \\ v_2}+s\cdot\colvec{v_1 \\ v_2}
\end{equation*}
For (8), 
that scalar multiplication distributes from the left over
vector addition, we have this.
\begin{equation*}
  r\cdot(\colvec{v_1 \\ v_2}+\colvec{w_1 \\ w_2})
  =\colvec{r(v_1+w_1) \\ r(v_2+w_2)}
  =\colvec{rv_1+rw_1 \\ rv_2+rw_2}
  =r\cdot\colvec{v_1 \\ v_2}+r\cdot\colvec{w_1 \\ w_2}
\end{equation*}
The ninth
\begin{equation*}
  (rs)\cdot\colvec{v_1 \\ v_2}
  =\colvec{(rs)v_1 \\ (rs)v_2}
  =\colvec{r(sv_1) \\ r(sv_2)}
  =r\cdot(s\cdot\colvec{v_1 \\ v_2})
\end{equation*}
and tenth conditions are also straightforward.
\begin{equation*}
  1\cdot\colvec{v_1 \\ v_2}
  =\colvec{1v_1 \\ 1v_2}
  =\colvec{v_1 \\ v_2}
\end{equation*}
\end{example}

In a similar way, 
each \( \Re^n \) is a vector space with the usual operations of vector addition
and scalar multiplication.
(In \( \Re^1 \), we usually do not write the members as
column vectors, i.e., we usually do not write `\( (\pi) \)'.
Instead we just write `\( \pi \)'.)

\begin{example}  \label{PlaneThruOriginSubsp}
This subset of \( \Re^3 \) that is a plane through the origin
\begin{equation*}
  P=\set{ \colvec{x \\ y \\ z}  \suchthat x+y+z=0}
\end{equation*}
is a vector space if `+' and `\(\cdot\)' are interpreted in this way.
\begin{equation*}
  \colvec{x_1 \\ y_1 \\ z_1}
  +
  \colvec{x_2 \\ y_2 \\ z_2}
  =
  \colvec{x_1+x_2 \\ y_1+y_2 \\ z_1+z_2}
  \qquad
  r\cdot
  \colvec{x \\ y \\ z}
  =
  \colvec{rx \\ ry \\ rz}
\end{equation*}
The addition and scalar multiplication operations here
are just the ones of \( \Re^3 \), reused on its subset $P$.
We say that \( P \) \definend{inherits\/}\index{inherited operations} 
these operations from \( \Re^3 \).
This example of an addition in $P$
\begin{equation*}
   \colvec[r]{1 \\ 1 \\ -2}+\colvec[r]{-1 \\ 0 \\ 1}=\colvec[r]{0 \\ 1 \\ -1}
\end{equation*}
illustrates that $P$ is closed under addition.
We've added two vectors from $P$\Dash that is, with the property that the sum 
of their three entries is zero\Dash and the result is a vector also in $P$.
Of course, this example is not a proof.
For the proof that $P$ is closed under addition, take two elements of $P$. 
\begin{equation*}
  \colvec{x_1 \\ y_1 \\ z_1} \quad \colvec{x_2 \\ y_2 \\ z_2} 
\end{equation*} 
Membership in $P$ means that $x_1+y_1+z_1=0$ and $x_2+y_2+z_2=0$. 
Observe that their sum
\begin{equation*}
  \colvec{x_1+x_2 \\ y_1+y_2 \\ z_1+z_2}
\end{equation*} 
is also in $P$ since its entries add
$(x_1+x_2)+(y_1+y_2)+(z_1+z_2)=(x_1+y_1+z_1)+(x_2+y_2+z_2)$ to $0$.
To show that \( P \) is closed under scalar multiplication, start with
a vector from $P$
\begin{equation*}
  \colvec{x \\ y \\ z}
\end{equation*}
where \( x+y+z=0 \), and then for \( r\in\Re \)
observe that the scalar multiple
\begin{equation*}
  r\cdot\colvec{x \\ y \\ z}
  =
  \colvec{rx \\ ry \\ rz}
\end{equation*}
gives \( rx+ry+rz=r(x+y+z)=0 \).
Thus the two closure conditions are satisfied.
Verification of the other conditions in the definition of a vector space
are just as straightforward.
\end{example}

\begin{example} \label{ex:ColsIntEntNotVS}
\nearbyexample{ex:RealVecSpaces} shows that the set of all two-tall vectors
with real entries is a vector space. 
\nearbyexample{PlaneThruOriginSubsp} gives a subset of an $\Re^n$ that is also
a vector space.
In contrast with those two, consider the set
of two-tall columns with entries that are integers
(under the usual operations of component-wise 
addition and scalar multiplication).
This is a subset of a vector space but it is not itself a vector space.
The reason is that this set is not closed under scalar multiplication, 
that is, it does not satisfy condition~(6). 
Here is a column with integer entries and a scalar 
such that the outcome of the operation 
\begin{equation*}
  0.5
  \cdot
  \colvec[r]{4 \\ 3}
  =
  \colvec[r]{2 \\ 1.5}
\end{equation*}
is not a member of the set, since its entries are not all integers.
\end{example}

\begin{example}  \label{ex:TrivSbspReFour}
The singleton set
\begin{equation*}
  \{ \colvec[r]{0 \\ 0 \\ 0 \\ 0} \}
\end{equation*}
is a vector space under the operations 
\begin{equation*}
  \colvec[r]{0 \\ 0 \\ 0 \\ 0}
  +
  \colvec[r]{0 \\ 0 \\ 0 \\ 0}
  =
  \colvec[r]{0 \\ 0 \\ 0 \\ 0}
  \qquad
  r\cdot
  \colvec[r]{0 \\ 0 \\ 0 \\ 0}
  =
  \colvec[r]{0 \\ 0 \\ 0 \\ 0}
\end{equation*}
that it inherits from \( \Re^4 \).
\end{example}

A vector space must have at least one element, its zero vector.
Thus a one-element vector space is the smallest possible.

\begin{definition} \label{df:TrivialVectorSpace}
%<*df:TrivialVectorSpace>
A one-element vector space is a \definend{trivial}\index{vector space!trivial}%
\index{trivial space}
space.
%</df:TrivialVectorSpace>
\end{definition}

The examples so far involve sets of column vectors with the usual operations.
But vector spaces need not be collections of column vectors, or even of row
vectors.
Below are some other types of vector spaces.
The term `vector space' does not mean `collection of columns of reals'.
It means something more like 
`collection in which any linear combination is sensible'.

\begin{example} \label{ex:PolySpaceThree}
Consider
\( \polyspace_3=\set{a_0+a_1x+a_2x^2+a_3x^3\suchthat a_0,\ldots,a_3\in\Re} \),
the set of polynomials of degree three or less
(in this book, we'll take constant polynomials, 
including the zero polynomial, to be of degree zero).
It is a vector space under the operations
\begin{multline*}
   (a_0+a_1x+a_2x^2+a_3x^3)+(b_0+b_1x+b_2x^2+b_3x^3)  \\
     =(a_0+b_0)+(a_1+b_1)x+(a_2+b_2)x^2+(a_3+b_3)x^3
\end{multline*}
and
\begin{equation*}
   r\cdot(a_0+a_1x+a_2x^2+a_3x^3)=
     (ra_0)+(ra_1)x+(ra_2)x^2+(ra_3)x^3
\end{equation*}
(the verification is easy).
This vector space is worthy of attention because
these are the polynomial operations familiar from high school algebra.
For instance,
$
  3\cdot(1-2x+3x^2-4x^3)-2\cdot(2-3x+x^2-(1/2)x^3)=-1+7x^2-11x^3$.

Although this space is not a subset of any \( \Re^n \),
there is a sense in which we can think of $\polyspace_3$ as ``the same'' as  
\( \Re^4 \).
If we identify these two space's elements in this way 
\begin{equation*}
  a_0+a_1x+a_2x^2+a_3x^3
  \quad\text{corresponds to}\quad
  \colvec{a_0 \\ a_1 \\ a_2 \\ a_3}
\end{equation*}
then the operations also correspond.
Here is an example of corresponding additions.
\begin{equation*}
  \mbox{\begin{tabular}{lr}
       &\(1-2x+0x^2+1x^3\) \\
     + &\(2+3x+7x^2-4x^3\) \\ \hline
       &\(3+1x+7x^2-3x^3\)
  \end{tabular}  }
  \quad\text{corresponds to}\quad
  \colvec[r]{1 \\ -2 \\ 0 \\ 1}
  +
  \colvec[r]{2 \\ 3 \\ 7 \\ -4}
  =
  \colvec[r]{3 \\ 1 \\ 7 \\ -3}
\end{equation*}
Things we are thinking of as ``the same'' add to ``the same'' sum.
Chapter Three makes precise this idea of vector space correspondence.
For now we shall just leave it as an intuition.
\end{example}

\begin{example} \label{ex:MatSpaceTwoByTwo}
The set \( \matspace_{\nbym{2}{2}} \) of \( \nbym{2}{2} \) matrices with
real number entries is a vector space under the natural 
entry-by-entry operations.
\begin{equation*}
  \begin{mat}
    a  &b \\
    c  &d
  \end{mat}
  +
  \begin{mat}
    w  &x \\
    y  &z
  \end{mat}
  =
  \begin{mat}
    a+w  &b+x \\
    c+y  &d+z
  \end{mat}
  \qquad
  r\cdot
  \begin{mat}
    a  &b \\
    c  &d
  \end{mat}
  =
  \begin{mat}
    ra  &rb \\
    rc  &rd
  \end{mat}
\end{equation*}
As in the prior example, we can think of this space as
``the same'' as \( \Re^4 \).
\end{example}

\begin{example}   \label{ex:FcnsNToRIsVecSp}
The set \( \set{f\suchthat \map{f}{\N}{\Re} } \) of all
real-valued functions of one natural number variable is a vector space
under the operations
\begin{equation*}
  (f_1+f_2)\,(n)=f_1(n)+f_2(n)
  \qquad
  (r\cdot f)\,(n)=r\,f(n)
\end{equation*}
so that if, for example, \( f_1(n)=n^2+2\sin(n) \) and
\( f_2(n)=-\sin(n)+0.5 \)
then \( (f_1+2f_2)\,(n)=n^2+1 \).

We can view this space
as a generalization of \nearbyexample{ex:RealVecSpaces}\Dash instead of 
$2$-tall vectors, these functions are like infinitely-tall
vectors.
\begin{equation*}
  \text{
    \begin{tabular}{c|c}
      \( n \)      &\( f(n)=n^2+1 \)  \\ \hline
      \( 0      \) &\( 1      \)    \\
      \( 1      \) &\( 2      \)    \\
      \( 2      \) &\( 5      \)    \\
      $3$          &$10$            \\
      \( \vdots \) &\( \vdots \)    
    \end{tabular} }
    \quad\text{corresponds to}\quad
    \colvec[r]{
           1           \\
           2           \\
           5           \\
           10          \\
           \vdotswithin{10}      }
\end{equation*}
Addition and scalar multiplication are component-wise, 
as in \nearbyexample{ex:RealVecSpaces}.
(We can formalize ``infinitely-tall'' by saying that it means an infinite
sequence, or that it means a function from $\N$ to $\Re$.)
\end{example}

\begin{example} \label{ex:PolysOfAllFiniteDegrees}
The set of polynomials with real coefficients
\begin{equation*}
 \set{ a_0+a_1x+\cdots+a_nx^n\suchthat n\in\N
    \text{ and } a_0,\ldots,a_n\in\Re} 
\end{equation*}
makes a vector space when given the natural `$+$' 
\begin{multline*}
  (a_0+a_1x+\cdots+a_nx^n)+(b_0+b_1x+\cdots+b_nx^n)  \\
     =(a_0+b_0)+(a_1+b_1)x+\cdots +(a_n+b_n)x^n
\end{multline*}
and `$\cdot$'.
\begin{equation*}
  r\cdot (a_0+a_1x+\ldots a_nx^n)
   =
  (ra_0)+(ra_1)x+\ldots (ra_n)x^n
\end{equation*}
This space differs from the space $\polyspace_3$ of
\nearbyexample{ex:PolySpaceThree}.
This space contains not just degree~three polynomials, 
but degree~thirty polynomials and
degree three~hundred polynomials, too.
Each individual polynomial of course is of a finite degree, 
but the set has no single bound on the degree of all of its members.

We can think of this example, like the prior one,  
in terms of infinite-tuples.
For instance, we can think of \( 1+3x+5x^2 \) as corresponding to
\( (1,3,5,0,0,\ldots) \).
However, this space differs from the one in
\nearbyexample{ex:FcnsNToRIsVecSp}.
Here, each member of the set has a finite degree, that is,
under the correspondence there is no element from this space 
matching \( (1,2,5,10,\,\ldots\,) \).
Vectors in this space correspond to infinite-tuples
that end in zeroes.
\end{example}

\begin{example}  \label{ex:RealValuedFcns}
The set
\( \set{f\suchthat \map{f}{\Re}{\Re} } \)
of all real-valued functions of one real variable
is a vector space under these.
\begin{equation*}
  (f_1+f_2)\,(x)=f_1(x)+f_2(x)
  \qquad
  (r\cdot f)\,(x)=r\,f(x)
\end{equation*}
The difference between this and \nearbyexample{ex:FcnsNToRIsVecSp} is the
domain of the functions.
\end{example}

\begin{example}\label{ex:ACos+BSin}
The set
\( F=\{ a\cos\theta+b\sin\theta \suchthat a,b\in\Re\} \)
of real-valued functions of the real variable \( \theta \)
is a vector space under the operations
%\nearbyexample{ex:RealValuedFcns}:
\begin{equation*}
  (a_1\cos\theta+b_1\sin\theta)+(a_2\cos\theta+b_2\sin\theta)
    =(a_1+a_2)\cos\theta+(b_1+b_2)\sin\theta
\end{equation*}
and
\begin{equation*}
  r\cdot (a\cos\theta+b\sin\theta)
   =(ra)\cos\theta+(rb)\sin\theta
\end{equation*}
inherited from the space in the prior example.
(We can think of \( F \) as ``the same'' as \( \Re^2 \)
in that $a\cos\theta+b\sin\theta$ corresponds to the vector with
components $a$ and $b$.)
\end{example}

\begin{example}  \label{ex:SpaceSatisfyingDiffQ}
The set
\begin{equation*}
  \set{\map{f}{\Re}{\Re}\suchthat \dfrac{d^2f}{dx^2}+f=0}
\end{equation*}
is a vector space under the, by now natural, interpretation.
\begin{equation*}
  (f+g)\,(x)=f(x)+g(x)
  \qquad
  (r\cdot f)\,(x)=r\,f(x)
\end{equation*}
In particular, notice that closure is a consequence
\begin{equation*}
   \frac{d^2(f+g)}{dx^2}+(f+g)
   =(\frac{d^2f}{dx^2}+f)+(\frac{d^2g}{dx^2}+g)
\end{equation*}
and
\begin{equation*}
   \frac{d^2(rf)}{dx^2}+(rf)
   =r(\frac{d^2 f}{dx^2}+f)
\end{equation*}
of basic Calculus.
This turns out to equal the space from the prior example\Dash functions
satisfying this differential equation have the form
$a\cos\theta+b\sin\theta$\Dash but this description 
suggests an extension to solutions sets of other
differential equations.
\end{example}

\begin{example} \label{ex:HomoSlnMakesVS}
The set of solutions of a homogeneous linear system in \( n \) variables
is a vector space under the operations inherited from \( \Re^n \).
For example, for closure under addition 
consider a typical equation in that system
$c_1x_1+\cdots+c_nx_n=0$ and suppose that both these vectors
\begin{equation*}
   \vec{v}=\colvec{v_1 \\ \vdotswithin{v_1} \\ v_n}
   \qquad
   \vec{w}=\colvec{w_1 \\ \vdotswithin{w_1} \\ w_n}
\end{equation*}
satisfy the equation. 
Then their sum
\( \vec{v}+\vec{w}\/ \) also satisfies that equation:
\(
  c_1(v_1+w_1)+\cdots+c_n(v_n+w_n)
  =(c_1v_1+\cdots+c_nv_n)+(c_1w_1+\cdots+c_nw_n)
  =0
\).
The checks of the other vector space conditions are just as routine.
\end{example}

We often omit the multiplication symbol `\( \cdot \)' between the 
scalar and the vector. 
We distinguish the multiplication in
\( c_1v_1 \) from that in \( r\vec{v}\, \) by context, since if both 
multiplicands are real numbers then it must be 
real-real multiplication while if one is a vector then it must be
scalar-vector multiplication.

\nearbyexample{ex:HomoSlnMakesVS} has brought us full circle since it is one of
our motivating examples.
Now, with some feel for the kinds of structures that satisfy the definition
of a vector space, we can reflect on that definition.
For example, why specify in the definition the condition that 
\( 1\cdot\vec{v}=\vec{v} \) but not a condition that \( 0\cdot\vec{v}=\zero \)?

One answer is that this is just a definition\Dash it gives the rules 
and you need to follow those rules to continue.

Another answer is perhaps more satisfying.
People in this area have worked to develop the
right balance of power and generality.
This definition is shaped so that it contains the conditions
needed to prove all of the interesting and
important properties of spaces of linear combinations.
As we proceed, we shall derive all of the properties natural to collections of
linear combinations from the conditions given in the definition.

The next result is an example.
We do not need to include these properties in the definition of vector space
because they follow from the properties already listed there.

\begin{lemma} \label{lm:ElementaryPropertiesOfVectorSpaces}
%<*lm:ElementaryPropertiesOfVectorSpaces>
In any vector space \( V \), 
for any \( \vec{v}\in V \) and \( r\in\Re \), we have
(1)~\( 0\cdot\vec{v}=\zero \), 
(2)~\( (-1\cdot\vec{v})+\vec{v}=\zero \), and 
(3)~\( r\cdot\zero=\zero \).
%</lm:ElementaryPropertiesOfVectorSpaces>
\end{lemma}

\begin{proof}
%<*pf:ElementaryPropertiesOfVectorSpaces0>
For (1) note that 
\( \vec{v}=(1+0)\cdot\vec{v}=\vec{v}+(0\cdot\vec{v}) \).
Add to both sides the additive inverse of \( \vec{v} \),
the vector \( \vec{w} \) such that \( \vec{w}+\vec{v}=\zero \).
\begin{align*}
  \vec{w}+\vec{v}
  &=\vec{w}+\vec{v}+0\cdot\vec{v}  \\
  \zero
  &=\zero+0\cdot\vec{v}                   \\
  \zero
  &=0\cdot\vec{v}
\end{align*}
Item~(2) is easy:
\(  (-1\cdot\vec{v})+\vec{v}=(-1+1)\cdot\vec{v}=0\cdot\vec{v}=\zero \).
For (3),
\( r\cdot\zero
  =
  r\cdot(0\cdot\zero)
  =
  (r\cdot 0)\cdot\zero
  =
  \zero \)
will do.
%</pf:ElementaryPropertiesOfVectorSpaces0>
\end{proof}

The second item
shows that we can write the additive inverse
of \( \vec{v} \) as `\( -\vec{v}\, \)' without worrying about any 
confusion with \( (-1)\cdot\vec{v} \).

\medskip
A recap:
our study in Chapter One of Gaussian reduction
led us to consider collections of linear combinations.
So in this chapter we have defined a vector space to be a 
structure in which we can form such combinations,
% expressions of the form \( c_1\cdot\vec{v}_1+\dots+c_n\cdot\vec{v}_n \)
subject to simple conditions on the addition and scalar
multiplication operations.
In a phrase:~vector spaces are
the right context in which to study linearity.

% Finally, a comment.
From the fact that it forms a whole chapter, and especially because that 
chapter is the first one, a reader could suppose that our purpose
in this book is the study of linear systems.
The truth is that we will not so much use vector spaces in
the study  of linear systems as we instead have linear systems 
start us on the study of vector spaces.
The wide variety of examples from this subsection shows that the study of
vector spaces is interesting and important in its own right.
Linear systems won't go away.
But from now on our primary objects of study will be vector spaces.

\begin{exercises}
  \item 
    Name the zero vector for each of these vector spaces.
    \begin{exparts}
      \partsitem The space of degree three polynomials under the natural
        operations.
      \partsitem The space of \( \nbym{2}{4} \) matrices.
      \partsitem The space
        \( \set{\map{f}{[0..1]}{\Re}\suchthat f\text{ is continuous}} \).
      \partsitem The space of real-valued functions of one natural 
        number variable.
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem \( 0+0x+0x^2+0x^3 \)
        \partsitem \( \begin{mat}[r]
                   0  &0  &0  &0  \\
                   0  &0  &0  &0
                 \end{mat} \)
        \partsitem The constant function \( f(x)=0 \)
        \partsitem The constant function \( f(n)=0 \)
      \end{exparts}  
    \end{answer}
  \recommended \item
    Find the additive inverse, in the vector space,
    of the vector.
    \begin{exparts}
      \partsitem In \( \polyspace_3 \), the vector \( -3-2x+x^2 \).
      \partsitem In the space \( \nbyn{2} \),
        \begin{equation*}
          \begin{mat}[r]
            1  &-1  \\
            0  &3
          \end{mat}.
        \end{equation*}
     \partsitem In \( \set{ae^x+be^{-x}\suchthat a,b\in\Re} \), the space 
       of functions of the real variable \( x \) under the natural operations,
       the vector \( 3e^x-2e^{-x} \).
    \end{exparts}
    \begin{answer}
      \begin{exparts*}
        \partsitem \( 3+2x-x^2 \)
        \partsitem \( \begin{mat}[r]
                   -1  &+1  \\
                    0  &-3
                 \end{mat} \)
        \partsitem \( -3e^x+2e^{-x} \)
      \end{exparts*}  
    \end{answer}
  \recommended \item 
    For each, list three elements and then show it is a vector space.
    \begin{exparts}
      \partsitem The set of linear polynomials
        \( \polyspace_1=\set{a_0+a_1x\suchthat a_0,a_1\in\Re} \) under the
        usual polynomial addition and scalar multiplication operations.
      \partsitem The set of linear polynomials
        \( \set{a_0+a_1x\suchthat a_0-2a_1=0} \), under the
        usual polynomial addition and scalar multiplication operations.
   \end{exparts}
   \textit{Hint.}  Use \nearbyexample{ex:RealVecSpaces} as a guide.
   Most of the ten conditions are just verifications. 
   \begin{answer}
      \begin{exparts}
        \partsitem 
          Three elements are: 
          $1+2x$, $2-1x$, and $x$.
          (Of course, many answers are possible.)

          The verification is just like \nearbyexample{ex:RealVecSpaces}. 
          We first do 
          conditions $1$-$5$ from 
          \nearbydefinition{def:VecSpace}, having to 
          do with addition.
          For closure under addition, condition~(1), 
          note that where $a+bx,c+dx\in\polyspace_1$
          we have that $(a+bx)+(c+dx)=(a+c)+(b+d)x$ is a linear polynomial
          with real coefficients and so is an element of 
          $\polyspace_1$. 
          Condition~(2) is verified with: where $a+bx,c+dx\in\polyspace_1$ then 
          $(a+bx)+(c+dx)=(a+c)+(b+d)x$, while in the other order they are
          $(c+dx)+(a+bx)=(c+a)+(d+b)x$, and both $a+c=c+a$ 
          and $b+d=d+b$ as these are real numbers.
          Condition~(3) is similar: suppose
          $a+bx,c+dx,e+fx\in\polyspace$ then
          $((a+bx)+(c+dx))+(e+fx)=(a+c+e)+(b+d+f)x$
          while
          $(a+bx)+((c+dx)+(e+fx))=(a+c+e)+(b+d+f)x$, and the two are equal
          (that is, real number addition is associative so $(a+c)+e=a+(c+e)$
          and $(b+d)+f=b+(d+f)$).
          For condition~(4) observe that the linear polynomial
          $0+0x\in\polyspace_1$ has the 
          property that $(a+bx)+(0+0x)=a+bx$ and 
          $(0+0x)+(a+bx)=a+bx$.
          For the last condition in this paragraph, condition~(5),
          note that for any $a+bx\in\polyspace_1$ the additive inverse
          is $-a-bx\in\polyspace_1$ since
          $(a+bx)+(-a-bx)=(-a-bx)+(a+bx)=0+0x$.
         
          We next also check conditions~(6)-(10), involving 
          scalar multiplication.
          For (6), the condition that the space be closed under scalar
          multiplication, 
          suppose that $r$ is a real number and $a+bx$ is an element of
          $\polyspace_1$, and then $r(a+bx)=(ra)+(rb)x$ is an element of 
          $\polyspace_1$ because it is a linear polynomial with real
          number coefficients.
          Condition~(7) holds because 
          $(r+s)(a+bx)=r(a+bx)+s(a+bx)$ is true from the distributive property
          for real number multiplication.
          Condition~(8) is similar: 
          $r((a+bx)+(c+dx))=r((a+c)+(b+d)x)=r(a+c)+r(b+d)x=(ra+rc)+(rb+rd)x
          =r(a+bx)+r(c+dx)$.
          For~(9) we have 
          $(rs)(a+bx)=(rsa)+(rsb)x=r(sa+sbx)=r(s(a+bx))$.
          Finally, condition~(10) is 
          $1(a+bx)=(1a)+(1b)x=a+bx$.
        \partsitem 
          Call the set $P$.
          In the prior item in this exercise 
          there was no restriction on the coefficients
          but here we are restricting attention to those linear polynomials
          where $a_0-2a_1=0$, that is, 
          where the constant term minus twice the coefficient of the linear
          term is zero. 
          Thus, three typical elements of $P$ are
          $2+1x$, $6+3x$, and $-4-2x$.

          For condition~(1) we must show that
          if we add two linear polynomials that satisfy the restriction then
          we get a linear polynomial also satisfying the restriction: here that
          argument is that
          if $a+bx,c+dx\in P$ then
          $(a+bx)+(c+dx)=(a+c)+(b+d)x$ is an element of $P$ because
          $(a+c)-2(b+d)=(a-2b)+(c-2d)=0+0=0$. 
          We can verify condition~(2) with: 
          where $a+bx,c+dx\in\polyspace_1$ then 
          $(a+bx)+(c+dx)=(a+c)+(b+d)x$, while in the other order they are
          $(c+dx)+(a+bx)=(c+a)+(d+b)x$, and both $a+c=c+a$ 
          and $b+d=d+b$ as these are real numbers. 
          (That is, this condition
          is not affected by the restriction and the verification is the same 
          as the verification in the first item of this exercise).
          Condition~(3) is also not affected by the extra restriction: 
          suppose that
          $a+bx,c+dx,e+fx\in\polyspace$ then
          $((a+bx)+(c+dx))+(e+fx)=(a+c+e)+(b+d+f)x$
          while
          $(a+bx)+((c+dx)+(e+fx))=(a+c+e)+(b+d+f)x$, and the two are equal.
          For condition~(4) observe that the linear polynomial
          satisfies the restriction $0+0x\in P$ because
          its constant term minus twice the coefficient of its linear term
          is zero, and then the verification from the first item of this 
          question applies:
          $0+0x\in\polyspace_1$ has the 
          property that $(a+bx)+(0+0x)=a+bx$ and 
          $(0+0x)+(a+bx)=a+bx$.
          To check condition~(5),
          note that for any $a+bx\in P$ the additive inverse
          is $-a-bx$ since
          it is an element of $P$ (because $a+bx\in P$ we know that 
          $a-2b=0$ and multiplying both sides by $-1$ gives that $-a+2b=0$), 
          and as in the first item it acts as the additive inverse
          $(a+bx)+(-a-bx)=(-a-bx)+(a+bx)=0+0x$.
         
          We must also check conditions~(6)-(10), those for 
          scalar multiplication.
          For (6), the condition that the space be closed under scalar
          multiplication, 
          suppose that $r$ is a real number and $a+bx\in P$ (so that $a-2b=0$),
          then $r(a+bx)=(ra)+(rb)x$ is an element of 
          $P$ because it is a linear polynomial with real
          number coefficients satisfying that $(ra)-2(rb)=r(a-2b)=0$.
          Condition~(7) holds for the same reason that it holds in 
          the first item of this exercise, because 
          $(r+s)(a+bx)=r(a+bx)+s(a+bx)$ is true from the distributive property
          for real number multiplication.
          Condition~(8) is also unchanged from the first item: 
          $r((a+bx)+(c+dx))=r((a+c)+(b+d)x)=r(a+c)+r(b+d)x=(ra+rc)+(rb+rd)x
          =r(a+bx)+r(c+dx)$.
          So is~(9): 
          $(rs)(a+bx)=(rsa)+(rsb)x=r(sa+sbx)=r(s(a+bx))$.
          Finally, so is condition~(10): 
          $1(a+bx)=(1a)+(1b)x=a+bx$.
       \end{exparts}
     \end{answer}
  \item 
    For each, list three elements and then show it is a vector space.
    \begin{exparts}
      \partsitem The set of \( \nbyn{2} \) matrices with real entries under
        the usual matrix operations.
      \partsitem The set of \( \nbyn{2} \) matrices with real entries where
        the $2,1$ entry is zero,  under
        the usual matrix operations.
    \end{exparts}
    \begin{answer}
      Use
      \nearbyexample{ex:RealVecSpaces} as a guide.
      (\textit{Comment.} 
      Because many of the conditions are quite easy to check,
      sometimes a person can feel that they must have missed
      something.
      Keep in mind that 
      easy to do, or routine, is different from not necessary to do.)
      \begin{exparts}
        \partsitem 
          Here are three elements.
          \begin{equation*}
            \begin{mat}
              1  &2  \\
              3  &4
            \end{mat},\,
            \begin{mat}
              -1  &-2  \\
              -3  &-4
            \end{mat},\,
            \begin{mat}
              0  &0  \\
              0  &0
            \end{mat}
          \end{equation*}

          For~(1), the sum of $\nbyn{2}$ real matrices is a $\nbyn{2}$
          real matrix.
          For~(2) we consider the sum of two matrices 
          \begin{equation*}
            \begin{mat}
              a  &b  \\
              c  &d
            \end{mat}+
            \begin{mat}
              e  &f  \\
              g  &h
            \end{mat}
            =
            \begin{mat}
              a+e  &b+f  \\
              c+g  &d+h
            \end{mat}
          \end{equation*}
          and apply commutativity of real number addition
          \begin{equation*}
            =\begin{mat}
              e+a  &f+b  \\
              g+c  &h+d
            \end{mat}
            =
            \begin{mat}
              e  &f  \\
              g  &h
            \end{mat}
            +
            \begin{mat}
              a  &b  \\
              c  &d
            \end{mat}
          \end{equation*}
          to verify that the addition of the matrices is commutative.
          The verification for condition~(3), 
          associativity of matrix addition, is similar
          to the prior verification:
          \begin{equation*}
            \big(
              \begin{mat}
                a  &b  \\
                c  &d  
              \end{mat}
              +\begin{mat}
                e  &f  \\
                g  &h  
              \end{mat}
            \big)
            +
            \begin{mat}
              i  &j  \\
              k  &l
            \end{mat}
            =
            \begin{mat}
              (a+e)+i  &(b+f)+j  \\
              (c+g)+k  &(d+h)+l
            \end{mat}
          \end{equation*}
          while
          \begin{equation*}
            \begin{mat}
              a  &b  \\
              c  &d  
            \end{mat}
            +
              \big(
              \begin{mat}
                e  &f  \\
                g  &h  
              \end{mat}
              +
              \begin{mat}
                i  &j  \\
                k  &l
              \end{mat}
            \big)
            =
            \begin{mat}
              a+(e+i)  &b+(f+j)  \\
              c+(g+k)  &d+(h+l)
            \end{mat}
          \end{equation*}
          and the two are the same entry-by-entry because real number addition
          is associative.
          For~(4), the zero element of this space is the $\nbyn{2}$ 
          matrix of zeroes.
          Condition~(5) holds because for any $\nbyn{2}$ matrix~$A$
          the additive inverse is the matrix whose entries are the negative of
          $A$'s, the matrix $-1\cdot A$.

          Condition~($6$) 
          holds because a scalar multiple of a $\nbyn{2}$ matrix 
          is a $\nbyn{2}$ matrix.
          For condition~(7) we have this.
          \begin{multline*}
            (r+s)
            \begin{mat}
              a  &b  \\
              c  &d
            \end{mat}
            =
            \begin{mat}
              (r+s)a  &(r+s)b  \\
              (r+s)c  &(r+s)d
            \end{mat}               \\
            =
            \begin{mat}
              ra+sa  &rb+sb  \\
              rc+sc  &rd+sd
            \end{mat}
            =
            r
            \begin{mat}
              a  &b  \\
              c  &d 
            \end{mat}
            +
            s
            \begin{mat}
              a  &b  \\
              c  &d 
            \end{mat}
          \end{multline*}
          Condition~(8) goes the same way.
          \begin{multline*}
            r
            \big(
              \begin{mat}
                a  &b  \\
                c  &d
              \end{mat}
              +
              \begin{mat}
                e  &f  \\
                g  &h
              \end{mat}
            \big)
            =
            r
            \begin{mat}
              a+e  &b+f  \\
              c+g  &d+h
            \end{mat}
            =
            \begin{mat}
              ra+re  &rb+rf  \\
              rc+rg  &rd+rh
            \end{mat}              \\
            =
            r
            \begin{mat}
              a  &b  \\
              c  &d 
            \end{mat}
            +
            r
            \begin{mat}
              e  &f  \\
              g  &h 
            \end{mat}
            =
            r
            \big(
            \begin{mat}
              a  &b  \\
              c  &d
            \end{mat}
            +
            \begin{mat}
              e  &f  \\
              g  &h
            \end{mat}
            \big)
          \end{multline*}
          For~(9) we have this.
          \begin{equation*}
            (rs)
            \begin{mat}
              a  &b  \\
              c  &d
            \end{mat}
            =
            \begin{mat}
              rsa  &rsb  \\
              rsc  &rsd
            \end{mat}
            =
            r
            \begin{mat}
              sa  &sb  \\
              sc  &sd
            \end{mat}
            =
            r\big( s
            \begin{mat}
              a  &b  \\
              c  &d
            \end{mat}
            \big)
          \end{equation*}
          Condition~(10) is just as easy.
          \begin{equation*}
            1
            \begin{mat}
              a  &b  \\
              c  &d
            \end{mat}
            =
            \begin{mat}
              1\cdot a  &1\cdot b  \\
              1\cdot c  &1\cdot d
            \end{mat}
            =
            \begin{mat}
              sa  &sb  \\
              sc  &sd
            \end{mat}
          \end{equation*}
        \partsitem 
          This differs from the prior item in this exercise only in that
          we are restricting to the set $T$ of matrices with a zero in the
          second row and first column. 
          Here are three elements of $T$.
          \begin{equation*}
            \begin{mat}
              1  &2  \\
              0  &4
            \end{mat},\,
            \begin{mat}
              -1  &-2  \\
              0  &-4
            \end{mat},\,
            \begin{mat}
              0  &0  \\
              0  &0
            \end{mat}
          \end{equation*}
          Some of the verifications for this item are the same as for the 
          first item in this exercise, and below we'll just do the ones that
          are different. 

          For~(1), the sum of $\nbyn{2}$ real matrices with a zero in the
          $2,1$ entry is also a $\nbyn{2}$ real matrix with a zero in the
          $2,1$ entry.
          \begin{equation*}
            \begin{mat}
              a  &b  \\
              0  &d
            \end{mat}
            +
            \begin{mat}
              e  &f  \\
              0  &h
            \end{mat}
            \begin{mat}
              a+e  &b+f  \\
              0    &d+h
            \end{mat}
          \end{equation*}
          The verification for condition~(2) given in the prior item
          works in this item also.
          The same holds for condition~(3).
          For~(4), note that the $\nbyn{2}$ 
          matrix of zeroes is an element of $T$.
          Condition~(5) holds because for any $\nbyn{2}$ matrix~$A$
          the additive inverse is the matrix 
          $-1\cdot A$ and so the additive inverse of a 
          matrix with a zero in the $2,1$ entry is also a matrix with a zero
          in the $2,1$ entry.

          Condition~$6$ holds because a scalar multiple of a $\nbyn{2}$ matrix 
          with a zero in the $2,1$ entry is 
          a $\nbyn{2}$ matrix with a zero in the $2,1$ entry.
          Condition~(7)'s verification is the same as in the prior item.
          So are condition~(8)'s, (9)'s, and~(10)'s.
      \end{exparts}        
    \end{answer}
  \recommended \item 
    For each, list three elements and then show it is a vector space.
    \begin{exparts}
      \partsitem The set of three-component row vectors with their usual
        operations.
      \partsitem The set
        \begin{equation*}
          \set{\colvec{x \\ y \\ z \\ w}\in\Re^4\suchthat x+y-z+w=0}
        \end{equation*}
        under the operations inherited from $\Re^4$.
    \end{exparts}
    \begin{answer}
      % Most of the conditions are easy to check; use
      % \nearbyexample{ex:RealVecSpaces} as a guide.
      \begin{exparts}
        \partsitem 
          Three elements are $\rowvec{1  &2  &3}$, 
          $\rowvec{2  &1  &3}$, and $\rowvec{0  &0  &0}$.

          We must check conditions (1)-(10) in \nearbydefinition{def:VecSpace}.
          Conditions (1)-(5) concern addition.
          For condition~(1) recall that the sum of two three-component 
          row vectors 
          \begin{equation*}
            \rowvec{a  &b  &c}
            +\rowvec{d  &e  &f}
            =\rowvec{a+d  &b+e  &c+f}
          \end{equation*}
          is also a three-component row vector
         (all of the letters $a,\ldots,f$ represent real numbers).
         Verification of~(2) is routine
          \begin{multline*}
            \rowvec{a  &b  &c}
            +\rowvec{d  &e  &f}
            =\rowvec{a+d  &b+e  &c+f}   \\
            =\rowvec{d+a  &e+b  &f+c}
            =\rowvec{d  &e  &f}
            +\rowvec{a  &b  &c}
          \end{multline*}
          (the second equality holds because the three entries are real
          numbers and real number addition commutes).
         Condition~(3)'s verification is similar.
          \begin{multline*}
            \big(\rowvec{a  &b  &c}
              +\rowvec{d  &e  &f}
            \big)
            +\rowvec{g  &h  &i}
            =\rowvec{(a+d)+g  &(b+e)+h  &(c+f)+i}    \\
            =\rowvec{a+(d+g)  &b+(e+h)  &c+(f+i)}
            =\rowvec{a  &b  &c}
            +\big(\rowvec{d  &e  &f}
               +\rowvec{g  &h  &i}
             \big) 
          \end{multline*}
          For (4), observe that the three-component row vector
          $\rowvec{0  &0  &0}$ is the additive identity:
          $\rowvec{a  &b  &c}+\rowvec{0  &0  &0}=\rowvec{a  &b  &c}$.
          To verify condition~(5), assume we are given the 
          element $\rowvec{a  &b  &c}$ of the set and note that
          $\rowvec{-a  &-b  &-c}$ is also in the set and has the
          desired property: 
          $\rowvec{a  &b  &c}+\rowvec{-a  &-b  &-c}=\rowvec{0  &0  &0}$.

          Conditions (6)-(10) involve scalar multiplication.
          To verify~(6), that the space is closed under the 
          scalar multiplication operation that was given, 
          note that $r\rowvec{a  &b  &c}=\rowvec{ra  &rb  &rc}$ is a 
          three-component row vector with real entries.
          For~(7) we compute.
          \begin{multline*}
            (r+s)\rowvec{a  &b  &c}=\rowvec{(r+s)a  &(r+s)b  &(r+s)c}
            =\rowvec{ra+sa  &rb+sb  &rc+sc}                             \\
            =\rowvec{ra  &rb  &rc}+\rowvec{sa  &sb  &sc}
            =r\rowvec{a  &b  &c}+s\rowvec{a  &b  &c}
          \end{multline*}
          Condition~(8) is very similar.
          \begin{multline*} 
            r\big(\rowvec{a  &b  &c}+\rowvec{d  &e  &f}\big)  \\
            \begin{aligned}
            &=r\rowvec{a+d  &b+e  &c+f}
            =\rowvec{r(a+d)  &r(b+e)  &r(c+f)}                     \\
            &=\rowvec{ra+rd  &rb+re  &rc+rf}
            =\rowvec{ra  &rb  &rc}+\rowvec{rd  &re  &rf}          \\
            &=r\rowvec{a  &b  &c}+r\rowvec{d  &e  &f}
            \end{aligned}
          \end{multline*}
          So is the computation for condition~(9).
          \begin{equation*}
            (rs)\rowvec{a  &b  &c}
             =\rowvec{rsa  &rsb  &rsc}
             =r\rowvec{sa  &sb  &sc}
             =r\big(s\rowvec{a  &b  &c}\big)
          \end{equation*}
          Condition~(10) is just as routine
          $1\rowvec{a  &b  &c}
             =\rowvec{1\cdot a  &1\cdot b  &1\cdot c}
             =\rowvec{a  &b  &c}$.
        \partsitem 
          Call the set $L$. 
          Closure of addition, condition~(1), involves checking that if the
          summands are members of~$L$ then
          the sum
          \begin{equation*}
            \colvec{a \\ b \\ c \\ d}
            +\colvec{e \\ f \\ g \\ h}
            =
            \colvec{a+e \\ b+f \\ c+g \\ d+h}
          \end{equation*}
          is also a member of \( L \), which is true because it satisfies the 
          criteria for membership in $L$:
          \( (a+e)+(b+f)-(c+g)+(d+h)
          =(a+b-c+d)+(e+f-g+h)=0+0 \).
          The verifications for conditions~(2), (3), and~(5) are similar to the
          ones in the first part of this exercise.
          For condition~(4) note that the vector of zeroes is a member of~$L$
          because its first component plus its second, minus its third,
          and plus its fourth, totals to zero.

          Condition~(6), closure of scalar multiplication, is similar:
          where the vector is an element of~$L$,
          \begin{equation*}
            r\colvec{a  \\ b \\ c \\ d}
            =\colvec{ra  \\  rb  \\  rc  \\  rd}
          \end{equation*}
          is also an element of~$L$ because $ra+rb-rc+rd=r(a+b-c+d)=r\cdot 0=0$.
          The verification for conditions~(7), (8), (9), and~(10) are as in the
          prior item of this exercise.          
     \end{exparts}  
     \end{answer}
  \recommended \item \label{exer:NotVectorSpaces}
    Show that each of these is not a vector space.
    (\textit{Hint.}  Check closure by listing two members of each set
    and trying some operations on them.)     
    \begin{exparts}
      \partsitem Under the operations inherited from \( \Re^3 \), this set
        \begin{equation*}
          \set{\colvec{x \\ y \\ z}\in\Re^3\suchthat x+y+z=1}
        \end{equation*}
      \partsitem Under the operations inherited from \( \Re^3 \), this set
        \begin{equation*}
          \set{\colvec{x \\ y \\ z}\in\Re^3\suchthat x^2+y^2+z^2=1}
        \end{equation*}
      \partsitem Under the usual matrix operations,
        \begin{equation*}
          \set{\begin{mat}
                 a  &1  \\
                 b  &c
               \end{mat} \suchthat a,b,c\in\Re}
        \end{equation*}
      \partsitem Under the usual polynomial operations,
        \begin{equation*}
          \set{a_0+a_1x+a_2x^2\suchthat a_0,a_1,a_2\in\Re^+}
        \end{equation*}
        where $\Re^+$ is the set of reals greater than zero
      \partsitem Under the inherited operations,
        \begin{equation*}
          \set{\colvec{x \\ y}\in\Re^2\suchthat
               \text{\( x+3y=4 \) and \( 2x-y=3 \) and \( 6x+4y=10 \)} }
        \end{equation*}
    \end{exparts}
    \begin{answer}
      In each item the set is called \( Q \).
      For some items, there are other correct ways to show that $Q$ is not
      a vector space.
      \begin{exparts}
        \partsitem It is not closed under addition; it fails to meet
          condition~(1).
          \begin{equation*}
            \colvec[r]{1 \\ 0 \\ 0},
            \colvec[r]{0 \\ 1 \\ 0}\in Q
            \qquad
            \colvec[r]{1 \\ 1 \\ 0}\not\in Q
          \end{equation*}
        \partsitem It is not closed under addition.
          \begin{equation*}
            \colvec[r]{1 \\ 0 \\ 0},
            \colvec[r]{0 \\ 1 \\ 0}\in Q
            \qquad
            \colvec[r]{1 \\ 1 \\ 0}\not\in Q
          \end{equation*}
        \partsitem It is not closed under addition.
          \begin{equation*}
            \begin{mat}[r]
              0  &1  \\
              0  &0
            \end{mat},
            \,
            \begin{mat}[r]
              1  &1  \\
              0  &0
            \end{mat}\in Q
            \qquad
            \begin{mat}[r]
              1  &2  \\
              0  &0
            \end{mat}\not\in Q
          \end{equation*}
        \partsitem It is not closed under scalar multiplication.
          \begin{equation*}
            1+1x+1x^2\in Q
            \qquad
            -1\cdot(1+1x+1x^2)\not\in Q
          \end{equation*}
        \item It is empty, violating condition~(4).
      \end{exparts}  
    \end{answer}
  \item 
    Define addition and scalar multiplication operations to 
    make the complex numbers a vector space over \( \Re \).
    \begin{answer}
      The usual operations
      \( (v_0+v_1i)+(w_0+w_1i)=(v_0+w_0)+(v_1+w_1)i \) and
      \( r(v_0+v_1i)=(rv_0)+(rv_1)i \) suffice.
      The check is easy.  
    \end{answer}
  \recommended \item
    Is the set of rational numbers a vector space over \( \Re \) under the
    usual addition and scalar multiplication operations?
    \begin{answer}
       No, it is not closed under scalar multiplication since, e.g., 
       \( \pi\cdot (1) \) is not a rational number. 
    \end{answer}
  \item 
    Show that 
    the set of linear combinations of the variables \( x,y,z \) is
    a vector space under the natural addition and scalar multiplication
    operations.
    \begin{answer}
      The natural operations are
      \( (v_1x+v_2y+v_3z)+(w_1x+w_2y+w_3z)=(v_1+w_1)x+(v_2+w_2)y+(v_3+w_3)z \)
      and \( r\cdot(v_1x+v_2y+v_3z)=(rv_1)x+(rv_2)y+(rv_3)z \).
      The check that this is a vector space is easy; use
      \nearbyexample{ex:RealVecSpaces} as a guide.  
    \end{answer}
  \item 
    Prove that 
    this is not a vector space: the set of two-tall column vectors
    with real entries subject to these operations.
    \begin{equation*}
      \colvec{x_1 \\ y_1}
      +\colvec{x_2 \\ y_2}
      =\colvec{x_1-x_2 \\ y_1-y_2}
      \qquad
      r\cdot\colvec{x \\ y}
      =\colvec{rx \\ ry}
    \end{equation*}
    \begin{answer}
      The `\( + \)' operation is not commutative (that is, condition~(2) is 
      not met); producing two members of the
      set witnessing this assertion is easy.
    \end{answer}
  \item 
    Prove or disprove that \( \Re^3 \) is a vector space under these
    operations.
    \begin{exparts}
      \partsitem \( 
               \colvec{x_1 \\ y_1 \\ z_1}
               +\colvec{x_2 \\ y_2 \\ z_2}
               =\colvec{0 \\ 0 \\ 0}
               \quad\text{and}\quad
               r\colvec{x \\ y \\ z}
               =\colvec{rx \\ ry \\ rz} \) 
      \partsitem \( 
               \colvec{x_1 \\ y_1 \\ z_1}
               +\colvec{x_2 \\ y_2 \\ z_2}
               =\colvec{0 \\ 0 \\ 0}   
               \quad\text{and}\quad
               r\colvec{x \\ y \\ z}
               =\colvec{0 \\ 0 \\ 0} \)
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem It is not a vector space.
          \begin{equation*}
            (1+1)\cdot\colvec[r]{1 \\ 0 \\ 0}\neq
            \colvec[r]{1 \\ 0 \\ 0}
            +\colvec[r]{1 \\ 0 \\ 0}
          \end{equation*}
        \partsitem It is not a vector space.
          \begin{equation*}
            1\cdot\colvec[r]{1 \\ 0 \\ 0}\neq\colvec[r]{1 \\ 0 \\ 0}
          \end{equation*}
      \end{exparts}   
    \end{answer}
  \recommended \item
    For each, decide if it is a vector space;
    the intended operations are the natural ones.
    \begin{exparts}
      \partsitem The \definend{diagonal} \( \nbyn{2} \) matrices
        \begin{equation*}
          \set{\begin{mat}
            a  &0  \\
            0  &b
          \end{mat}\suchthat a,b\in\Re}
        \end{equation*}
      \partsitem This set of \( \nbyn{2} \) matrices
        \begin{equation*}
          \set{\begin{mat}
            x    &x+y  \\
            x+y  &y
          \end{mat}\suchthat x,y\in\Re}
        \end{equation*}
      \partsitem This set
        \begin{equation*}
          \set{\colvec{x \\ y \\ z \\ w}\in\Re^4
               \suchthat x+y+w=1}
        \end{equation*}
      \partsitem The set of functions
        \( \set{\map{f}{\Re}{\Re}\suchthat df/dx+2f=0} \)
      \partsitem The set of functions
        \( \set{\map{f}{\Re}{\Re}\suchthat df/dx+2f=1} \)
    \end{exparts}
    \begin{answer}
      For each ``yes'' answer, you must give a check of all the 
      conditions given in the
      definition of a vector space.
      For each ``no'' answer, give a specific example of the failure 
      of one of the
      conditions.
      \begin{exparts}
        \partsitem Yes.
        \partsitem Yes.
        \partsitem No, this set is not closed under the natural addition
          operation.
          The vector of all $1/4$'s is a member of this set 
          but when added to itself the result, the 
          vector of all $1/2$'s, is a nonmember.
        \partsitem Yes.
        \partsitem No, \( f(x)=e^{-2x}+(1/2) \) is in the set but 
           \( 2\cdot f \) is not (that is, condition~(6) fails).
      \end{exparts}  
    \end{answer}
  \recommended \item
    Prove or disprove that this is a vector space: the real-valued functions
    \( f \) of one real variable such that \( f(7)=0 \).
    \begin{answer}
      It is a vector space.
      Most conditions of the definition of vector space are routine; we here
      check only closure.
      For addition,
      \( (f_1+f_2)\,(7)=f_1(7)+f_2(7)=0+0=0 \).
      For scalar multiplication,
      \( (r\cdot f)\,(7)=rf(7)=r0=0 \).  
    \end{answer}
  \recommended \item
    Show that the set \( \Re^+ \) of positive reals
    is a vector space when we interpret `\( x+y \)' to mean
    the product of \( x \) and \( y \) (so that \( 2+3 \) is \( 6 \)),
    and we interpret `\( r\cdot x \)' as the \( r \)-th power of \( x \).
    \begin{answer}
      We check \nearbydefinition{def:VecSpace}.

      First, closure under `\( + \)'
      holds because the product of two positive reals is
      a positive real.
      The second condition is satisfied because real multiplication commutes.
      Similarly, as real multiplication associates, the third checks.
      For the fourth condition, observe that multiplying a number by
      \( 1\in\Re^+ \) won't change the number.
      Fifth, any positive real has a reciprocal that is a positive real.

      The sixth, closure under `\( \cdot \)', 
      holds because any power of a positive real is a
      positive real.
      The seventh condition is just the rule that \( v^{r+s} \) equals
      the product of \( v^r \) and \( v^s \).
      The eight condition says that \( (vw)^r=v^rw^r \).
      The ninth condition asserts that \( (v^r)^s=v^{rs} \).
      The final condition says that \( v^1=v \).  
    \end{answer}
  \item 
      Is \( \set{(x,y)\suchthat x,y\in\Re} \) a vector space under
      these operations?
      \begin{exparts}
        \partsitem \( (x_1,y_1)+(x_2,y_2)=(x_1+x_2,y_1+y_2) \)
        and \( r\cdot (x,y)=(rx,y) \)
        \partsitem \( (x_1,y_1)+(x_2,y_2)=(x_1+x_2,y_1+y_2) \)
        and \( r\cdot (x,y)=(rx,0) \)
      \end{exparts}
      \begin{answer}
        \begin{exparts}
           \partsitem No: \( 1\cdot(0,1)+1\cdot(0,1)\neq (1+1)\cdot(0,1) \).
           \partsitem No; the same calculation as the prior answer shows
              a condition in the definition of a vector space that is 
              violated. 
              Another example of a violation of the conditions for a 
              vector space is that \( 1\cdot (0,1)\neq (0,1) \). 
        \end{exparts}  
      \end{answer}
  \item 
    Prove or disprove that 
    this is a vector space: the set of polynomials of
    degree greater than or equal to two, along with the zero polynomial.
    \begin{answer}
      It is not a vector space since it is not closed under addition, as
      \( (x^2)+(1+x-x^2) \) is not in the set.  
    \end{answer}
  \item 
    At this point ``the same'' is only an
    intuition, but nonetheless for each vector space identify the
    \( k \) for which the space is ``the same'' as \( \Re^k \).
    \begin{exparts}
      \partsitem The \( \nbym{2}{3} \) matrices under the usual operations
      \partsitem The \( \nbym{n}{m} \) matrices (under their usual operations)
      \partsitem This set of \( \nbyn{2} \) matrices
        \begin{equation*}
          \set{\begin{mat}
            a  &0  \\
            b  &c
          \end{mat} \suchthat a,b,c\in\Re}
        \end{equation*}
      \partsitem This set of \( \nbyn{2} \) matrices
        \begin{equation*}
          \set{\begin{mat}
            a  &0  \\
            b  &c
          \end{mat} \suchthat a+b+c=0}
        \end{equation*}
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem \( 6 \)
        \partsitem \( nm \)
        \partsitem \( 3 \)
        \partsitem To see that the answer is \( 2 \), rewrite it as
        \begin{equation*}
          \set{\begin{mat}
            a  &0  \\
            b  &-a-b
          \end{mat} \suchthat a,b\in\Re}
        \end{equation*}
        so that there are two parameters.
      \end{exparts}  
     \end{answer}
  \recommended \item 
    Using \( \vec{+} \) to represent vector addition
    and \( \,\vec{\cdot}\, \) for scalar multiplication,
    restate the definition of vector space.
    \begin{answer}
      {
      \def\plus{\mathbin{\vec{+}}}
      \def\tim{\mathbin{\vec{\cdot}}}
        A \definend{vector space}\index{vector space!definition}
        (over \( \Re \)) consists of a set \( V \) along with
        two operations `\( \plus \)' and `\( \tim \)' subject to these 
        conditions.
        Where \( \vec{v},\vec{w}\in V \), 
        (1)~their \definend{vector sum}
          \( \vec{v}\plus\vec{w} \) is an element of \( V \).
        If \( \vec{u},\vec{v},\vec{w}\in V \) then 
        (2)~\( \vec{v}\plus\vec{w}=\vec{w}\plus\vec{v} \) and 
        (3)~\( (\vec{v}\plus\vec{w})\plus\vec{u}
                 =\vec{v}\plus(\vec{w}\plus\vec{u}) \).
        (4)~There is a \definend{zero vector}
          \( \zero\in V \) such that
          \( \vec{v}\plus\zero=\vec{v}\, \) for all \( \vec{v}\in V\).
        (5)~Each \( \vec{v}\in V \) has an
          \definend{additive inverse}
          \( \vec{w}\in V \) such that \( \vec{w}\plus\vec{v}=\zero \).
        If \( r,s \) are \definend{scalars\/},
        that is, members of \( \Re \)),
        and \( \vec{v},\vec{w}\in V \) then 
        (6)~each
           \definend{scalar multiple}
           \( r\cdot\vec{v} \) is in \( V \).
        If \( r,s\in\Re \) and \( \vec{v},\vec{w}\in V \) then
        (7)~\( (r+ s)\cdot\vec{v}=r\cdot\vec{v}\plus s\cdot\vec{v} \), 
        and (8)~\( r\tim (\vec{v}+\vec{w})
           =r\tim\vec{v}+r\tim\vec{w} \),
        and (9)~\( (rs)\tim \vec{v} =r\tim (s\tim\vec{v}) \),
        and (10)~\( 1\tim \vec{v}=\vec{v} \).
     }
    \end{answer}
  \recommended \item 
    Prove these.
    \begin{exparts}
      \partsitem For any $\vec{v}\in V$, if $\vec{w}\in V$ is an additive 
        inverse of $\vec{v}$, then $\vec{v}$ is an additive inverse of 
        $\vec{w}$.
        So a vector is an additive inverse of any additive inverse of
        itself.
      \partsitem Vector addition left-cancels:~if 
        \( \vec{v},\vec{s},\vec{t}\in V \)
        then \( \vec{v}+\vec{s}=\vec{v}+\vec{t}\, \) implies
        that \( \vec{s}=\vec{t} \).
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem Let \( V \) be a vector space, 
          let \( \vec{v}\in V \), and
          assume that \( \vec{w}\in V \) is an additive inverse of $\vec{v}$
          so that \( \vec{w}+\vec{v}=\zero \).
          Because addition is commutative,
          \( \zero=\vec{w}+\vec{v}=\vec{v}+\vec{w} \),
          so therefore \( \vec{v} \) is also 
          the additive inverse of \( \vec{w} \).
        \partsitem Let \( V \) be a vector space and suppose
          \( \vec{v},\vec{s},\vec{t}\in V \).
          The additive inverse of \( \vec{v} \) is \( -\vec{v} \) so
          \( \vec{v}+\vec{s}=\vec{v}+\vec{t} \) gives that
          \( -\vec{v}+\vec{v}+\vec{s}=-\vec{v}+\vec{v}+\vec{t} \),
          which says that \( \zero+\vec{s}=\zero+\vec{t} \) and so
          \( \vec{s}=\vec{t} \).
      \end{exparts}  
     \end{answer}
  \item 
    The definition of vector spaces does not explicitly say that
    \( \zero+\vec{v}=\vec{v} \) 
    (it instead says that \( \vec{v}+\zero=\vec{v} \)).
    Show that it must nonetheless hold in any vector space.
    \begin{answer}
      Addition is commutative, so in any vector space,
      for any vector \( \vec{v} \) we have that
      \( \vec{v}=\vec{v}+\zero=\zero+\vec{v} \).  
    \end{answer}
  \recommended \item
    Prove or disprove that 
    this is a vector space: the set of all matrices, under
    the usual operations.
    \begin{answer}
      It is not a vector space since addition of two matrices of unequal
      sizes is not defined, and thus the set fails to satisfy the closure
      condition.
    \end{answer}
  \item 
    In a vector space every element has an additive inverse.
    Can some elements have two or more?
    \begin{answer}
      Each element of a vector space has one and only one additive
      inverse.

      For, let \( V \) be a vector space and suppose that \( \vec{v}\in V \).
      If \( \vec{w}_1,\vec{w}_2\in V \) are both additive inverses of
      \( \vec{v} \) then consider \( \vec{w}_1+\vec{v}+\vec{w}_2 \).
      On the one hand, we have that it equals $\vec{w}_1+(\vec{v}+\vec{w}_2)=
      \vec{w}_1+\zero=\vec{w}_1$.
      On the other hand we have that it equals $(\vec{w}_1+\vec{v})+\vec{w}_2=
      \zero+\vec{w}_2=\vec{w}_2$.
      Therefore, $\vec{w}_1=\vec{w}_2$.
    \end{answer}
  \item 
    \begin{exparts}
      \partsitem Prove that every point, line, or plane thru the origin in 
         \( \Re^3 \) is a vector space under the inherited operations.
      \partsitem What if it doesn't contain the origin?
    \end{exparts}
    \begin{answer}
     \begin{exparts}
      \partsitem Every such set has the form
        \( \set{r\cdot\vec{v}+s\cdot\vec{w}\suchthat r,s\in\Re} \)
        where either or both of \( \vec{v},\vec{w} \) may be \( \zero \).
        With the inherited operations, closure of addition
        \( (r_1\vec{v}+s_1\vec{w})+(r_2\vec{v}+s_2\vec{w})
           =(r_1+r_2)\vec{v}+(s_1+s_2)\vec{w} \)
        and scalar multiplication
        \( c(r\vec{v}+s\vec{w})=(cr)\vec{v}+(cs)\vec{w} \)
        are easy.
        The other conditions are also routine.
      \partsitem No such set can be a vector space under the inherited
        operations because it does not have a zero element.
     \end{exparts}  
    \end{answer}
  \recommended \item 
    Using the idea of a vector space we can easily reprove that
    the solution set of a homogeneous linear system has either 
    one element or infinitely many elements. 
    Assume that \( \vec{v}\in V \) is not \( \zero \).
    \begin{exparts}
      \partsitem Prove that \( r\cdot\vec{v}=\zero \) if and only if \( r=0 \).
      \partsitem Prove that \( r_1\cdot\vec{v}=r_2\cdot\vec{v} \) if
      and only if \( r_1=r_2 \).
      \partsitem Prove that any nontrivial vector space is infinite.
      \partsitem Use the fact that a nonempty solution set of a homogeneous
        linear system is a vector space to draw the conclusion.
    \end{exparts}
    \begin{answer}
      Assume that \( \vec{v}\in V \) is not \( \zero \).
      \begin{exparts}
        \partsitem One direction of the if and only if is clear:~if $r=0$
          then $r\cdot\vec{v}=\zero$.
          For the other way, let \( r \) be a nonzero scalar.
          If \( r\vec{v}=\zero \) then
          \( (1/r)\cdot r\vec{v}=(1/r)\cdot \zero \) shows that
          $\vec{v}=\zero$,  contrary to the assumption.
        \partsitem Where \( r_1,r_2 \) are scalars, 
          \( r_1\vec{v}=r_2\vec{v}\, \)
          holds if and only if \( (r_1-r_2)\vec{v}=\zero \).
          By the prior item, then \( r_1-r_2=0 \).
        \partsitem A nontrivial space has a vector 
          \( \vec{v}\neq\zero \).
          Consider the set \( \set{k\cdot\vec{v}\suchthat k\in\Re} \).
          By the prior item this set is infinite.
        \partsitem The solution set is either trivial, or nontrivial.
          In the second case, it is infinite.   
     \end{exparts}  
    \end{answer}
  \item 
    Is this a vector space under the natural operations: the real-valued
    functions of one real variable that are differentiable?
    \begin{answer}
      Yes.
      A theorem of first semester calculus says that a sum of differentiable
      functions is differentiable and that
      \( (f+g)^\prime=f^\prime+g^\prime \), and that 
      a multiple of a differentiable
      function is differentiable and that \( (r\cdot f)^\prime=r\,f^\prime \). 
    \end{answer}
  \item 
    A \definend{vector space over the complex numbers}%
    \index{vector space!complex scalars}%
    \index{complex numbers!vector space over}
    $\C$ has the same definition
    as a vector space over the reals except that scalars are drawn from
    \( \C \) instead of from \( \Re \).
    Show that each of these is a vector space over the complex numbers.
    (Recall how complex numbers add and multiply:
    \( (a_0+a_1i)+(b_0+b_1i)=(a_0+b_0)+(a_1+b_1)i \) and
    \( (a_0+a_1i)(b_0+b_1i)=(a_0b_0-a_1b_1)+(a_0b_1+a_1b_0)i \).)
    \begin{exparts}
      \partsitem The set of degree~two polynomials with complex 
         coefficients
      \partsitem This set
        \begin{equation*}
          \set{\begin{mat}
                 0  &a  \\
                 b  &0
               \end{mat}\suchthat a,b\in\C\text{\ and\ }
                               a+b=0+0i }
        \end{equation*}
    \end{exparts}
    \begin{answer}
      The check is routine.
      Note that `\( 1 \)' is \( 1+0i \) and the zero elements are these.
      \begin{exparts}
        \partsitem \( (0+0i)+(0+0i)x+(0+0i)x^2 \)
        \partsitem \( \begin{mat}
                   0+0i  &0+0i  \\
                   0+0i  &0+0i
                 \end{mat} \)
      \end{exparts}  
    \end{answer}
  \item 
    Name a property shared by all of the \( \Re^n \)'s 
    but not listed as a
    requirement for a vector space.
    \begin{answer}
      Notably absent from the definition of a vector space is a distance
      measure.  
    \end{answer}
  \recommended \item 
    \begin{exparts}
      \partsitem Prove that for any four vectors
        \( \vec{v}_1,\ldots,\vec{v}_4\in V \) we can associate
        their sum in any way without changing the result.
        \begin{multline*}
          ((\vec{v}_1+\vec{v}_2)+\vec{v}_3)+\vec{v}_4
          =(\vec{v}_1+(\vec{v}_2+\vec{v}_3))+\vec{v}_4  
          =(\vec{v}_1+\vec{v}_2)+(\vec{v}_3+\vec{v}_4)  \\
          =\vec{v}_1+((\vec{v}_2+\vec{v}_3)+\vec{v}_4)  
          =\vec{v}_1+(\vec{v}_2+(\vec{v}_3+\vec{v}_4))
        \end{multline*}
        This allows us to write
        `\( \vec{v}_1+\vec{v}_2+\vec{v}_3+\vec{v}_4 \)'
        without ambiguity.
      \partsitem Prove that any two ways of associating a sum of any number of
        vectors give the same sum.
        (\textit{Hint.}  Use induction on the number of vectors.)
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem A small rearrangement does the trick.
          \begin{align*}
            (\vec{v}_1+(\vec{v}_2+\vec{v}_3))+\vec{v}_4
            &=((\vec{v}_1+\vec{v}_2)+\vec{v}_3)+\vec{v}_4  \\
            &=(\vec{v}_1+\vec{v}_2)+(\vec{v}_3+\vec{v}_4)  \\
            &=\vec{v}_1+(\vec{v}_2+(\vec{v}_3+\vec{v}_4))  \\
            &=\vec{v}_1+((\vec{v}_2+\vec{v}_3)+\vec{v}_4)
          \end{align*}
          Each equality above follows from the associativity of three vectors
          that is given as a condition in the definition of a vector space.
          For instance, the second `$=$' applies the rule
          $(\vec{w}_1+\vec{w}_2)+\vec{w}_3=\vec{w}_1+(\vec{w}_2+\vec{w}_3)$
          by taking $\vec{w}_1$ to be $\vec{v}_1+\vec{v}_2$, 
          taking $\vec{w}_2$ to be $\vec{v}_3$, 
          and taking $\vec{w}_3$ to be $\vec{v}_4$. 
        \partsitem The base case for induction is the three vector case.
          This case
          \( \vec{v}_1+(\vec{v}_2+\vec{v}_3)
          =(\vec{v}_1+\vec{v}_2)+\vec{v}_3 \) is one of the conditions in
          the definition of a vector space.

          For the inductive step, assume that any two sums of three vectors,
          any two sums of four vectors,
          \ldots, any two sums of $k$ vectors 
          are equal no matter how we  parenthesize the sums.
          We will show that any sum of \( k+1 \) vectors equals this one
          \( ((\cdots((\vec{v}_1+\vec{v}_2)+\vec{v}_3)+\cdots)+\vec{v}_k)
              +\vec{v}_{k+1} \).

          Any parenthesized sum has an outermost `\( + \)'.
          Assume that it lies between \( \vec{v}_m \) and \( \vec{v}_{m+1} \)
          so the sum looks like this.
          \begin{equation*}
            (\cdots\,\vec{v}_1\cdots\vec{v}_m\,\cdots)
           +(\cdots\,\vec{v}_{m+1}\cdots\vec{v}_{k+1}\,\cdots)
          \end{equation*}
          The second half involves fewer than $k+1$ additions, so
          by the inductive hypothesis we can re-parenthesize it
          so that it reads left to right from the inside out, and in 
          particular, so that its outermost `$+$' occurs right before 
          $\vec{v}_{k+1}$.
          \begin{equation*}
            =(\cdots\,\vec{v}_1\,\cdots\,\vec{v}_m\,\cdots)
             +((\cdots(\vec{v}_{m+1}+\vec{v}_{m+2})+\cdots+\vec{v}_{k})
                 +\vec{v}_{k+1})
          \end{equation*}
          Apply the associativity of the sum of three things
          \begin{equation*}
            =((\,\cdots\, \vec{v}_1\,\cdots\,\vec{v}_m\,\cdots\,)
             +(\,\cdots\,(\vec{v}_{m+1}+\vec{v}_{m+2})+\cdots\,\vec{v}_k))
             +\vec{v}_{k+1}
          \end{equation*}
          and finish by applying the inductive hypothesis inside these 
          outermost parenthesis.
      \end{exparts}  
    \end{answer}
  \item \nearbyexample{ex:ColsIntEntNotVS} gives a subset of $\Re^2$
   that is not a vector space, under the obvious operations, because
   while it is closed under addition, it is not closed under scalar
   multiplication.
   Consider the set of vectors in the plane whose components have the 
   same sign or are~$0$.
   Show that this set is closed under scalar multiplication but not 
   addition.
   \begin{answer}
     Let $\vec{v}$ be a member of $\Re^2$ with components $v_1$ and $v_2$.
     We can abbreviate the condition that both components have the same
     sign or are~$0$ by $v_1v_2\geq 0$.

     To show the set is closed under scalar multiplication, observe that
     the components of $r\vec{v}$ satisfy $(rv_1)(rv_2)=r^2(v_1v_2)$
     and $r^2\geq 0$ so $r^2v_1v_2\geq 0$.

     To show the set is not closed under addition we need only produce one
     example.  
     The vector with components $-1$ and~$0$, when added to the vector
     with components $0$ and~$1$ makes a vector with mixed-sign components
     of $-1$ and~$1$.
   \end{answer}
  % \item 
  %   For any vector space, a subset that is itself a vector space
  %   under the inherited operations
  %   (e.g., a plane through the origin inside of \( \Re^3 \))
  %   is a \definend{subspace}.
  %   \begin{exparts}
  %     \partsitem Show that \( \set{a_0+a_1x+a_2x^2\suchthat a_0+a_1+a_2=0} \)
  %       is a subspace of the vector space of degree~two polynomials.
  %     \partsitem Show that this is a subspace of the \( \nbyn{2} \) matrices.
  %       \begin{equation*}
  %         \set{\begin{mat}
  %                a  &b  \\
  %                c  &0
  %              \end{mat} \suchthat a+b=0}
  %       \end{equation*}
  %     \partsitem Show that a nonempty subset \( S \) of a real vector
  %       space is a
  %       subspace if and only if it is closed under linear combinations of
  %       pairs of vectors: whenever \( c_1,c_2\in\Re \) and
  %       \( \vec{s}_1,\vec{s}_2\in S \) then the combination
  %       \( c_1\vec{v}_1+c_2\vec{v}_2 \) is in \( S \).
  %   \end{exparts}
  %   \begin{answer}
  %     \begin{exparts}
  %       \partsitem We outline the check of the conditions from
  %         \nearbydefinition{def:VecSpace}.

  %         Additive closure holds because if \( a_0+a_1+a_2=0 \)
  %         and \( b_0+b_1+b_2=0 \) then
  %         \begin{equation*}
  %           (a_0+a_1x+a_2x^2)+(b_0+b_1x+b_2x^2)=
  %           (a_0+b_0)+(a_1+b_1)x+(a_2+b_2)x^2
  %         \end{equation*}
  %         is in the set since
  %         \( (a_0+b_0)+(a_1+b_1)+(a_2+b_2)=(a_0+a_1+a_2)+(b_0+b_1+b_2) \)
  %         is zero.
  %         The second through fifth conditions are easy.

  %         Closure under scalar multiplication holds because
  %         if \( a_0+a_1+a_2=0 \) then
  %         \begin{equation*}
  %           r\cdot(a_0+a_1x+a_2x^2)=
  %           (ra_0)+(ra_1)x+(ra_2)x^2
  %         \end{equation*}
  %         is in the set as \( ra_0+ra_1+ra_2=r(a_0+a_1+a_2) \) is zero.
  %         The remaining conditions here are also easy.
  %       \partsitem This is similar to the prior answer.
  %       \partsitem Call the vector space \( V \).
  %         We have two implications: left to right, if \( S \) is a subspace
  %         then it is closed under linear combinations of pairs of vectors and,
  %         right to left, if a nonempty subset is closed under linear
  %         combinations of pairs of vectors then it is a subspace.
  %         The left to right implication is easy; we here sketch the
  %         other one by
  %         assuming \( S \) is nonempty and closed, and checking the conditions
  %         of \nearbydefinition{def:VecSpace}.

  %         First, to show closure under addition, if
  %         \( \vec{s}_1,\vec{s}_2\in S \) then \( \vec{s}_1+\vec{s}_2\in S \)
  %         as \( \vec{s}_1+\vec{s}_2=1\cdot\vec{s}_1+1\cdot\vec{s}_2 \).
  %         Second, for any \( \vec{s}_1,\vec{s}_2\in S \), because addition
  %         is inherited from \( V \), the sum \( \vec{s}_1+\vec{s}_2 \)
  %         in \( S \) equals the sum \( \vec{s}_1+\vec{s}_2 \)
  %         in \( V \) and that equals the sum \( \vec{s}_2+\vec{s}_1 \) in
  %         \( V \) and that in turn equals the sum \( \vec{s}_2+\vec{s}_1 \) in
  %         \( S \).
  %         The argument for the third condition is similar to that for the
  %         second.
  %         For the fourth, suppose that 
  %         \( \vec{s} \) is in the nonempty set \( S \)
  %         and note that \( 0\cdot\vec{s}=\zero\in S \); showing that 
  %         the \( \zero \) of
  %         \( V \) acts under the inherited operations as the additive identity
  %         of \( S \) is easy.
  %         The fifth condition is satisfied because for any \( \vec{s}\in S \)
  %         closure under linear combinations shows that the vector
  %         \( 0\cdot\zero+(-1)\cdot\vec{s} \) is in \( S \); showing 
  %         that it is the
  %         additive inverse of \( \vec{s} \) under the inherited operations is
  %         routine.

  %         The proofs for the remaining conditions are similar.
  %     \end{exparts}  
  %   \end{answer}
\end{exercises}





















 
\subsection{Subspaces and Spanning Sets}
\index{subspace|(}
One of the examples that led us to define vector spaces was
the solution set of a homogeneous system.
For instance, we saw in \nearbyexample{PlaneThruOriginSubsp}
such a space that is a planar subset of $\Re^3$. 
There, the vector space $\Re^3$ contains inside it another
vector space, the plane.

\begin{definition} \label{df:Subspace}
%<*df:Subspace>
For any vector space,
a \definend{subspace}\index{vector space!subspace}\index{subspace!definition} 
is a subset that is itself a vector space,
under the inherited operations.
%</df:Subspace>
\end{definition}

\begin{example}  \label{ex:PlaneSubspRThree}
\nearbyexample{PlaneThruOriginSubsp}'s plane
\begin{equation*}
  P=\set{\colvec{x \\ y \\ z}\suchthat x+y+z=0}
\end{equation*}
is a subspace of \( \Re^3 \).
As required by the definition 
the plane's operations are inherited from the larger space,
that is,
vectors add in $P$ as they add in $\Re^3$
\begin{equation*}
   \colvec{x_1 \\ y_1 \\ z_1}+\colvec{x_2 \\ y_2 \\ z_2}
   =\colvec{x_1+x_2 \\ y_1+y_2 \\ z_1+z_2}
\end{equation*}
and scalar multiplication is also the same as in $\Re^3$.
To show that $P$ is a subspace we need only note that it is a subset and then
verify that it is a space.
We have already checked in \nearbyexample{PlaneThruOriginSubsp}
that $P$ satisfies the conditions in the definition of a 
vector space.
For instance, for closure under addition we noted that if
the summands satisfy that
$x_1+y_1+z_1=0$ and $x_2+y_2+z_2=0$ then the sum satisfies that
$(x_1+x_2)+(y_1+y_2)+(z_1+z_2)=(x_1+y_1+z_1)+(x_2+y_2+z_2)=0$.
\end{example}

\begin{example}   \label{ex:SubspacesRTwo}
The \( x \)-axis in \( \Re^2 \) 
is a subspace, where
the addition and scalar multiplication operations are 
the inherited ones.
\begin{equation*}
  \colvec{x_1 \\ 0}
    +
  \colvec{x_2 \\ 0}
    =
  \colvec{x_1+x_2 \\ 0}
  \qquad
  r\cdot\colvec{x \\ 0}
  =\colvec{rx \\ 0}
\end{equation*}
As in the prior example, to verify directly from the definition 
that this is a subspace we simply that it is a
subset and then check that it satisfies
the conditions in definition of a vector space.
For instance the two closure conditions are 
satisfied: adding two vectors with a second component of zero results
in a vector with a second component of zero and multiplying a 
scalar times a vector with a second component of zero 
results in a vector with a second component of zero.
\end{example}

\begin{example}
Another subspace of $\Re^2$ is 
its trivial subspace.
\begin{equation*}
  \set{\colvec[r]{0 \\ 0}}
\end{equation*}
\end{example}

%<*ProperSubspace>
Any vector space has a trivial subspace \( \set{\zero\,} \).
At the opposite extreme, any vector space has itself for a subspace.
These two are the \definend{improper}\index{subspace!improper}%
\index{improper subspace} subspaces.
Other subspaces are \definend{proper}\index{subspace!proper}%
\index{proper subspace}. 
%</ProperSubspace>

\begin{example}  \label{ex:LinSubspPolyThree}
Vector spaces that are not $\Re^n$'s also have subspaces.
The space of cubic polynomials
\( \set{a+bx+cx^2+dx^3\suchthat a,b,c,d\in\Re} \) 
has a subspace comprised of all linear polynomials
\( \set{m+nx\suchthat m,n\in\Re} \).
\end{example}

\begin{example}
Another example of a subspace that is not a subset of an $\Re^n$ 
followed the definition of a vector space.
The space in \nearbyexample{ex:RealValuedFcns}
of all real-valued functions of one real variable 
\( \set{f\suchthat \map{f}{\Re}{\Re} } \) has the subspace in
\nearbyexample{ex:SpaceSatisfyingDiffQ}
of functions satisfying
the restriction $(d^2\,f/dx^2)+f=0$.
\end{example}

\begin{example} \label{ex:OperNotInherit}
The definition requires that the 
addition and scalar multiplication operations
must be the ones inherited from the larger space.
The set \( S=\set{1} \) is a subset of \( \Re^1 \).
And, under the operations $1+1=1$ and  $r\cdot 1=1$
the set $S$ is a vector space, specifically, a trivial space.
However, $S$ is not a subspace of \( \Re^1 \) because those aren't the
inherited operations, since of course \( \Re^1 \) has \( 1+1=2 \).
\end{example}

\begin{example}  \label{cex:RPlusNotSubSp}
Being vector spaces themselves, subspaces must satisfy the closure
conditions.
The set \( \Re^+ \) is not a subspace of the vector space \( \Re^1 \)
because with the inherited operations it is not closed under scalar
multiplication: if \( \vec{v}=1 \) then \( -1\cdot\vec{v}\not\in\Re^+ \).
\end{example}

The next result says that \nearbyexample{cex:RPlusNotSubSp} is prototypical. 
The only way that a subset can fail to be a subspace, 
if it is nonempty and uses the inherited operations,
is if it isn't closed.

\begin{lemma}     \label{th:SubspIffClosed} \index{subspace!closed}
%<*lm:SubspIffClosed>
For a nonempty subset \( S \) of a vector space, under the inherited 
operations the following are equivalent 
statements.\appendrefs{equivalence of statements}   %\spacefactor=1000
\begin{tfae}
  \item \( S \) is a subspace of that vector space
  \item \( S \) is closed under linear combinations of pairs of vectors:
    for any vectors \( \vec{s}_1,\vec{s}_2\in S \) and scalars \( r_1,r_2 \)
    the vector \( r_1\vec{s}_1+r_2\vec{s}_2 \) is in \( S \)
  \item \( S \) is closed under linear combinations of any number of vectors:
    for any vectors \( \vec{s}_1,\ldots,\vec{s}_n\in S \) and scalars
    \( r_1, \ldots,r_n \)
    the vector \( r_1\vec{s}_1+\cdots+r_n\vec{s}_n \) is an element of \( S \).
\end{tfae}
%</lm:SubspIffClosed>
\end{lemma}
\noindent Briefly, a subset is a 
subspace if and only if  it is closed under linear combinations.

\begin{proof}
%<*pf:SubspIffClosed0>
`The following are equivalent' means that each pair of 
statements are equivalent.
\begin{equation*}
  (1)\!\iff\!(2)
  \qquad
  (2)\!\iff\!(3)
  \qquad
  (3)\!\iff\!(1)
\end{equation*}
We will prove the equivalence by establishing that
\( (1)\implies (3)\implies (2)\implies (1)\).
This strategy is suggested by the observation that the implications 
\( (1)\implies (3) \) and \( (3)\implies (2) \) are easy and so we need only
argue that \( (2)\implies (1) \).
%</pf:SubspIffClosed0>

%<*pf:SubspIffClosed1>
Assume that \( S \) is a nonempty subset of a vector space
$V$ that is $S$ closed under combinations of pairs of vectors.
We will show that $S$ is a vector space by checking the conditions.

The vector space definition has five conditions on addition.
First, for closure under addition, if
\( \vec{s}_1,\vec{s}_2\in S \) then \( \vec{s}_1+\vec{s}_2\in S \),
as it
is a combination of a pair of vectors 
and we are assuming that~\( S \) is closed under those.
Second, for any \( \vec{s}_1,\vec{s}_2\in S \), because addition
is inherited from \( V \), the sum \( \vec{s}_1+\vec{s}_2 \)
in \( S \) equals the sum \( \vec{s}_1+\vec{s}_2 \)
in \( V \), and that equals the sum \( \vec{s}_2+\vec{s}_1 \) in
\( V \) (because $V$ is a vector space, its addition is commutative), 
and that in turn equals the sum \( \vec{s}_2+\vec{s}_1 \) in \( S \).
The argument for the third condition is similar to that for the second.
For the fourth, consider the zero vector of \( V \) and note that 
closure of $S$ under linear combinations of pairs of vectors gives that 
\( 0\cdot\vec{s}+0\cdot\vec{s}=\zero \) is an element of \( S\) 
(where \( \vec{s} \) is any member of the nonempty set \( S \));
checking that \( \zero \) acts under the inherited operations as the additive
identity of \( S \) is easy.
The fifth condition is satisfied because for any \( \vec{s}\in S \),
closure under linear combinations of pairs of vectors shows that 
\( 0\cdot\zero+(-1)\cdot\vec{s} \) is an element of~\( S \), and it is
obviously the
additive inverse of \( \vec{s} \) under the inherited operations.
%</pf:SubspIffClosed1>

The verifications for the scalar multiplication conditions are similar;
see \nearbyexercise{exer:SubspIffClosed}.
\end{proof}

We will usually verify that a subset is a subspace by checking that it 
satisfies statement~(2).

\begin{remark}
At the start of this chapter we introduced vector spaces as collections in
which linear combinations ``make sense.''
% The vector space definition has ten conditions but eight of them, the
% conditions not about closure, simply ensure that referring to the
% operations by the names `addition' and `scalar multiplication' is sensible.
% For instance, calling an operation `$+$' would not
% be odd unless it is commutative, as in condition~(2). 
% The proof above checks that the subspace satisfies these 
% eight non-closure conditions because they hold in the
% surrounding vector space, provided that the nonempty set $S$ satisfies
% \nearbytheorem{th:SubspIffClosed}'s statement~(2) 
% (e.g., commutativity of addition in $S$ follows right from
% commutativity of addition in $V$).
% There is a second way in which we can regard vector spaces as structures in 
% which linear combinations are ``sensible.''
% It has to do with the closure conditions.
\nearbytheorem{th:SubspIffClosed}'s statements (1)-(3) 
say that we can always make sense of 
an expression like
$r_1\vec{s}_1+r_2\vec{s}_2$
in that the vector described is in the set~$S$.

As a contrast, consider the set $T$ of two-tall vectors whose entries add to
a number greater than or equal to zero.
Here we cannot just write any linear combination such as $2\vec{t}_1-3\vec{t}_2$
and be confident the result is an element of $T$.
\end{remark}

\nearbylemma{th:SubspIffClosed} suggests that a good way to think of
a vector space is as a collection of unrestricted linear combinations.
The next two examples take some spaces and recasts their
descriptions to be in that form.

\begin{example}
We can show that this plane through the origin subset of $\Re^3$
\begin{equation*}
  S=\set{\colvec{x \\ y \\ z}\suchthat x-2y+z=0}
\end{equation*}
is a subspace under the usual addition and scalar multiplication
operations of column vectors by checking that it is nonempty and closed under
linear combinations of two vectors.
But there is another way.
Think of  $x-2y+z=0$  as a one-equation linear system and parametrize it
by expressing the leading
variable in terms of the free variables $x=2y-z$.
\begin{equation*}
     S
     =\set{\colvec{2y-z \\ y \\ z}\suchthat y,z\in\Re}
     =\set{y\colvec[r]{2 \\ 1 \\ 0}+
            z\colvec[r]{-1 \\ 0 \\ 1}\suchthat y,z\in\Re}
    \tag{$*$}
\end{equation*}
Now, to show that this is a subspace consider
$r_1\vec{s}_1+r_2\vec{s}_2$.
Each $\vec{s}_i$ is a linear combination of the two vectors in ($*$)
so this is a linear combination of linear combinations.
\begin{equation*}
  r_1\cdot(y_1\colvec[r]{2 \\ 1 \\ 0}+
            z_1\colvec[r]{-1 \\ 0 \\ 1})
  +
  r_2\cdot(y_2\colvec[r]{2 \\ 1 \\ 0}+
            z_2\colvec[r]{-1 \\ 0 \\ 1})
\end{equation*}
The Linear Combination Lemma, Lemma~One.III.\ref{lm:LinearCombinationLemma},
shows that the total is a linear combination of the two vectors
and so \nearbytheorem{th:SubspIffClosed}'s statement~(2) is satisfied.
\end{example}

\begin{example} \label{ex:ParamSubspace}
This is a subspace of the \( \nbyn{2} \) matrices $\matspace_{\nbyn{2}}$.
\begin{equation*}
  L=\set{\begin{mat}
         a  &0  \\
         b  &c
       \end{mat}
       \suchthat a+b+c=0}
\end{equation*}
To parametrize, express the condition as $a=-b-c$.
\begin{equation*}
  L
  =\set{\begin{mat}
         -b-c  &0  \\
         b     &c
       \end{mat}
       \suchthat b,c\in\Re}
  =\set{b\begin{mat}[r]
         -1    &0  \\
         1     &0
       \end{mat}
       +c\begin{mat}[r]
         -1    &0  \\
         0     &1
       \end{mat}
       \suchthat b,c\in\Re}
\end{equation*}
As above, we've described the subspace as a collection of unrestricted linear
combinations.
To show it is a subspace, note that a linear combination of vectors from
$L$ is a linear combination of linear combinations and so statement~(2)
is true.
\end{example}

% Parametrization is easy, but important.
% We shall use it often.

\begin{definition} \label{df:Span}
%<*df:Span>
The \definend{span}\index{span}\index{sets!span of}\index{closure} (or
\definend{linear closure}) of a nonempty subset \( S \) of a
vector space is the set of all linear combinations of vectors from \( S \).
\begin{equation*}
  \spanof{S} =\{ c_1\vec{s}_1+\cdots+c_n\vec{s}_n
            \suchthat c_1,\ldots, c_n\in\Re
            \text{\ and\ } \vec{s}_1,\ldots,\vec{s}_n\in S \}
\end{equation*}
The span of the empty subset of a vector space is its trivial subspace.
%</df:Span>
\end{definition}

%<*NotationForSpan>
\noindent No notation for the span is completely standard.
The square brackets used here are common but so are
`$\mbox{span}(S)$' and `$\mbox{sp}(S)$'.
%</NotationForSpan>

\begin{remark}
In Chapter One, after we showed that we can write the solution
set of a homogeneous linear system as 
$\set{c_1\vec{\beta}_1+\cdots+c_k\vec{\beta}_k\suchthat
  c_1,\ldots,c_k\in\Re}$,
we described that as the set `generated' by the $\smash{\vec{\beta}}$'s.
We now call that the span of
$\set{\vec{\beta}_1,\ldots,\vec{\beta}_k}$.

Recall also from that proof that 
the span of the empty set is defined to be the set \( \set{\zero} \) because
of the convention that a trivial linear combination, a combination of 
zero-many vectors, adds to~\( \zero \).
Besides, defining the empty set's span to be the trivial subspace 
is convenient because it keeps results
like the next one from needing exceptions for the empty set.
\end{remark}

\begin{lemma}   \label{le:SpanIsASubsp}
%<*lm:SpanIsASubsp>
In a vector space, the span of any subset is a subspace.
%</lm:SpanIsASubsp>
\end{lemma}

\begin{proof}
%<*pf:SpanIsASubsp>
If the subset \( S \)
is empty then by definition its span is the trivial
subspace.
If \( S\) is not empty then by \nearbylemma{th:SubspIffClosed} we need
only check that the span \( \spanof{S} \) is closed under linear combinations
of pairs of elements.
For a pair of vectors from that span,
\( \vec{v}=c_1\vec{s}_1+\cdots+c_n\vec{s}_n \) and
\( \vec{w}=c_{n+1}\vec{s}_{n+1}+\cdots+c_m\vec{s}_m \),
a linear combination 
\begin{multline*}
  p\cdot(c_1\vec{s}_1+\cdots+c_n\vec{s}_n)+
       r\cdot(c_{n+1}\vec{s}_{n+1}+\cdots+c_m\vec{s}_m)  \\
  =
  pc_1\vec{s}_1+\cdots+pc_n\vec{s}_n
    +rc_{n+1}\vec{s}_{n+1}+\cdots+rc_m\vec{s}_m
\end{multline*}
is a linear combination of elements of~\( S \) 
and so is an element of \( \spanof{S} \)
(possibly some of the $\vec{s}_i$'s from $\vec{v}$ equal some 
of the $\vec{s}_j$'s from $\vec{w}$ but that does not matter).
%</pf:SpanIsASubsp>
\end{proof}

The converse of the lemma
holds: any subspace is the span of some set, because 
a subspace is obviously the span of itself, the set of all of its members.
Thus a subset of a vector space is a subspace if and only if it is a span.
This fits the intuition 
that a good way to think of a vector space is as
a collection in which linear combinations are sensible.

Taken together, \nearbylemma{th:SubspIffClosed} and
\nearbylemma{le:SpanIsASubsp} show that the span of a subset $S$ of a
vector space is the smallest subspace containing all of the members of $S$.

\begin{example}   \label{ex:SpanSingVec}
In any vector space \( V \), for any vector \( \vec{v}\in V \), the set
\( \set{r\cdot\vec{v} \suchthat r\in\Re} \) is a subspace of \( V \).
For instance, for any vector \( \vec{v}\in\Re^3 \)
the line through the origin containing that vector
\( \set{k\vec{v}\suchthat k\in\Re } \) is a subspace of \( \Re^3 \).
This is true even if $\vec{v}$ is the zero vector, in which case 
it is the degenerate line, the trivial subspace.
\end{example}

\begin{example}
The span of this set
is all of $\Re^2$.
\begin{equation*}
  \set{\colvec[r]{1 \\ 1},\colvec[r]{1 \\ -1}}
\end{equation*}
We know that the span is some subspace of~$\Re^2$.
To check that it is all of~$\Re^2$ 
we must show that any member of $\Re^2$ is a linear combination
of these two vectors.
So we ask:~for which
vectors with real components $x$ and~$y$ 
are there scalars $c_1$ and $c_2$ such that this holds?
\begin{equation*}
   c_1\colvec[r]{1 \\ 1}+c_2\colvec[r]{1 \\ -1}=\colvec{x \\ y}
  \tag{$*$}
\end{equation*} 
Gauss's Method 
\begin{equation*}
  \begin{linsys}{2}
    c_1  &+  &c_2  &=  &x  \\
    c_1  &-  &c_2  &=  &y
  \end{linsys}
  \grstep{-\rho_1+\rho_2}
  \begin{linsys}{2}
    c_1  &+  &c_2    &=  &x\hfill  \\
         &   &-2c_2  &=  &-x+y
  \end{linsys}
\end{equation*}
with back substitution gives $c_2=(x-y)/2$ and $c_1=(x+y)/2$.
This shows that for any $x,y$ there 
are appropriate coefficients $c_1,c_2$ making~($*$)
true\Dash
we can write any element of $\Re^2$ as a linear combination of the 
two given ones.
For instance, for $x=1$ and $y=2$ the coefficients $c_2=-1/2$ and
$c_1=3/2$ will do.
\end{example}

Since spans are subspaces, and we know that a
good way to understand a subspace is
to parametrize its description, we can try to understand a set's span in 
that way.

\begin{example}
Consider, in the vector space of quadratic polynomials~\( \polyspace_2 \), 
the span of the set \( S=\set{3x-x^2, 2x} \).
By the definition of span, it is the set of unrestricted linear
combinations of the two $\set{c_1(3x-x^2)+c_2(2x)\suchthat c_1,c_2\in\Re}$.
Clearly polynomials in this span must have a constant term of zero.
Is that necessary condition also sufficient?

We are asking:~for which members $a_2x^2+a_1x+a_0$ 
of $\polyspace_2$ are there $c_1$ and $c_2$ such that
$a_2x^2+a_1x+a_0=c_1(3x-x^2)+c_2(2x)$? 
Polynomials are equal when their coefficients are equal so
we want conditions on $a_2$, $a_1$, and $a_0$ 
making that triple a solution of this system.
\begin{equation*}
  \begin{linsys}{2}
    -c_1  &   &     &=  &a_2   \\
    3c_1  &+  &2c_2 &=  &a_1   \\
          &   &0    &=  &a_0                                   
  \end{linsys}
\end{equation*} 
Gauss's Method and back-substitution gives 
$c_1=-a_2$, and $c_2=(3/2)a_2+(1/2)a_1$, and $0=a_0$.
Thus 
%the only condition on elements  $a_0+a_1x+a_2x^2$ of the span
%is the condition that we knew: 
as long as there is no constant term $a_0=0$
we can give coefficients $c_1$ and $c_2$
to describe that polynomial as an element of the span.
For instance, for the polynomial $0-4x+3x^2$, the coefficients
$c_1=-3$ and $c_2=5/2$ will do.
So the span of the given set is 
$\spanof{S}=\set{a_1x+a_2x^2\suchthat a_1,a_2\in\Re}$. 

Incidentally, this shows that
the set \( \set{x,x^2} \) spans the same subspace.
A space can have more than one spanning set.
Two other sets spanning this subspace are
\( \set{x,x^2,-x+2x^2} \) and
\( \set{x,x+x^2,x+2x^2,\ldots\,} \).
% (Usually we prefer to work with spanning sets that have only
% a small number of members.)
\end{example}

\begin{example}  \label{ex:SubspRThree}
The picture below shows the subspaces of \( \Re^3 \) that we now know of: the 
trivial subspace, lines through the origin,
planes through the origin, and the whole space.
(Of course, the picture shows only a few of the infinitely many cases.
Line segments connect subsets with 
their supersets.) 
In the next section we will prove that $\Re^3$ has no other
kind of subspace, so in fact this lists them all.

This describes each subspace 
as the span of a set with a minimal number of members.
With this, the subspaces 
fall naturally into levels\Dash planes on one level, 
lines on another,
etc.
\begin{center}
  \setlength{\unitlength}{4pt}
  \begin{picture}(75,38)(0,-2) %subspaces of R-three
      \thinlines
      \put(45,31){\makebox(0,0)[bl]{
                        \scriptsize \( %\Re^3=
                                   \set{x\colvec[r]{1 \\ 0 \\ 0}
                                              +y\colvec[r]{0 \\ 1 \\ 0}
                                              +z\colvec[r]{0 \\ 0 \\ 1}} \)} }
    %Next the dimension two subspaces
      \put(43,31){\line(-4,-1){25} } % connects R3 to 2,a
      %set 2,a:
      \put(0,22){\makebox(0,0)[l]{\scriptsize\( \set{x\colvec[r]{1 \\ 0 \\ 0}
                                                 +y\colvec[r]{0 \\ 1 \\ 0} }\) }}
      \put(48,29){\line(-3,-1){12} } % connects R3 to 2,b
      %set 2,b:
      \put(20,22){\makebox(0,0)[l]{\scriptsize\( \set{x\colvec[r]{1 \\ 0 \\ 0}
                                                 +z\colvec[r]{0 \\ 0 \\ 1} }\) }}
      \put(53,29){\line(-1,-1){3}} % connects R3 to 2,c
      %set 2,c:
      \put(40,22){\makebox(0,0)[l]{\scriptsize\( \set{x\colvec[r]{1 \\ 1 \\ 0}
                                                 +z\colvec[r]{0 \\ 0 \\ 1} }\) }}
      \put(60,22){\makebox(0,0)[l]{$\cdots$} }
    %Next the dimension one subspaces
      \put(7,18){\line(-1,-4){1} } % connects 2,a to 1,a
      \put(22,18){\line(-3,-1){13} } % connects 2,c to 1,a
      %set 1,a:
      \put(0,10){\makebox(0,0)[l]{\scriptsize\( \set{x\colvec[r]{1 \\ 0 \\ 0}} \)} }
      \put(12,18){\line(1,-2){2} } % connects 2,a to 1,b
      %set 1,b:
      \put(12,10){\makebox(0,0)[l]{\scriptsize\( \set{y\colvec[r]{0 \\ 1 \\ 0}} \)} }
      \put(14,18){\line(2,-1){10} } % connects 2,a to 1,c
      %set 1,c:
      \put(24,10){\makebox(0,0)[l]{\scriptsize\( \set{y\colvec[r]{2 \\ 1 \\ 0}} \)} }
      \put(46,18){\line(-1,-1){3.25} } %connects 2,c to 1,d
      %set 1,d:
      \put(36,10){\makebox(0,0)[l]{\scriptsize\( \set{y\colvec[r]{1 \\ 1 \\ 1}} \)} }
      \put(48,10){\makebox(0,0)[l]{$\cdots$} }
    %Finally, the trivial subspace.
      \put(9,7.5){\line(4,-1){30} } %connects 1,a to trivial
      \put(18.9,7.7){\line(3,-1){20} } %connects 1,b to trivial
      \put(30.4,6.8){\line(2,-1){10} } %connects 1,c to trivial
      \put(41,6){\line(1,-2){1.5} } %connects 1,d to trivial
      \put(45,0){\makebox(0,0){\scriptsize\( \set{\colvec[r]{0 \\ 0 \\ 0} } \)} }
  \end{picture}
\end{center}
\end{example}

So far in this chapter we have seen that to study the
properties of linear combinations, the right setting is a
collection that is closed under these combinations.
In the first subsection we introduced such collections, vector spaces,
and we saw a great variety of examples.
In this subsection we saw still
more spaces, ones that are subspaces of others.
In all of the variety there is a commonality.
\nearbyexample{ex:SubspRThree} above 
brings it out:~vector spaces and subspaces are best understood as a span, 
and especially as a span of a small number of vectors.
The next section studies spanning sets that are minimal.





\begin{exercises}
  \recommended \item
    Which of these subsets of the vector space of \( \nbyn{2} \) matrices
    are subspaces under the inherited operations?
    For each one that is a subspace, parametrize its description.
    For each that is not, give a condition that fails.
    \begin{exparts}
      \partsitem \( \set{\begin{mat}
                      a  &0  \\
                      0  &b
                    \end{mat}  \suchthat a,b\in\Re}  \)
      \partsitem \( \set{\begin{mat}
                      a  &0  \\
                      0  &b
                    \end{mat}  \suchthat a+b=0} \)
      \partsitem \( \set{\begin{mat}
                      a  &0  \\
                      0  &b
                    \end{mat}  \suchthat a+b=5} \)
      \partsitem \( \set{\begin{mat}
                      a  &c  \\
                      0  &b
                    \end{mat}  \suchthat a+b=0, c\in\Re} \)
    \end{exparts}
    \begin{answer}
      By \nearbylemma{th:SubspIffClosed}, to see if each
      subset of $\matspace_{\nbyn{2}}$ is a subspace, we need only
      check if it is nonempty and closed.
      \begin{exparts}
        \partsitem Yes, we can easily check that it is nonempty and closed.
          This is a parametrization.
          \begin{equation*}
            \set{a\begin{mat}[r]
                    1  &0  \\
                    0  &0
                  \end{mat}
                 +b\begin{mat}[r]
                    0  &0  \\
                    0  &1  
                   \end{mat}
                 \suchthat a,b\in\Re}
          \end{equation*}
          By the way, the parametrization also shows that it is a subspace,
          since it is given as the span of the two-matrix set,
          and any span is a subspace.
        \partsitem Yes; it is easily checked to be nonempty and closed.
         Alternatively, as mentioned in the prior answer, the existence
         of a parametrization shows that it is a subspace.
         For the parametrization, 
         the condition $a+b=0$ can be rewritten as $a=-b$.
         Then we have this.
          \begin{equation*}
            \set{\begin{mat}
                    -b  &0  \\
                    0   &b
                  \end{mat}
                 \suchthat b\in\Re}
            =\set{b\begin{mat}[r]
                    -1  &0  \\
                    0   &1
                  \end{mat}
                 \suchthat b\in\Re}
          \end{equation*}
        \partsitem No.
          It is not closed under addition.
          For instance, 
          \begin{equation*}
            \begin{mat}[r]
              5  &0  \\
              0  &0
            \end{mat}
            +\begin{mat}[r]
              5  &0  \\
              0  &0
            \end{mat}
            =\begin{mat}[r]
              10  &0  \\
              0  &0
            \end{mat}
          \end{equation*}
          is not in the set.
          (This set is also not closed under scalar multiplication,
          for instance, it does not contain the zero matrix.)
        \partsitem Yes.
          \begin{equation*}
            \set{b\begin{mat}[r]
                    -1  &0  \\
                    0   &1
                  \end{mat}
                 +c\begin{mat}[r]
                    0  &1  \\
                    0  &0  
                   \end{mat}
                 \suchthat b,c\in\Re}
          \end{equation*}
      \end{exparts}  
     \end{answer}
  \recommended \item 
    Is this a subspace of \( \polyspace_2 \):
    \( \set{a_0+a_1x+a_2x^2\suchthat a_0+2a_1+a_2=4} \)?
    If it is then parametrize its description.
    \begin{answer}
      No, it is not closed.
      In particular, it is not closed under scalar multiplication because it
      does not contain the zero polynomial.  
    \end{answer}
  \item Is the vector in the span of the set?
    \begin{equation*}
      \colvec{1 \\ 0 \\ 3}
      \quad
      \set{\colvec{2 \\ 1 \\ -1},
           \colvec{1 \\ -1 \\ 1}}
    \end{equation*}
    \begin{answer}
      The equation
      \begin{equation*}
        \colvec{1 \\ 0 \\ 3} = 
                 c_1\colvec{2 \\ 1 \\ -1}
                 +c_2\colvec{1 \\ -1 \\ 1}
      \end{equation*}
      gives rise to a linear system
      \begin{equation*}
        \begin{amat}{2}
          2  &1  &1  \\
          1  &-1 &0  \\
          -1 &1  &3
        \end{amat}
        \grstep[(1/2)\rho_1+\rho_3]{(-1/2)\rho_1+\rho_2}
        \begin{amat}{2}
          2  &1    &1  \\
          0  &-3/2 &-1/2  \\
          0  &0  &3
        \end{amat}
      \end{equation*}
      that has no solution, so the vector is not in the span.   
    \end{answer}
  \recommended \item 
    Decide if the vector lies in the span of the set, inside of the
    space.
    \begin{exparts}
      \partsitem \( \colvec[r]{2 \\ 0 \\ 1} \),
        \( \set{\colvec[r]{1 \\ 0 \\ 0},
                \colvec[r]{0 \\ 0 \\ 1}  } \),
        in \( \Re^3 \)
      \partsitem \( x-x^3 \),
        \( \set{x^2,2x+x^2,x+x^3} \),
        in \( \polyspace_3 \)
      \partsitem \( \begin{mat}[r]
                 0  &1  \\
                 4  &2
               \end{mat}  \),
        \( \set{\begin{mat}[r]
                  1  &0  \\
                  1  &1
                \end{mat},
                \begin{mat}[r]
                  2  &0  \\
                  2  &3
                \end{mat}  } \),
        in \( \matspace_{\nbyn{2}} \)
    \end{exparts}
    \begin{answer}
      \begin{exparts}
         \partsitem Yes, solving the linear system arising from
           \begin{equation*}
             r_1\colvec[r]{1 \\ 0 \\ 0}+r_2\colvec[r]{0 \\ 0 \\ 1}
               =\colvec[r]{2 \\ 0 \\ 1}
           \end{equation*}
           gives \( r_1=2 \) and \( r_2=1 \).
         \partsitem Yes; the linear system arising from
           \( r_1(x^2)+r_2(2x+x^2)+r_3(x+x^3)=x-x^3 \)
           \begin{equation*}
             \begin{linsys}{3}
                   &  &2r_2 &+ &r_3 &= &1  \\
               r_1 &+ &r_2  &  &    &= &0  \\
                   &  &     &  &r_3 &= &-1   
             \end{linsys}
           \end{equation*}
           gives that \( -1(x^2)+1(2x+x^2)-1(x+x^3)=x-x^3 \).
        \partsitem No; any combination of the two given matrices has a zero
           in the upper right.
      \end{exparts}  
    \end{answer}
  \item 
    Which of these are members of the span
    \( \spanof{\set{\cos^2x,\sin^2x} } \)
    in the vector space of real-valued functions of one real variable?
    \begin{exparts*}
      \partsitem \( f(x)=1 \)
      \partsitem \( f(x)=3+x^2 \)
      \partsitem \( f(x)=\sin x \)
      \partsitem \( f(x)=\cos (2x) \)
    \end{exparts*}
    \begin{answer}
      \begin{exparts}
        \partsitem Yes; it is in that span since 
          \( 1\cdot\cos^2x+1\cdot\sin^2x=f(x) \).
        \partsitem No, since \( r_1\cos^2x+r_2\sin^2x=3+x^2 \) has no scalar
          solutions that work for all \( x \).
          For instance, setting $x$ to be $0$ and $\pi$ gives the two
          equations $r_1\cdot 1+r_2\cdot 0=3$ and 
          $r_1\cdot 1+r_2\cdot 0=3+\pi^2$, which are not consistent with each
          other. 
        \partsitem No; consider what happens on setting $x$ to be $\pi/2$ and
          $3\pi/2$.
        \partsitem Yes, \( \cos (2x)=1\cdot\cos^2(x)-1\cdot\sin^2(x) \).
      \end{exparts}  
     \end{answer}
  \recommended \item 
    Which of these sets spans \( \Re^3 \)?
    That is, which of these sets has the property that any three-tall
    vector can be expressed as a suitable linear combination of the
    set's elements?
    \begin{exparts*}
      \partsitem \( \set{ \colvec[r]{1 \\ 0 \\ 0},
               \colvec[r]{0 \\ 2 \\ 0},
               \colvec[r]{0 \\ 0 \\ 3}  } \)
      \partsitem \( \set{ \colvec[r]{2 \\ 0 \\ 1},
               \colvec[r]{1 \\ 1 \\ 0},
               \colvec[r]{0 \\ 0 \\ 1}  } \)
      \partsitem \( \set{ \colvec[r]{1 \\ 1 \\ 0},
               \colvec[r]{3 \\ 0 \\ 0}  } \)
      \partsitem \( \set{ \colvec[r]{1 \\ 0 \\ 1},
               \colvec[r]{3 \\ 1 \\ 0},
               \colvec[r]{-1 \\ 0 \\ 0},
               \colvec[r]{2 \\ 1 \\ 5}  } \)
      \partsitem \( \set{ \colvec[r]{2 \\ 1 \\ 1},
               \colvec[r]{3 \\ 0 \\ 1},
               \colvec[r]{5 \\ 1 \\ 2},
               \colvec[r]{6 \\ 0 \\ 2}  } \)
    \end{exparts*}
    \begin{answer}
      \begin{exparts}
         \partsitem Yes, for any \( x,y,z\in\Re \) this equation
           \begin{equation*}
              r_1\colvec[r]{1 \\ 0 \\ 0}
              +r_2\colvec[r]{0 \\ 2 \\ 0}
              +r_3\colvec[r]{0 \\ 0 \\ 3}
              =\colvec{x \\ y \\ z}
           \end{equation*}
           has the solution \( r_1=x \), \( r_2=y/2 \), and
           \( r_3=z/3 \).
         \partsitem Yes, the equation
           \begin{equation*}
             r_1\colvec[r]{2 \\ 0 \\ 1}
             +r_2\colvec[r]{1 \\ 1 \\ 0}
             +r_3\colvec[r]{0 \\ 0 \\ 1}
             =\colvec{x \\ y \\ z}
           \end{equation*}
           gives rise to this 
           \begin{equation*}
             \begin{linsys}{3}
               2r_1 &+  &r_2  &  &    &=  &x \\
                    &   &r_2  &  &    &=  &y \\
                r_1 &   &     &+ &r_3 &=  &z \\
             \end{linsys}
             \grstep{-(1/2)\rho_1+\rho_3}\repeatedgrstep{(1/2)\rho_2+\rho_3}
             \begin{linsys}{3}
               2r_1 &+  &r_2  &  &    &=  &x\hfill\hbox{} \\
                    &   &r_2  &  &    &=  &y\hfill\hbox{} \\
                    &   &     &  &r_3 &=  &-(1/2)x+(1/2)y+z \\
             \end{linsys}
           \end{equation*}
           so that, given any $x$, $y$, and $z$, we can compute that
           \( r_3=(-1/2)x+(1/2)y+z \), \( r_2=y \), and
           \( r_1=(1/2)x-(1/2)y \).
        \partsitem No.
           In particular, we cannot get the vector
           \begin{equation*}
             \colvec[r]{0 \\ 0 \\ 1}
           \end{equation*}
           as a linear combination since the two given
           vectors both have a third component of zero.
       \partsitem Yes.
         The equation
         \begin{equation*}
           r_1\colvec[r]{1 \\ 0 \\ 1}
           +r_2\colvec[r]{3 \\ 1 \\ 0}
           +r_3\colvec[r]{-1\\ 0 \\ 0}
           +r_4\colvec[r]{2 \\ 1 \\ 5}
           =\colvec{x \\ y \\ z}
         \end{equation*}
         leads to this reduction.
         \begin{equation*}
           \begin{amat}{4}
             1  &3  &-1  &2  &x  \\
             0  &1  &0   &1  &y  \\
             1  &0  &0   &5  &z  
           \end{amat}                                              
           \grstep{-\rho_1+\rho_3}\repeatedgrstep{3\rho_2+\rho_3}
           \begin{amat}{4}
             1  &3  &-1  &2  &x \\
             0  &1  &0   &1  &y  \\
             0  &0  &1   &6  &-x+3y+z
           \end{amat}
         \end{equation*}
         We have infinitely many solutions.
         We can, for example, set $r_4$ to be zero and solve for
         $r_3$, $r_2$, and $r_1$ in terms of $x$, $y$, and $z$ by the usual
         methods of back-substitution.
       \partsitem No.
         The equation
         \begin{equation*}
           r_1\colvec[r]{2 \\ 1 \\ 1}
           +r_2\colvec[r]{3 \\ 0 \\ 1}
           +r_3\colvec[r]{5 \\ 1 \\ 2}
           +r_4\colvec[r]{6 \\ 0 \\ 2}
           =\colvec{x \\ y \\ z}
         \end{equation*}
         leads to this reduction.
         \begin{multline*}
           \begin{amat}{4}
             2  &3  &5   &6  &x  \\
             1  &0  &1   &0  &y  \\
             1  &1  &2   &2  &z  
           \end{amat}                                             \\
           \grstep[-(1/2)\rho_1+\rho_3]{-(1/2)\rho_1+\rho_2}
           \repeatedgrstep{-(1/3)\rho_2+\rho_3}
           \begin{amat}{4}
             2  &3     &5     &6  &x \\
             0  &-3/2  &-3/2  &-3 &-(1/2)x+y  \\
             0  &0     &0     &0  &-(1/3)x-(1/3)y+z
           \end{amat}
         \end{multline*}
         This shows that not every three-tall vector can be so expressed.
         Only the vectors satisfying the restriction that
         $-(1/3)x-(1/3)y+z=0$ are in the span.
         (To see that any such vector is indeed expressible, 
         take $r_3$ and $r_4$
         to be zero and solve for $r_1$ and $r_2$ in terms of $x$, $y$, and
         $z$ by back-substitution.)
      \end{exparts}  
    \end{answer}
  \recommended \item 
    Parametrize each subspace's description.
    Then express each subspace as a span.
    \begin{exparts}
      \partsitem  The subset \( \set{\rowvec{a &b &c}\suchthat a-c=0}   \)
        of the three-wide row vectors
      \partsitem This subset of \( \matspace_{\nbyn{2}} \)
        \begin{equation*}
          \set{\begin{mat}
                 a  &b  \\
                 c  &d
               \end{mat}  \suchthat a+d=0}
        \end{equation*}
      \partsitem This subset of \( \matspace_{\nbyn{2}} \)
        \begin{equation*}
          \set{\begin{mat}
                 a  &b  \\
                 c  &d
               \end{mat}  \suchthat \text{\( 2a-c-d=0 \) 
                                                 and \( a+3b=0 \)} }
        \end{equation*}
      \partsitem The subset \( \set{a+bx+cx^3\suchthat a-2b+c=0} \) of
        \( \polyspace_3 \)
      \partsitem The subset of \( \polyspace_2 \) of quadratic polynomials 
        \( p \) such that \( p(7)=0 \)
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem \( \set{\rowvec{c &b &c}\suchthat b,c\in\Re}
                      =\set{b\rowvec{0 &1 &0}+c\rowvec{1 &0 &1}
                        \suchthat b,c\in\Re} \)
           The obvious choice for the set that spans is 
           $\set{\rowvec{0 &1 &0},\rowvec{1 &0 &1}}$.
        \partsitem \( \set{\begin{mat}
                       -d &b  \\
                       c  &d
                      \end{mat} \suchthat b,c,d\in\Re}
                     =\set{b\begin{mat}[r]
                       0  &1  \\
                       0  &0
                      \end{mat}
                     +c\begin{mat}[r]
                       0  &0  \\
                       1  &0
                      \end{mat}
                     +d\begin{mat}[r]
                       -1  &0  \\
                       0  &1
                      \end{mat}  \suchthat b,c,d\in\Re} \)
            One set that spans this space consists of those three matrices. 
        \partsitem The system
          \begin{equation*}
            \begin{linsys}{4}
              a  &+  &3b  &   &   &  &  &=  &0  \\
             2a  &   &    &   &-c &- &d &=  &0  
            \end{linsys}
          \end{equation*}
          gives \( b=-(c+d)/6 \) and \( a=(c+d)/2 \).
          So one description is this.
          \begin{equation*}
            \set{c\begin{mat}[r]
                       1/2  &-1/6  \\
                       1    &0
                      \end{mat}
                     +d\begin{mat}[r]
                       1/2  &-1/6  \\
                       0    &1
                      \end{mat}  \suchthat c,d\in\Re}
           \end{equation*}
          That shows that a set spanning this subspace consists of those
          two matrices.
        \partsitem The $a=2b-c$ gives that the set
           \( \set{(2b-c)+bx+cx^3 \suchthat b,c\in\Re} \)
           equals the set
           \( \set{b(2+x)+c(-1+x^3) \suchthat b,c\in\Re}  \).
           So the subspace is the span of the set $\set{2+x, -1+x^3}$.
        \partsitem The set
          \( \set{a+bx+cx^2\suchthat a+7b+49c=0} \)
          can be parametrized as
          \begin{equation*}
            \set{b(-7+x)+c(-49+x^2)\suchthat b,c\in\Re} 
          \end{equation*}
          and so 
          has the spanning set $\set{-7+x,-49+x^2}$.
      \end{exparts}  
    \end{answer}
  \recommended \item 
    Find a set to span the given subspace of the given space.
    (\textit{Hint.}   Parametrize each.)
    \begin{exparts}
      \partsitem  the \( xz \)-plane in \( \Re^3 \)
      \partsitem \( \set{\colvec{x \\ y \\ z}\suchthat 3x+2y+z=0} \)
            in \( \Re^3 \)
      \partsitem \( \set{\colvec{x \\ y \\ z \\ w}\suchthat
                       2x+y+w=0 \text{\ and\ } y+2z=0} \)
            in \( \Re^4 \)
      \partsitem \( \set{a_0+a_1x+a_2x^2+a_3x^3\suchthat
                        a_0+a_1=0 \text{\ and\ } a_2-a_3=0} \)
            in \( \polyspace_3 \)
      \partsitem The set \( \polyspace_4 \) in the space \( \polyspace_4 \)
      \partsitem \( \matspace_{\nbyn{2}} \) in \( \matspace_{\nbyn{2}} \)
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem We can parametrize in this way 
          \begin{equation*}
             \set{\colvec{x \\ 0 \\ z}\suchthat x,z\in\Re}
             =\set{x\colvec[r]{1 \\ 0 \\ 0}
                  +z\colvec[r]{0 \\ 0 \\ 1}\suchthat x,z\in\Re}
          \end{equation*}
          giving this for a spanning set.
          \begin{equation*}
             \set{\colvec[r]{1 \\ 0 \\ 0},\colvec[r]{0 \\ 0 \\ 1}} 
          \end{equation*}
        \item Here is a parametrization, and the associated spanning set.
          \begin{equation*}
             \set{y\colvec[r]{-2/3 \\ 1 \\ 0}+z\colvec[r]{-1/3 \\ 0 \\ 1}
                 \suchthat y,z\in\Re }
             \qquad
             \set{\colvec[r]{-2/3 \\ 1 \\ 0},\colvec[r]{-1/3 \\ 0 \\ 1} } 
          \end{equation*}
        \partsitem \( \set{\colvec[r]{1 \\ -2 \\ 1 \\ 0},
                      \colvec[r]{-1/2 \\ 0 \\ 0 \\ 1} } \)
        \partsitem Parametrize the description as
          \( \set{-a_1+a_1x+a_3x^2+a_3x^3\suchthat a_1,a_3\in\Re } \)
          to get \( \set{-1+x,x^2+x^3}. \)
        \partsitem \( \set{1,x,x^2,x^3,x^4} \)
        \partsitem \( \set{ \begin{mat}[r]
                   1  &0  \\
                   0  &0
                 \end{mat},
                 \begin{mat}[r]
                   0  &1  \\
                   0  &0
                 \end{mat},
                 \begin{mat}[r]
                   0  &0  \\
                   1  &0
                 \end{mat},
                 \begin{mat}[r]
                   0  &0  \\
                   0  &1
                 \end{mat} } \)
      \end{exparts}  
    \end{answer}
  \item 
    Is \( \Re^2 \) a subspace of \( \Re^3 \)?
    \begin{answer}
      Technically, no.
      Subspaces of \( \Re^3 \) are sets of three-tall vectors, while
      \( \Re^2 \) is a set of two-tall vectors.
      Clearly though, \( \Re^2 \) is ``just like'' this subspace of 
      \( \Re^3 \).
      \begin{equation*}
        \set{\colvec{x \\ y \\ 0}\suchthat x,y\in\Re}
      \end{equation*}  
    \end{answer}
  \recommended \item 
    Decide if each is a subspace of the vector space of real-valued
    functions of one real variable.
    \begin{exparts}
      \partsitem The 
        \definend{even}\index{function!even}\index{even functions} functions
        \( \set{\map{f}{\Re}{\Re} \suchthat f(-x)=f(x) \text{ for all } x} \).
        For example, two members of this set are $f_1(x)=x^2$ 
        and $f_2(x)=\cos (x)$.
      \partsitem The \definend{odd}\index{function!odd}\index{odd function}
        functions
        \( \set{\map{f}{\Re}{\Re} \suchthat f(-x)=-f(x) \text{ for all } x} \).
        Two members are $f_3(x)=x^3$ and $f_4(x)=\sin(x)$.
    \end{exparts}
    \begin{answer}
      Of course, the addition and scalar multiplication operations are the
      ones inherited from the enclosing space.
      \begin{exparts}
        \partsitem This is a subspace.
          It is not empty as it contains at least the two example functions
          given.
          It is closed because if \( f_1,f_2 \) are even and
          \( c_1,c_2 \) are scalars then we have this.
          \begin{equation*}
            (c_1f_1+c_2f_2)\,(-x)
            =c_1\,f_1(-x)+c_2\,f_2(-x)
            =c_1\,f_1(x)+c_2\,f_2(x)
            =(c_1f_1+c_2f_2)\,(x)
          \end{equation*}
        \partsitem This is also a subspace; the check is similar to
          the prior one.
      \end{exparts}  
    \end{answer}
  \item 
    \nearbyexample{ex:SpanSingVec} says that for any vector $\vec{v}$ 
    that is an element of 
    a vector space $V$, the set $\set{r\cdot\vec{v}\suchthat r\in\Re}$
    is a subspace of $V$.
    (This is of course, simply the span\index{span!of a singleton} 
    of the singleton set $\set{\vec{v}}$.)
    Must any such subspace be a proper subspace, or can it be improper?
    \begin{answer}
      It can be improper.
      If \( \vec{v}=\zero \) then this is a trivial subspace.
      At the opposite extreme,
      if the vector space is \( \Re^1 \) and \( \vec{v}\neq\zero\, \)
      then the subspace is all of $\Re^1$.  
    \end{answer}
  \item 
    An example following the definition of a vector space shows that the
    solution set of a homogeneous linear system is a vector space.
    In the terminology of this subsection, it is a subspace of $\Re^n$ where
    the system has $n$ variables.
    What about a non-homogeneous linear system; do its solutions form a 
    subspace (under the inherited operations)?
    \begin{answer}
      No, such a set is not closed.
      For one thing, it does not contain the zero vector.  
    \end{answer}
  \item \cite{Cleary} 
   Give an example of each or explain why it would be impossible 
   to do so.
   \begin{exparts}
     \item A nonempty subset of $\matspace_{\nbyn{2}}$ that is
       not a subspace.
     \item A set of two vectors in $\Re^2$ that does not span the space.
   \end{exparts}
   \begin{answer}
     \begin{exparts}
       \item This nonempty subset of $\matspace_{\nbyn{2}}$ is not a subspace.
         \begin{equation*}
           A=\set{
             \begin{mat}[r]
               1 &2 \\
               3 &4
             \end{mat},
             \begin{mat}[r]
               5 &6 \\
               7 &8
             \end{mat}}
         \end{equation*}
         One reason that it is not a subspace of $\matspace_{\nbyn{2}}$ is that 
         it does not contain the zero matrix.
         (Another reason is that it is not closed under addition, since the sum
         of the two is not an element of $A$.
         It is also not closed under scalar multiplication.)
        \item This set of two vectors does not span $\Re^2$.
          \begin{equation*}
            \set{\colvec[r]{1 \\ 1},\colvec[r]{3 \\ 3}}
          \end{equation*}
          No linear combination of these two can give
          a vector whose second component is unequal to its first component.
      \end{exparts}
   \end{answer}
  \item 
    \nearbyexample{ex:SubspRThree} shows that 
    $\Re^3$ has infinitely many subspaces.
    Does every nontrivial space have infinitely many subspaces?
    \begin{answer}
      No.
      The only subspaces of \( \Re^1 \) are the space itself and its 
      trivial subspace.
      Any subspace $S$ of $\Re$ that contains a nonzero member $\vec{v}$ 
      must contain the set of all of its scalar multiples 
      $\set{r\cdot\vec{v}\suchthat r\in\Re}$. 
      But this set is all of $\Re$.  
    \end{answer}
  \item \label{exer:SubspIffClosed}
    Finish the proof of \nearbylemma{th:SubspIffClosed}.
    \begin{answer}
      Item~(1) is checked in the text.

      Item~(2) has five conditions.
      First, for closure, if \( c\in\Re \) and \( \vec{s}\in S \) then
      \( c\cdot\vec{s}\in S \) as 
      \( c\cdot\vec{s}=c\cdot\vec{s}+0\cdot\zero \).
      Second, because the operations in \( S \) are inherited from \( V \),
      for \( c,d\in\Re \) and \( \vec{s}\in S \), the scalar product
      \( (c+d)\cdot\vec{s}\, \) in \( S \) equals the product
      \( (c+d)\cdot\vec{s}\, \) in \( V \), and that equals
      \( c\cdot\vec{s}+d\cdot\vec{s}\, \) in \( V \), which equals
      \( c\cdot\vec{s}+d\cdot\vec{s}\, \) in \( S \).

      The check for the third, fourth, and fifth conditions are similar to the
      second condition's check just given.  
    \end{answer}
  \item 
    Show that each vector space has only one trivial subspace.
    \begin{answer}
      An exercise in the prior subsection shows that every vector space
      has only one zero vector (that is, there is only one vector that is the
      additive identity element of the space).
      But a trivial space has only one element and that element must be this
      (unique) zero vector.
    \end{answer}
  \recommended \item 
    Show that for any subset \( S \) of a vector space,
    the span of the span equals the span 
    \( \spanof{ \spanof{S} }=\spanof{S} \).
    (\textit{Hint.} 
    Members of $\spanof{S}$ are linear combinations of members of $S$.
    Members of $\spanof{\spanof{S}}$ are linear combinations of 
    linear combinations of members of $S$.)
    \begin{answer}
      As the hint suggests, the basic reason is the Linear Combination Lemma
      from the first chapter.
      For the full proof, we will show mutual containment between the two sets.

      The first containment \( \spanof{ \spanof{S} }\supseteq\spanof{S}  \)
      is an instance of the more general, and obvious, fact that for any 
      subset \( T \) of a vector space, \( \spanof{T}\supseteq T \).

      For the other containment, 
      that \( \spanof{ \spanof{S} }\subseteq\spanof{S} \),
      take $m$ vectors from \( \spanof{S} \), namely
      \( c_{1,1}\vec{s}_{1,1}+\cdots+c_{1,n_1}\vec{s}_{1,n_1} \), \ldots,
      \( c_{1,m}\vec{s}_{1,m}+\cdots+c_{1,n_m}\vec{s}_{1,n_m} \),
      and note that any linear combination of those
      \begin{equation*}
         r_1(c_{1,1}\vec{s}_{1,1}+\cdots+c_{1,n_1}\vec{s}_{1,n_1})+\cdots
        +r_m(c_{1,m}\vec{s}_{1,m}+\cdots+c_{1,n_m}\vec{s}_{1,n_m})
      \end{equation*}
      is a linear combination of elements of \( S \)
      \begin{equation*}
        = (r_1c_{1,1})\vec{s}_{1,1}+\cdots+(r_1c_{1,n_1})\vec{s}_{1,n_1}+\cdots
        +(r_mc_{1,m})\vec{s}_{1,m}+\cdots+(r_mc_{1,n_m})\vec{s}_{1,n_m}
      \end{equation*}
      and so is in \( \spanof{S} \).  
      That is, simply recall that a linear combination of linear combinations
      (of members of $S$)
      is a linear combination (again of members of $S$).
    \end{answer}
  \item 
    All of the subspaces that we've seen in some way use zero in their 
    description.
    For example, the subspace in \nearbyexample{ex:SubspacesRTwo} consists of
    all the vectors from $\Re^2$ with a second component of zero.
    In contrast,
    the collection of vectors from $\Re^2$ with a second component of one
    does not form a subspace (it is not closed under scalar multiplication).
    Another example is \nearbyexample{ex:PlaneSubspRThree}, where the condition
    on the vectors is that the three components add to zero.
    If the condition there 
    were that the three components add to one then it would 
    not be a subspace (again, it would fail to be closed).
    However, a reliance on zero is not strictly necessary.
    Consider the set 
    \begin{equation*}
       \set{\colvec{x \\ y \\ z}\suchthat x+y+z=1}
    \end{equation*}
    under these operations.
    \begin{equation*}
       \colvec{x_1 \\ y_1 \\ z_1}+\colvec{x_2 \\ y_2 \\ z_2}
       =\colvec{x_1+x_2-1 \\ y_1+y_2 \\ z_1+z_2}
       \qquad
       r\colvec{x \\ y \\ z}=\colvec{rx-r+1 \\ ry \\ rz}
    \end{equation*}
    \begin{exparts}
      \partsitem Show that it is not a subspace of $\Re^3$.
         (\textit{Hint.}   See \nearbyexample{ex:OperNotInherit}).
      \partsitem Show that it is a vector space.         
         Note that by the prior item,
         \nearbylemma{th:SubspIffClosed} can not apply.  
      \partsitem Show that any subspace of $\Re^3$ must pass through the origin,
         and so any subspace of $\Re^3$ must involve zero in its description.
         Does the converse hold?  
          Does any subset of $\Re^3$ that contains the origin become a
          subspace when given the inherited operations?
    \end{exparts}
    \begin{answer}
      \begin{exparts}
         \partsitem It is not a subspace because these are not the inherited 
           operations.  
           For one thing, in this space,
           \begin{equation*}
             0\cdot\colvec{x \\ y \\ z}=\colvec[r]{1 \\ 0 \\ 0}
           \end{equation*}
           while this does not, of course, hold in $\Re^3$.
         \partsitem We can combine the argument showing closure under
           addition with the argument showing closure under 
           scalar multiplication into one single argument
           showing closure under linear combinations of two vectors.
           If $r_1,r_2,x_1,x_2,y_1,y_2,z_1,z_2$ are in $\Re$ then      
           \begin{multline*}
              r_1\colvec{x_1 \\ y_1 \\ z_1}
              +r_2\colvec{x_2 \\ y_2 \\ z_2}
              =\colvec{r_1x_1-r_1+1 \\ r_1y_1 \\ r_1z_1}
               +\colvec{r_2x_2-r_2+1 \\ r_2y_2 \\ r_2z_2}                 \\
            =\colvec{r_1x_1-r_1+r_2x_2-r_2+1 \\ r_1y_1+r_2y_2 \\ r_1z_1+r_2z_2}
           \end{multline*} 
           (note that the definition of addition in this space is that
           the first
           components combine as $(r_1x_1-r_1+1)+(r_2x_2-r_2+1)-1$,
           so the first component of the last vector does not say
           `$\hbox{}+2$').
           Adding the three components of the last vector gives
           $r_1(x_1-1+y_1+z_1)+r_2(x_2-1+y_2+z_2)+1=r_1\cdot0+r_2\cdot0+1=1$.

           Most of the other checks of the conditions are easy (although the
           oddness of the operations keeps them from being routine).
           Commutativity of addition goes like this.
           \begin{equation*}
             \colvec{x_1 \\ y_1 \\ z_1}+\colvec{x_2 \\ y_2 \\ z_2}
             =\colvec{x_1+x_2-1 \\ y_1+y_2 \\ z_1+z_2}
             =\colvec{x_2+x_1-1 \\ y_2+y_1 \\ z_2+z_1}
             =\colvec{x_2 \\ y_2 \\ z_2}+\colvec{x_1 \\ y_1 \\ z_1}
           \end{equation*}
           Associativity of addition has
           \begin{equation*}
             (\colvec{x_1 \\ y_1 \\ z_1}+\colvec{x_2 \\ y_2 \\ z_2})
              +\colvec{x_3 \\ y_3 \\ z_3}
             =\colvec{(x_1+x_2-1)+x_3-1 \\ (y_1+y_2)+y_3 \\ (z_1+z_2)+z_3}
           \end{equation*}
           while
           \begin{equation*}
             \colvec{x_1 \\ y_1 \\ z_1}
             +(\colvec{x_2 \\ y_2 \\ z_2}+\colvec{x_3 \\ y_3 \\ z_3})
             =\colvec{x_1+(x_2+x_3-1)-1 \\ y_1+(y_2+y_3) \\ z_1+(z_2+z_3)}
           \end{equation*}
           and they are equal.
           The identity element with respect to this addition operation 
           works this way
           \begin{equation*}
             \colvec{x \\ y \\ z}+\colvec[r]{1 \\ 0 \\ 0}
             =\colvec{x+1-1 \\ y+0 \\ z+0}
             =\colvec{x \\ y \\ z}
           \end{equation*}
           and the additive inverse is similar.
           \begin{equation*}
             \colvec{x \\ y \\ z}+\colvec{-x+2 \\ -y \\ -z}
             =\colvec{x+(-x+2)-1 \\ y-y \\ z-z}
             =\colvec[r]{1 \\ 0 \\ 0}
           \end{equation*}

           The conditions on scalar multiplication are also easy.
           For the first condition,
           \begin{equation*}
             (r+s)\colvec{x \\ y \\ z}
             =\colvec{(r+s)x-(r+s)+1 \\ (r+s)y \\ (r+s)z}
           \end{equation*}
           while
           \begin{multline*}
             r\colvec{x \\ y \\ z}+s\colvec{x \\ y \\ z}
             =\colvec{rx-r+1 \\ ry \\ rz}+\colvec{sx-s+1 \\ sy \\ sz}  \\
             =\colvec{(rx-r+1)+(sx-s+1)-1 \\ ry+sy \\ rz+sz}
           \end{multline*}
           and the two are equal.
           The second condition compares
           \begin{equation*}
             r\cdot(\colvec{x_1 \\ y_1 \\ z_1}+\colvec{x_2 \\ y_2 \\ z_2})
             =r\cdot\colvec{x_1+x_2-1 \\ y_1+y_2 \\ z_1+z_2}
             =\colvec{r(x_1+x_2-1)-r+1 \\ r(y_1+y_2) \\ r(z_1+z_2)}
           \end{equation*}
           with
           \begin{multline*}
             r\colvec{x_1 \\ y_1 \\ z_1}+r\colvec{x_2 \\ y_2 \\ z_2}
             =\colvec{rx_1-r+1 \\ ry_1 \\ rz_1}
                     +\colvec{rx_2-r+1 \\ ry_2 \\ rz_2}                  \\
             =\colvec{(rx_1-r+1)+(rx_2-r+1)-1 \\ ry_1+ry_2 \\ rz_1+rz_2}
           \end{multline*}
           and they are equal.
           For the third condition,
           \begin{equation*}
             (rs)\colvec{x \\ y \\ z}
             =\colvec{rsx-rs+1 \\ rsy \\ rsz}
           \end{equation*}
           while
           \begin{equation*}
             r(s\colvec{x \\ y \\ z})
             =r(\colvec{sx-s+1 \\ sy \\ sz})
             =\colvec{r(sx-s+1)-r+1 \\ rsy \\ rsz}
           \end{equation*}
           and the two are equal.
           For scalar multiplication by $1$ we have this.
           \begin{equation*}
             1\cdot\colvec{x \\ y \\ z}
             =\colvec{1x-1+1 \\ 1y \\ 1z}
             =\colvec{x \\ y \\ z}
           \end{equation*}
           Thus all the conditions on a vector space are met by these two
           operations.

           \textit{Remark.}
           A way to understand this vector space is to think of it as 
           the plane in $\Re^3$
           \begin{equation*}
             P=\set{\colvec{x \\ y \\ z}\suchthat x+y+z=0}
           \end{equation*}
           displaced away from the origin by $1$ along the $x$-axis.
           Then addition becomes:~to add two members of this space, 
           \begin{equation*}
             \colvec{x_1 \\ y_1 \\ z_1},\;\colvec{x_2 \\ y_2 \\ z_2}
           \end{equation*}
           (such that $x_1+y_1+z_1=1$ and $x_2+y_2+z_2=1$)
           move them back by $1$ to place them in $P$ and
           add as usual,
           \begin{equation*}
             \colvec{x_1-1 \\ y_1 \\ z_1}+\colvec{x_2-1 \\ y_2 \\ z_2}
             =\colvec{x_1+x_2-2 \\ y_1+y_2 \\ z_1+z_2}
             \qquad\text{(in $P$)}
           \end{equation*}
           and then move the result back out by $1$ along the $x$-axis.
           \begin{equation*}
             \colvec{x_1+x_2-1 \\ y_1+y_2 \\ z_1+z_2}.
           \end{equation*}
           Scalar multiplication is similar.
         \partsitem For the subspace to be closed under the inherited scalar 
           multiplication, where $\vec{v}$ is a member of that subspace,
           \begin{equation*}
             0\cdot\vec{v}=\colvec[r]{0 \\ 0 \\ 0}
           \end{equation*}
           must also be a member.

           The converse does not hold.
           Here is a subset of $\Re^3$ that contains the origin 
           \begin{equation*}
             \set{\colvec[r]{0 \\ 0 \\ 0},\colvec[r]{1 \\ 0 \\ 0}}
           \end{equation*}
           (this subset has only two elements) but is not a subspace.
      \end{exparts}
    \end{answer}
  \item 
    We can give a justification for the convention that the sum of 
    zero-many vectors equals the zero vector.
    Consider this sum of three vectors $\vec{v}_1+\vec{v}_2+\vec{v}_3$.
    \begin{exparts}
      \partsitem What is the difference between this sum of three vectors 
        and the sum of the first two of these three?
      \partsitem What is the difference between the prior sum and the sum 
        of just the first one vector?
      \partsitem What should be the difference between the prior sum of 
        one vector and the sum of no vectors?
      \partsitem So what should be the definition of the sum of no vectors?
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \item $(\vec{v}_1+\vec{v}_2+\vec{v}_3)-(\vec{v}_1+\vec{v}_2)
                 =\vec{v}_3$
        \item $(\vec{v}_1+\vec{v}_2)-(\vec{v}_1)
                 =\vec{v}_2$
        \item Surely, $\vec{v}_1$.
        \item Taking the one-long sum and subtracting gives
          ($\vec{v}_1)-\vec{v}_1=\zero$.
      \end{exparts}
    \end{answer}
  \item 
    Is a space determined by its subspaces?
    That is, if two vector spaces have the same subspaces, must the
    two be equal?
    \begin{answer}
      Yes; any space is a subspace of itself, so each space contains the
      other.  
    \end{answer}
  \item 
     \begin{exparts}
      \partsitem Give a set that is closed under scalar multiplication
        but not addition.
      \partsitem Give a set closed under addition but not scalar
        multiplication.
      \partsitem Give a set closed under neither.
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem The union of the \( x \)-axis and the \( y \)-axis
          in \( \Re^2 \) is one.
        \partsitem The set of integers, as a subset of \( \Re^1 \), is one.
        \partsitem The subset \( \set{\vec{v}} \) of \( \Re^2 \) is one,
          where $\vec{v}$ is any nonzero vector.
      \end{exparts}  
     \end{answer}
  \item 
    Show that the span of a set of vectors does not depend on the order in
    which the vectors are listed in that set.
    \begin{answer}
      Because vector space addition is commutative, a reordering of
      summands leaves a linear combination unchanged.  
    \end{answer}
  \item  
    Which trivial subspace is the span of the empty set?
    Is it
    \begin{equation*}
      \set{\colvec[r]{0 \\ 0 \\ 0}}\subseteq \Re^3,
      \quad\text{or}\quad
      \set{0+0x}\subseteq \polyspace_1,
    \end{equation*}
    or some other subspace?
    \begin{answer}
      We always consider that span in the context of an enclosing space.  
    \end{answer}
  \item   
    Show that if a  vector is in the span of a set then adding that
    vector to the set won't make the span any bigger.
    Is that also `only if'?
    \begin{answer}
      It is both `if' and `only if'.
 
      For `if',
      let \( S \) be a subset of a vector space \( V \) and assume
      \( \vec{v}\in S \) satisfies
      \( \vec{v}=c_1\vec{s}_1+\dots+c_n\vec{s}_n \) where
      \( c_1,\ldots,c_n \) are scalars and
      \( \vec{s}_1,\ldots,\vec{s}_n\in S \).
      We must show that \( \spanof{S\union\set{\vec{v}} }=\spanof{S} \).

      Containment one way,
      \( \spanof{S}\subseteq\spanof{S\union\set{\vec{v}} } \) is obvious.
      For the other direction,
      \( \spanof{S\union\set{\vec{v}} }\subseteq\spanof{S} \), note that if a
      vector is in the set on the left then it has the form
      \( d_0\vec{v}+d_1\vec{t}_1+\dots+d_m\vec{t}_m \) where the \( d \)'s are
      scalars and the \( \vec{t}\, \)'s are in \( S \).
      Rewrite that as
      \( d_0(c_1\vec{s}_1+\dots+c_n\vec{s}_n)
      +d_1\vec{t}_1+\cdots+d_m\vec{t}_m \) and note that 
      the result is a member of the span of \( S \).

      The `only if' is clearly true\Dash adding \( \vec{v} \) 
      enlarges the span to
      include at least \( \vec{v} \).
    \end{answer}
  \recommended \item
    Subspaces are subsets and so we naturally consider how `is a subspace of'
    interacts with the usual set operations.
    \begin{exparts}
      \partsitem If \( A,B \) are subspaces of a vector space, must
        their intersection
        \( A\intersection B \) be a subspace?
        Always?  Sometimes?  Never?
      \partsitem Must the union \( A\union B \) be a subspace?
      \partsitem If \( A \) is a subspace, must
        its complement be a subspace?
    \end{exparts}
    (\textit{Hint.}   Try some test subspaces from 
    \nearbyexample{ex:SubspRThree}.)
    \begin{answer}
      \begin{exparts}
        \partsitem Always.

          Assume that \( A,B \) are subspaces of \( V \).
          Note that 
          their intersection is not empty as both contain the zero vector.
          If \( \vec{w},\vec{s}\in A\intersection B \) and \( r,s \) are
          scalars then \( r\vec{v}+s\vec{w}\in A \) because
          each vector is in \( A \) and so a linear combination is in \( A \),
          and \(r\vec{v}+s\vec{w}\in B \) for the same reason.
          Thus the intersection is closed.
          Now \nearbylemma{th:SubspIffClosed} applies.
        \partsitem Sometimes (more precisely, only if \( A\subseteq B \) or
          \( B\subseteq A \)).

          To see the answer is not `always', take \( V \) to be \( \Re^3 \),
          take \( A \) to be the $x$-axis, and \( B \) to be the
          \( y \)-axis.
          Note that
          \begin{equation*}
            \colvec[r]{1 \\ 0}\in A \text{ and }\colvec[r]{0 \\ 1}\in B
            \quad\text{but}\quad
            \colvec[r]{1 \\ 0}+\colvec[r]{0 \\ 1}\not\in A\union B
          \end{equation*}
          as the sum is in neither \( A \) nor \( B \).

          The answer is not `never' because if \( A\subseteq B \) or
          \( B\subseteq A \) then clearly \( A\union B \) is a subspace.

          To show that \( A\union B \) is a subspace only if one
          subspace contains the other, we assume that \( A\not\subseteq B \)
          and \( B\not\subseteq A \) and prove that 
          the union is not a subspace.
          The assumption that \( A \) is not a subset of \( B \) means that 
          there is an \( \vec{a}\in A \) with \( \vec{a}\not\in B \).
          The other assumption gives a \( \vec{b}\in B \) with
          \( \vec{b}\not\in A \).
          Consider \( \vec{a}+\vec{b} \).
          Note that sum is not an element of \( A \) or else
          \( (\vec{a}+\vec{b})-\vec{a} \) would be in \( A \), which it is not.
          Similarly the sum is not an element of \( B \).
          Hence the sum is not an element of \( A\union B \), and so the union
          is not a subspace.
        \partsitem Never.
          As \( A \) is a subspace, it contains the zero vector, and therefore
          the set that is $A$'s complement does not.
          Without the zero vector, the complement cannot be a vector space.
      \end{exparts}  
    \end{answer}
  \recommended \item 
    Does the span of a set depend on the enclosing space?
    That is, if \( W \) is a subspace of \( V \) and \( S \) is a subset of
    \( W \) (and so also a subset of \( V \)), might the span of \( S \) in
    \( W \) differ from the span of \( S \) in \( V \)?
    \begin{answer}
      The span of a set does not depend on the enclosing space.
      A linear combination of vectors from \( S \) gives the same sum
      whether we regard the operations as those of \( W \) or as those of
      \( V \), because the operations of \( W \) are inherited from \(  V \).  
    \end{answer}
  \item 
    Is the relation `is a subspace of' transitive?
    That is, if $V$ is a subspace of $W$ and $W$ is a subspace of
    $X$, must $V$ be a subspace of $X$? 
    \begin{answer}
      It is;
      apply \nearbylemma{th:SubspIffClosed}.
      (You must consider the following.
      Suppose \( B \) is a subspace of a vector space \( V \) and suppose
      \( A\subseteq B\subseteq V \) is a subspace.
      From which space does \( A \) inherit its operations?
      The answer is that it doesn't matter\Dash \( A \) will inherit the
      same operations in either case.)  
    \end{answer}
  \recommended \item
    Because `span of' is an operation on sets we naturally consider
    how it interacts with the usual set operations.
    \begin{exparts}
      \partsitem If \( S\subseteq T \) are subsets of a vector space, is
        \( \spanof{S}\subseteq\spanof{T} \)?
        Always?  Sometimes?  Never?
      \partsitem If \( S,T \) are subsets of a vector space, is
        \( \spanof{S\union T}=\spanof{S}\union\spanof{T} \)?
      \partsitem If \( S,T \) are subsets of a vector space, is
        \( \spanof{S\intersection T}=\spanof{S}\intersection\spanof{T} \)?
      \partsitem Is the span of the complement equal to the complement of
        the span?
    \end{exparts}
    \begin{answer}
      \begin{exparts}
         \partsitem Always;
           if \( S\subseteq T \) then a linear combination of elements of
           \( S \) is also a linear combination of elements of \( T \).
         \partsitem Sometimes (more precisely, if and only if 
           \( S\subseteq T \) or \( T\subseteq S \)).

           The answer is not `always' as is shown by this example from
           \( \Re^3 \)
           \begin{equation*}
             S=\set{\colvec[r]{1 \\ 0 \\ 0},\colvec[r]{0 \\ 1 \\ 0}},\quad
             T=\set{\colvec[r]{1 \\ 0 \\ 0},\colvec[r]{0 \\ 0 \\ 1}}
           \end{equation*}
           because of this.
           \begin{equation*}
             \colvec[r]{1 \\ 1 \\ 1}\in\spanof{S\union T}
             \qquad
             \colvec[r]{1 \\ 1 \\ 1}\not\in\spanof{S}\union \spanof{T}
           \end{equation*}

           The answer is not `never' because if either set contains the other
           then equality is clear.
           We can
           characterize equality as happening only when either set contains
           the other by assuming \( S\not\subseteq T \) (implying the
           existence of a vector \( \vec{s}\in S \) with 
           \( \vec{s}\not\in T \))
           and \( T\not\subseteq S \) (giving a \( \vec{t}\in T \) with
           \( \vec{t}\not\in S \)), noting
           \( \vec{s}+\vec{t}\in\spanof{S\union T} \),
           and showing that 
           \( \vec{s}+\vec{t}\not\in\spanof{S}\union\spanof{T} \).
         \partsitem Sometimes.

           Clearly
           \( \spanof{S\intersection T}
             \subseteq\spanof{S}\intersection\spanof{T} \)
           because any linear combination of vectors from 
           \( S\intersection T \)
           is a combination of vectors from \( S \) and also a combination of
           vectors from \( T \).

           Containment the other way does not always hold.
           For instance, in \( \Re^2 \), take
           \begin{equation*}
             S=\set{\colvec[r]{1 \\ 0},\colvec[r]{0 \\ 1}},\quad
             T=\set{\colvec[r]{2 \\ 0}}
           \end{equation*}
           so that \( \spanof{S}\intersection\spanof{T} \) is the \( x \)-axis
           but \( \spanof{S\intersection T}  \) is the trivial subspace.

           Characterizing exactly when equality holds is tough.
           Clearly equality holds if either set contains the other, but that is
           not `only if' by this example in \( \Re^3 \).
           \begin{equation*}
             S=\set{\colvec[r]{1 \\ 0 \\ 0},\colvec[r]{0 \\ 1 \\ 0}},
             \quad
             T=\set{\colvec[r]{1 \\ 0 \\ 0},\colvec[r]{0 \\ 0 \\ 1}}
           \end{equation*}
        \partsitem Never, as the span of the complement is a subspace, while
          the complement of the span is not (it does not contain the zero 
          vector).
      \end{exparts}  
     \end{answer}
  \item 
    Reprove \nearbylemma{le:SpanIsASubsp} without doing the
    empty set separately.
    \begin{answer}
      Call the subset \( S \).
      By \nearbylemma{th:SubspIffClosed},
      we need to check that 
      \( \spanof{S} \) is closed under linear combinations.
      If \( c_1\vec{s}_1+\dots+c_n\vec{s}_n,
        c_{n+1}\vec{s}_{n+1}+\dots+c_m\vec{s}_m\in\spanof{S} \) then
      for any \( p,r\in\Re \) we have
      \begin{multline*}
        p\cdot(c_1\vec{s}_1+\cdots+c_n\vec{s}_n)+
             r\cdot(c_{n+1}\vec{s}_{n+1}+\cdots+c_m\vec{s}_m)        \\
        =
        pc_1\vec{s}_1+\cdots+pc_n\vec{s}_n
          +rc_{n+1}\vec{s}_{n+1}+\cdots+rc_m\vec{s}_m
      \end{multline*}
      which is an element of \( \spanof{S} \).
      % (\textit{Remark.}
      % If the set $S$ is empty, then that
      % `if \ldots\ then \ldots' statement is vacuously true.)  
    \end{answer}
  \item Find a structure that is closed under linear combinations, 
    and yet is not a vector space.
    % (\textit{Remark.} 
    % This is a bit of a trick question.)
    \begin{answer}
      For this to happen, one of the conditions giving the sensibleness of the
      addition and scalar multiplication operations must be violated.
      Consider \( \Re^2 \) with these operations.
      \begin{equation*}
        \colvec{x_1 \\ y_1}
        +\colvec{x_2 \\ y_2}
        =\colvec[r]{0 \\ 0}
        \qquad
        r\colvec{x \\ y}
        =\colvec[r]{0 \\ 0}
      \end{equation*}
      The set $\Re^2$ is closed under these operations. 
      But it is not a vector space.
      \begin{equation*}
        1\cdot\colvec[r]{1 \\ 1}
        \neq\colvec[r]{1 \\ 1}
      \end{equation*}  
    \end{answer}
\index{subspace|)}
\index{vector space|)}
\end{exercises}
