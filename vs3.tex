% Chapter 2, Section 3 _Linear Algebra_ Jim Hefferon
%  http://joshua.smcvt.edu/linearalgebra
%  2001-Jun-10
\section{Basis and Dimension}
\index{basis|(}
The prior section ends with the observation that
a spanning set is minimal when it is linearly independent and
a linearly independent set is maximal when it spans the space.
So the notions of minimal spanning set and maximal independent set 
coincide.
In this section we will name this idea and study its properties.





\subsection{Basis}
\begin{definition} \label{def:basis}
%<*df:basis>
A \definend{basis}\index{basis!definition}\index{vector space!basis} 
for a vector
space is a sequence of vectors that is linearly independent 
and that spans the space.
%</df:basis>
\end{definition}

%<*BasisNotation>
Because a basis is a 
sequence, 
meaning that 
bases are different if they contain the same elements but in different
orders,
% the 
% order of the elements is significant,
we denote it with angle brackets
\( \sequence{\vec{\beta}_1,\vec{\beta}_2,\ldots} \).\appendrefs{sequences}\@ %\spacefactor=1000{}
%</BasisNotation>
(A sequence is linearly independent if the multiset
consisting of the elements of the sequence in is independent.
Similarly, 
a sequence spans the space if the set of elements 
of the sequence spans the space.)

\begin{example}  \label{ex:FstBasisRTwo}
This is a basis for \( \Re^2 \).
\begin{equation*}
  \sequence{ \colvec{2 \\ 4},\colvec{1 \\ 1} }
\end{equation*}
It is linearly independent
\begin{equation*}
  c_1\colvec{2 \\ 4}+c_2\colvec{1 \\ 1}=\colvec{0 \\ 0}
  \quad\implies\quad
  \begin{linsys}{2}
     2c_1  &+  &1c_2  &=  &0  \\
     4c_1  &+  &1c_2  &=  &0  
   \end{linsys}
  \quad\implies\quad
  c_1=c_2=0
\end{equation*}
and it spans \( \Re^2 \).
\begin{equation*}
  \begin{linsys}{2}
    2c_1  &+  &1c_2  &=  &x  \\
    4c_1  &+  &1c_2  &=  &y  
  \end{linsys}
  \quad\implies\quad
  c_2=2x-y\text{\ and\ } c_1=(y-x)/2
\end{equation*}
\end{example}

\begin{example}
This basis for \( \Re^2 \) differs from the prior one 
\begin{equation*}
  \sequence{\colvec[r]{1 \\ 1},\colvec[r]{2 \\ 4}}
\end{equation*}
because it is in a different order.
The verification that it is a basis is just as in the prior example.
\end{example}

\begin{example}
The space \( \Re^2 \) has many bases.
Another one is this.
\begin{equation*}
  \sequence{ \colvec[r]{1 \\ 0},\colvec[r]{0 \\ 1} }
\end{equation*}
The verification is easy.
\end{example}

\begin{definition} \label{df:StandardBasis}
%<*df:StandardBasis>
For any \( \Re^n \)
\begin{equation*}
   \stdbasis_n=\sequence{
     \colvec[r]{1 \\ 0 \\ \vdotswithin{0} \\ 0},
     \colvec[r]{0 \\ 1 \\ \vdotswithin{0} \\ 0},
     \dots,\,
     \colvec[r]{0 \\ 0 \\ \vdotswithin{0} \\ 1}}
\end{equation*}
is the \definend{standard}\index{standard basis}%
\index{basis!standard} (or \definend{natural}) basis.
We denote these vectors \( \vec{e}_1,\dots,\vec{e}_n \).
%</df:StandardBasis>
\end{definition}

\noindent 
Calculus books denote $\Re^2$'s standard basis vectors as
\( \vec{\imath} \) and \( \vec{\jmath} \) instead of $\vec{e}_1$
and $\vec{e}_2$ and they denote to 
\( \Re^3 \)'s standard basis vectors as
\( \vec{\imath} \), \( \vec{\jmath} \), and \( \vec{k} \)
instead of $\vec{e}_1$, $\vec{e}_2$, and $\vec{e}_3$.
Note that \( \vec{e}_1 \) means something different in a
discussion of \( \Re^3 \) than it means in a discussion of \( \Re^2 \).

\begin{example}  \label{ex:BasisForCosPlusSin}
Consider the space
\( \set{a\cdot\cos\theta+b\cdot\sin\theta\suchthat a,b\in\Re} \)
of functions of the real variable $\theta$.
This is a natural basis 
$ \sequence{\cos\theta, \sin\theta}=\sequence{1\cdot\cos\theta+0\cdot\sin\theta,
             0\cdot\cos\theta+1\cdot\sin\theta}
    $.
A more generic basis for this space is
\( \sequence{\cos\theta-\sin\theta,
             2\cos\theta+3\sin\theta} \).
Verification that these two are bases is
\nearbyexercise{exer:VerifBasesCosPlusSin}.
\end{example}

\begin{example}
A natural basis for the vector space of cubic polynomials \( \polyspace_3 \)
is \( \sequence{1,x,x^2,x^3} \).
Two other bases for this space are \( \sequence{x^3,3x^2,6x,6} \)
and \( \sequence{1,1+x,1+x+x^2,1+x+x^2+x^3} \).
Checking that each is linearly independent and spans the space is easy.
\end{example}

\begin{example}
The trivial space\index{trivial space}\index{vector space!trivial} 
$\set{\zero}$ has only one basis, the empty one
\( \sequence{} \).
\end{example}

\begin{example}
The space of finite-degree polynomials has a basis with infinitely many
elements
\( \sequence{1,x,x^2,\ldots} \).
\end{example}

\begin{example}
We have seen bases before.
In the first chapter we described the solution set of homogeneous systems
such as this one
\begin{equation*}
   \begin{linsys}{4}
      x  &+  &y  &   &   &-   &w   &=  &0  \\
         &   &   &   &z  &+   &w   &=  &0  
   \end{linsys}
\end{equation*}
by parametrizing.
\begin{equation*}
  \set{\colvec[r]{-1 \\ 1 \\ 0 \\ 0}y
       +\colvec[r]{1 \\ 0 \\ -1 \\ 1}w
       \suchthat y,w\in\Re }
\end{equation*}
Thus the vector space of solutions is 
the span of a two-element set.
This two-vector set is also linearly independent, which is easy to check.
Therefore the solution set is a subspace of \( \Re^4 \) with a
basis comprised of these two vectors.
\end{example}

\begin{example}
Parametrization finds bases for other vector spaces, not just
for solution sets of homogeneous systems.
To find a basis for this subspace of $\matspace_{\nbyn{2}}$
\begin{equation*}
  \set{\begin{mat}
         a  &b  \\
         c  &0
       \end{mat} \suchthat a+b-2c=0}
\end{equation*}
we rewrite the condition as $a=-b+2c$.
\begin{equation*}
  \set{\begin{mat}
         -b+2c  &b  \\
          c     &0
       \end{mat} \suchthat b,c \in \Re}
  =\set{b\begin{mat}[r]
         -1  &1  \\
          0  &0
       \end{mat}+
       c\begin{mat}[r]
          2  &0  \\
          1  &0
       \end{mat} \suchthat b,c \in \Re}
\end{equation*}
Thus, this is a natural candidate for a basis.
\begin{equation*}
  \sequence{\begin{mat}[r]
         -1  &1  \\
          0  &0
       \end{mat},
       \begin{mat}[r]
          2  &0  \\
          1  &0
       \end{mat} }
\end{equation*}
The above work shows that it spans the space.
Linear independence is also easy.
\end{example}

Consider again \nearbyexample{ex:FstBasisRTwo}.
% To verify linearly independence we looked at 
% linear combinations of the set's members that total to
% the zero vector 
% $c_1\vec{\beta}_1+c_2\vec{\beta}_2=\binom{0}{0}$.
% The resulting 
% calculation shows that such a combination is unique,
% that $c_1$ must be $0$ and $c_2$ must be $0$.
To verify that the set spans the space we
looked at linear combinations that total to a
member of the space
$c_1\vec{\beta}_1+c_2\vec{\beta}_2=\binom{x}{y}$.
We only noted in that example that such a combination 
exists, that for each $x,y$ there exists a $c_1,c_2$, but
in fact the calculation also shows that the combination is 
unique:~$c_1$ must be $(y-x)/2$ and $c_2$ must be $2x-y$.
 
\begin{theorem} \label{th:BasisIffUniqueRepWRT}
%<*th:BasisIffUniqueRepWRT>
In any vector space, a subset is a basis\index{basis}
if and only if each vector in the
space can be expressed as a linear combination of elements of the subset 
in one and only one way.
%</th:BasisIffUniqueRepWRT>
\end{theorem}

\noindent We consider linear combinations to be the same if they have the 
same summands but in a different order,
or if they differ only in the addition or deletion of terms of the form 
`\( 0\cdot\vec{\beta} \)'.


\begin{proof}
%<*pf:BasisIffUniqueRepWRT0>
A sequence is a basis if and only if its vectors form a set
that spans and that is linearly independent.
A subset is a spanning set if and only if each vector in the space is a linear
combination of elements of that subset in at least one way.
Thus we need only show 
that a spanning subset is linearly independent 
if and only if every vector in the space
is a linear combination of elements from the subset in at most one way.
%</pf:BasisIffUniqueRepWRT0>

%<*pf:BasisIffUniqueRepWRT1>
Consider two expressions of a vector as a linear combination of the
members of the subset.
Rearrange the two sums, and if necessary
add some \( 0\cdot\vec{\beta}_i \) terms, so that the two sums
combine the same \( \vec{\beta} \)'s in the same order:
\( \vec{v}=\lincombo{c}{\vec{\beta}} \) and 
\( \vec{v}=\lincombo{d}{\vec{\beta}} \). 
Now
\begin{equation*}
   \lincombo{c}{\vec{\beta}}=\lincombo{d}{\vec{\beta}}
\end{equation*}
holds if and only if
\begin{equation*}
   (c_1-d_1)\vec{\beta}_1+\dots+(c_n-d_n)\vec{\beta}_n=\zero
\end{equation*}
holds. 
So, asserting that 
each coefficient in the lower equation is zero is 
the same thing as asserting that \( c_i=d_i \) for each \( i \),
that is, that every vector is expressible as a linear combination of
the \( \vec{\beta} \)'s in a unique way.
%</pf:BasisIffUniqueRepWRT1>
\end{proof}

\begin{definition} \label{def:RepresentingVectors}
%<*df:RepresentingVectors>
In a vector space with basis $B$
the \definend{representation of \( \vec{v} \) with respect to \( B \)}%
\index{representation!of a vector}\index{vector!representation of} is
the column vector of the coefficients used to express $\vec{v}$ as a 
linear combination of the basis vectors:
\begin{equation*}
  \rep{\vec{v}}{B}
  =
  \colvec{c_1 \\ c_2 \\ \vdots \\ c_n}
\end{equation*}
where
\( B=\sequence{\vec{\beta}_1,\dots,\vec{\beta}_n} \) and 
\( \vec{v}=\lincombo{c}{\vec{\beta}} \).
The \( c \)'s are the
\definend{coordinates of \( \vec{v} \) with respect to \( B \)}%
\index{coordinates!with respect to a basis}.
%</df:RepresentingVectors>
\end{definition}

\begin{example}
In \( \polyspace_3 \), with respect to the basis
\( B=\sequence{1,2x,2x^2,2x^3} \),
the representation of \( x+x^2 \) is
\begin{equation*}
  \rep{x+x^2}{B}=\colvec[r]{0 \\ 1/2 \\ 1/2 \\ 0}_B
\end{equation*}
because $x+x^2=0\cdot 1+(1/2)\cdot 2x+(1/2)\cdot 2x^2+0\cdot 2x^3$.
With respect to a different basis \( D=\sequence{1+x,1-x,x+x^2,x+x^3} \),
the representation is different.
\begin{equation*}
  \rep{x+x^2}{D}=\colvec[r]{0 \\ 0 \\ 1 \\ 0}_D
\end{equation*}
(When there is more than one basis around,
to help keep straight which representation is with respect to which 
basis we often write it as a subscript on the column vector.)
\end{example}

\begin{remark}
\nearbydefinition{def:basis} requires that a basis be a sequence
because
without that we couldn't write these coordinates in a fixed order. 
\end{remark}

\begin{example}
In \( \Re^2 \), where $\vec{v}=\binom{3}{2}$, to find
the coordinates of that vector with respect to the basis
\begin{equation*}
  B=\sequence{
              \colvec[r]{1 \\ 1},
              \colvec[r]{0 \\ 2} }
\end{equation*}
we solve
\begin{equation*}
  c_1\colvec[r]{1 \\ 1}
  +c_2\colvec[r]{0 \\ 2}
  =
  \colvec[r]{3 \\ 2}
\end{equation*}
and get that $c_1=3$ and $c_2=-1/2$.
\begin{equation*}
  \rep{\vec{v}}{B}=\colvec[r]{3 \\ -1/2}
\end{equation*}  
\end{example}

\begin{remark}
This use of column notation and the term `coordinate' has both a disadvantage
and an advantage.
The disadvantage is that
representations look like vectors from \( \Re^n \), which can be
confusing when the vector space is $\Re^n$, as in the prior example.
We must infer the intent from the context.
For example, the phrase `in \( \Re^2 \), where $\vec{v}=\binom{3}{2}$'
refers to the plane vector that, when in canonical position, ends at
\( (3,2) \).
And in the end of that example,
although we've omitted a subscript \( B \) from the column, 
that the right side is a representation is clear from the context.

The advantage of the notation and the term is that they
generalize the familiar case:~in \( \Re^n \) 
and with respect to the standard
basis \( \stdbasis_n \), the vector starting at the origin and ending at
\( (v_1,\dots,v_n) \) has this representation.
\begin{equation*}
  \rep{\colvec{v_1 \\ \vdots \\ v_n}}{\stdbasis_n}
    =
  \colvec{v_1 \\ \vdots \\ v_n}_{\stdbasis_n}
\end{equation*}
\end{remark}

Our main use of representations will come later but
the definition appears here because the fact that every vector is a linear
combination of basis vectors in a unique way is a crucial property of bases,
and also to help make a point.
For calculation of coordinates among other things, we shall
restrict our attention to spaces with bases having only finitely many elements.
That will start in the next subsection.

\begin{exercises}
  \item Decide if each is a basis for $\polyspace_2$.
    \begin{exparts*}
      \partsitem $\sequence{x^2-x+1, 2x+1, 2x-1}$
      \partsitem $\sequence{x+x^2, x-x^2}$
    \end{exparts*}
    \begin{answer}
      \begin{exparts}
        \partsitem This is a basis for $\polyspace_2$.
           To show that it spans the space we consider a generic
           $a_2x^2+a_1x+a_0\in\polyspace_2$ and look for 
           scalars $c_1, c_2, c_3\in\Re$ such that
           $a_2x^2+a_1x+a_0=c_1\cdot(x^2-x+1) +c_2\cdot(2x+1) +c_3(2x-1)$.
           Gauss's Method on the linear system
           \begin{equation*}
             \begin{linsys}{3}
               c_1  &\spaceforemptycolumn  &     &  &     &=  &a_2 \\
                    &  &2c_2 &+ &2c_3 &=  &a_1 \\
                    &  &c_2  &- &c_3   &=  &a_0
             \end{linsys}
           \end{equation*}
           shows that given the $a_i$'s we can compute the $c_j$'s as
           $c_1=a_2$, $c_2=(1/4)a_1+(1/2)a_0$, and $c_3=(1/4)a_1-(1/2)a_0$.
           Thus each element of $\polyspace_2$ is a combination of the 
           given three.

           To prove that the set of the given three is linearly independent
           we can set up the equation 
           $0x^2+0x+0=c_1\cdot(x^2-x+1) +c_2\cdot(2x+1) +c_3(2x-1)$
           and solve, and it will give that $c_1=0$, $c_2=0$, and $c_3=0$.
           Or, we can instead
           observe that the solution in the prior paragraph is
           unique, and cite \nearbytheorem{th:BasisIffUniqueRepWRT}.
        \partsitem This is not a basis.
           It does not span the space since no combination of the two
           $c_1\cdot (x+x^2) +c_2\cdot (x-x^2)$ will sum to the polynomial 
           $3\in\polyspace_2$.  
      \end{exparts}
    \end{answer}
  \recommended \item 
    Decide if each is a basis for \( \Re^3 \).
    \begin{exparts*}
      \partsitem \( \sequence{
                 \colvec[r]{1 \\ 2 \\ 3},
                 \colvec[r]{3 \\ 2 \\ 1},
                 \colvec[r]{0 \\ 0 \\ 1}}  \)
      \partsitem \( \sequence{
                 \colvec[r]{1 \\ 2 \\ 3},
                 \colvec[r]{3 \\ 2 \\ 1}}  \)
      \partsitem \( \sequence{
                 \colvec[r]{0 \\ 2 \\ -1},
                 \colvec[r]{1 \\ 1 \\ 1},
                 \colvec[r]{2 \\ 5 \\ 0}}  \)
      \partsitem \( \sequence{
                 \colvec[r]{0 \\ 2 \\ -1},
                 \colvec[r]{1 \\ 1 \\ 1},
                 \colvec[r]{1 \\ 3 \\ 0}}  \)
    \end{exparts*}
    \begin{answer}
      By \nearbytheorem{th:BasisIffUniqueRepWRT}, each is a basis if and only
      if we can express each vector in the space in a unique way as a linear
      combination of the given vectors.
      \begin{exparts}
        \partsitem Yes this is a basis.
          The relation
          \begin{equation*}
            c_1\colvec[r]{1 \\ 2 \\ 3}
            +c_2\colvec[r]{3 \\ 2 \\ 1}
            +c_3\colvec[r]{0 \\ 0 \\ 1}
            =\colvec{x \\ y \\ z}
          \end{equation*}
          gives
          \begin{equation*}
            \begin{amat}{3}
              1  &3  &0  &x  \\
              2  &2  &0  &y  \\
              3  &1  &1  &z
            \end{amat}
            \grstep[-3\rho_1+\rho_3]{-2\rho_1+\rho_2}
            \repeatedgrstep{-2\rho_2+\rho_3}
            \begin{amat}{3}
              1  &3  &0  &x \hfill\hbox{}  \\
              0  &-4 &0  &-2x+y \hfill\hbox{}  \\
              0  &0  &1  &x-2y+z 
            \end{amat}
          \end{equation*}
          which has the unique solution
          \( c_3=x-2y+z \), \( c_2=x/2-y/4 \), and
          \( c_1=-x/2+3y/4 \).
        \partsitem This is not a basis.
          Setting it up as in the prior item
          \begin{equation*}
            c_1\colvec[r]{1 \\ 2 \\ 3}
            +c_2\colvec[r]{3 \\ 2 \\ 1}
            =\colvec{x \\ y \\ z}
          \end{equation*}
          gives a linear system whose solution
          \begin{equation*}
            \begin{amat}{2}
              1  &3  &x  \\ 
              2  &2  &y  \\
              3  &1  &z
            \end{amat}
            \grstep[-3\rho_1+\rho_3]{-2\rho_1+\rho_2}
            \repeatedgrstep{-2\rho_2+\rho_3}
            \begin{amat}{2}
              1  &3  &x\hfill\hbox{}  \\ 
              0  &-4 &-2x+y\hfill\hbox{}  \\
              0  &0  &x-2y+z
            \end{amat}
          \end{equation*}
          is possible if and only if the three-tall vector's components
          $x$, $y$, and $z$ satisfy $x-2y+z=0$.
          For instance, we can find the coefficients $c_1$ and $c_2$ that
          work when $x=1$, $y=1$, and $z=1$.
          However, there are no $c$'s that work for
          $x=1$, $y=1$, and $z=2$.
          Thus this is not a basis; it does not span the space.
        \partsitem Yes, this is a basis.
         Setting up the relationship leads to this reduction
          \begin{equation*}
            \begin{amat}{3}
              0  &1  &2  &x \hfill\hbox{} \\
              2  &1  &5  &y \hfill\hbox{} \\
             -1  &1  &0  &z 
            \end{amat}
            \grstep{\rho_1\swap\rho_3}
            \repeatedgrstep{2\rho_1+\rho_2}
            \repeatedgrstep{-(1/3)\rho_2+\rho_3}
            \begin{amat}{3}
             -1  &1  &0   &z  \hfill\hbox{} \\
              0  &3  &5   &y+2z \hfill\hbox{} \\
              0  &0  &1/3 &x-y/3-2z/3
            \end{amat}
          \end{equation*}
          which has a unique solution for each triple of components
          $x$, $y$, and $z$.
        \partsitem No, this is not a basis.
          The reduction  
          \begin{equation*}
            \begin{amat}{3}
              0  &1  &1  &x   \\
              2  &1  &3  &y   \\
             -1  &1  &0  &z  
            \end{amat}
            \grstep{\rho_1\swap\rho_3}
            \repeatedgrstep{2\rho_1+\rho_2}
            \repeatedgrstep{(-1/3)\rho_2+\rho_3}
            \begin{amat}{3}
             -1  &1  &0  &z  \hfill\hbox{} \\
              0  &3  &3  &y+2z  \hfill\hbox{} \\
              0  &0  &0  &x-y/3-2z/3
            \end{amat}
          \end{equation*}
          which does not have a solution for each triple $x$, $y$, and $z$.
          Instead, the span of the given set 
          includes only those three-tall vectors where \( x=y/3+2z/3 \).
      \end{exparts}  
     \end{answer}
  \recommended \item 
    Represent the vector with respect to the basis.
    \begin{exparts}
      \partsitem \( \colvec[r]{1 \\ 2} \),
        \( B=\sequence{\colvec[r]{1 \\ 1},\colvec[r]{-1 \\ 1}}\subseteq\Re^2 \)
      \partsitem \( x^2+x^3 \),
        \( D=\sequence{1,1+x,1+x+x^2,1+x+x^2+x^3}\subseteq\polyspace_3 \)
      \partsitem \( \colvec[r]{0 \\ -1 \\ 0 \\ 1} \),
        \( \stdbasis_4\subseteq\Re^4 \)
    \end{exparts}
    \begin{answer}  
      \begin{exparts}
        \partsitem We solve
          \begin{equation*}
            c_1\colvec[r]{1 \\ 1}
            +c_2\colvec[r]{-1 \\ 1}
            =\colvec[r]{1 \\ 2}
          \end{equation*}
          with
          \begin{equation*}
             \begin{amat}[r]{2}
               1  &-1  &1  \\
               1  &1   &2
             \end{amat}
             \grstep{-\rho_1+\rho_2}
             \begin{amat}[r]{2}
               1  &-1  &1  \\
               0  &2   &1
             \end{amat}
          \end{equation*}
          and conclude that \( c_2=1/2 \) and so \( c_1=3/2 \).
          Thus, the representation is this.
          \begin{equation*}
            \rep{\colvec[r]{1 \\ 2}}{B}=\colvec[r]{3/2 \\ 1/2}_B
          \end{equation*}
        \partsitem The relationship
           $c_1\cdot(1)+c_2\cdot(1+x)+c_3\cdot(1+x+x^2)+c_4\cdot(1+x+x^2+x^3)
             =x^2+x^3$
           is easily solved by eye to give that $c_4=1$, $c_3=0$, $c_2=-1$, and
           $c_1=0$.
           \begin{equation*}
              \rep{x^2+x^3}{D}=\colvec[r]{0 \\ -1 \\ 0 \\ 1}_D
           \end{equation*} 
        \partsitem \( \rep{\colvec[r]{0 \\ -1 \\ 0 \\ 1}}{\stdbasis_4}
                     =\colvec[r]{0 \\ -1 \\ 0 \\ 1}_{\stdbasis_4} \)
      \end{exparts}  
    \end{answer}
  \item  Represent the vector with respect to each of the two bases.
    \begin{equation*}
      \vec{v}=\colvec{3  \\ -1}
      \quad
      B_1=\sequence{\colvec{1  \\ -1}, \colvec{1  \\ 1}},\;
      B_2=\sequence{\colvec{1 \\ 2}, \colvec{1 \\ 3}}
    \end{equation*}
    \begin{answer}
       Solving
      \begin{equation*}
        \colvec{3 \\ -1}=\colvec{1 \\ -1}\cdot c_1
                         +\colvec{1 \\ 1}\cdot c_2
      \end{equation*}
      gives $c_1=2$ and~$c_2=1$. 
      \begin{equation*}
        \rep{\colvec{3 \\ -1}}{B_1}
           =\colvec{2 \\ 1}_{B_1}
      \end{equation*}
      Similarly, solving
      \begin{equation*}
        \colvec{3 \\ -1}=\colvec{1 \\ 2}\cdot c_1
                         +\colvec{1 \\ 3}\cdot c_2
      \end{equation*}
      gives this. 
      \begin{equation*}
        \rep{\colvec{3 \\ -1}}{B_2}
           =\colvec{10 \\ -7}_{B_2}
      \end{equation*}     
    \end{answer}
  \item  
    Find a basis for \(  \polyspace_2 \), the space of all quadratic
    polynomials.
    Must any such basis contain a polynomial of each degree:~degree zero, 
    degree one, and degree two?
    \begin{answer}
      A natural basis is \( \sequence{1,x,x^2} \).
      There are bases for $\polyspace_2$ that do not contain any polynomials 
      of degree one or degree zero.
      One is \( \sequence{1+x+x^2,x+x^2,x^2} \).
      (Every basis has at least one polynomial of degree two, though.)
    \end{answer}
  \item
    Find a basis for the solution set of this system.
    \begin{equation*}
      \begin{linsys}{4}
         x_1  &-  &4x_2  &+  &3x_3  &-  &x_4  &=  &0  \\
        2x_1  &-  &8x_2  &+  &6x_3  &-  &2x_4 &=  &0  
      \end{linsys}
    \end{equation*}
    \begin{answer}
      The reduction
      \begin{equation*}
        \begin{amat}[r]{4}
          1  &-4  &3  &-1  &0  \\
          2  &-8  &6  &-2  &0
        \end{amat}
        \grstep{-2\rho_1+\rho_2}
        \begin{amat}[r]{4}
          1  &-4  &3  &-1  &0  \\
          0  &0   &0  &0   &0
        \end{amat}
      \end{equation*}
      gives that the only condition is that 
      $x_1=4x_2-3x_3+x_4$.
      The solution set is
      \begin{multline*}
        \set{\colvec{4x_2-3x_3+x_4 \\ x_2 \\ x_3 \\ x_4}
               \suchthat x_2,x_3,x_4\in\Re}                          \\
        =\set{x_2\colvec[r]{4 \\ 1 \\ 0 \\ 0}
             +x_3\colvec[r]{-3 \\ 0 \\ 1 \\ 0}
             +x_4\colvec[r]{1 \\ 0 \\ 0 \\ 1} \suchthat x_2,x_3,x_4\in\Re}
      \end{multline*}
      and so the obvious candidate for the basis is this.
      \begin{equation*}
       \sequence{\colvec[r]{4 \\ 1 \\ 0 \\ 0},
                   \colvec[r]{-3 \\ 0 \\ 1 \\ 0},
                   \colvec[r]{1 \\ 0 \\ 0 \\ 1}  }
      \end{equation*}  
      We've shown that this spans the space, and showing it is also linearly
      independent is routine.
    \end{answer}
  \recommended \item
    Find a basis for \( \matspace_{\nbyn{2}} \),
    the space of \( \nbyn{2} \) matrices.
    \begin{answer}
      There are many bases.
      This is a natural one.
      \begin{equation*}
        \sequence{
           \begin{mat}[r]
             1  &0  \\
             0  &0
           \end{mat},
           \begin{mat}[r]
             0  &1  \\
             0  &0
           \end{mat},
           \begin{mat}[r]
             0  &0  \\
             1  &0
           \end{mat},
           \begin{mat}[r]
             0  &0  \\
             0  &1
           \end{mat}  }
      \end{equation*} 
    \end{answer}
  \recommended \item 
      Find a basis for each.
      \begin{exparts}
        \partsitem The subspace $\set{a_2x^2+a_1x+a_0\suchthat a_2-2a_1=a_0}$ 
          of $\polyspace_2$
        \partsitem The space of three-wide row vectors whose first and second
          components add to zero
        \partsitem This subspace of the $\nbyn{2}$ matrices
          \begin{equation*}
            \set{\begin{mat}
                   a  &b  \\
                   0  &c  
                 \end{mat} \suchthat c-2b=0}
          \end{equation*}
      \end{exparts}
      \begin{answer}
        For each item, many answers are possible.
        \begin{exparts}
          \partsitem One way to proceed is to parametrize by
            expressing the $a_2$ as a combination of the other two
            $a_2=2a_1+a_0$.
            Then
            $a_2x^2+a_1x+a_0$ is $(2a_1+a_0)x^2+a_1x+a_0$ and
            \begin{multline*} 
              \set{(2a_1+a_0)x^2+a_1x+a_0\suchthat a_1,a_0\in\Re}          \\
              =\set{a_1\cdot(2x^2+x)+a_0\cdot(x^2+1)\suchthat a_1,a_0\in\Re}
            \end{multline*}
            suggests 
            $\sequence{2x^2+x,x^2+1}$.
            This only shows that 
            it spans, but checking that it is linearly independent
            is routine.
          \partsitem Parametrize $\set{\rowvec{a  &b  &c}\suchthat a+b=0}$ 
            to get $\set{\rowvec{-b &b &c}\suchthat b,c\in\Re}$, which suggests
            using the sequence $\sequence{\rowvec{-1 &1 &0},\rowvec{0 &0 &1}}$.
            We've shown that it spans, and checking that it is linearly 
            independent is easy.
          \partsitem Rewriting
            \begin{equation*}
              \set{\begin{mat}
                     a  &b  \\
                     0  &2b
                   \end{mat}\suchthat a,b\in\Re}
              =\set{a\cdot\begin{mat}[r]
                     1  &0  \\
                     0  &0 
                   \end{mat}
                  +b\cdot\begin{mat}[r]
                           0  &1  \\
                           0  &2
                          \end{mat} \suchthat a,b\in\Re}
            \end{equation*}
            suggests this for the basis.
            \begin{equation*}
              \sequence{\begin{mat}[r]
                          1  &0  \\
                          0  &0
                        \end{mat},
                        \begin{mat}[r]
                          0  &1  \\
                          0  &2
                        \end{mat}}
            \end{equation*}
         \end{exparts}  
      \end{answer}
  \item Find a basis for each space, and verify that it is a basis.
    \begin{exparts}
      \partsitem
        The subspace $M=\set{a+bx+cx^2+dx^3\suchthat a-2b+c-d=0}$ 
        of $\polyspace_3$.
      \partsitem  This subspace of $\matspace_{\nbyn{2}}$.
         \begin{equation*}
           W=\set{
             \begin{mat}
               a  &b  \\
               c  &d
             \end{mat}
             \suchthat a-c=0}
         \end{equation*}

    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem
          Parametrize $a-2b+c-d=0$ as $a=2b-c+d$, $b=b$, $c=c$, and~$d=d$ to get
          this description of $M$ as the span of a set of three vectors.
          \begin{equation*}
            M=\set{
             (2+x)\cdot b+(-1+x^2)\cdot c+(1+x^3)\cdot d
             \suchthat b,c,d\in\Re
             }
          \end{equation*}
          To show that this three-vector set is a basis, what remains is to
          verify that it is linearly independent.
          \begin{equation*}
             0+0x+0x^2+0x^3=(2+x)\cdot c_1+(-1+x^2)\cdot c_2+(1+x^3)\cdot c_3
          \end{equation*}
          From the $x$~terms we see that $c_1=0$.
          From the $x^2$~terms we see that $c_2=0$.
          The $x^3$~terms give that $c_3=0$. 
       \partsitem
         First parametrize the description
         (note that the fact that $b$ and~$d$ are not mentioned in the 
         description
         of $W$ does not mean
         they are zero or absent, it means that they are unrestricted).
         \begin{equation*}
           W=\set{
             \begin{mat}
               0 &1 \\
               0 &0
             \end{mat}\cdot b
             +
             \begin{mat}
               1 &0 \\
               1 &0
             \end{mat}\cdot c
             +
             \begin{mat}
               0 &0 \\
               0 &1
             \end{mat}\cdot d
             \suchthat b,c,d\in\Re}
         \end{equation*}
         That gives $W$ as the span of a three element set.
         We will be done if we show that the set is linearly independent.
         \begin{equation*}
             \begin{mat}
               0 &0 \\
               0 &0
             \end{mat}
             =
             \begin{mat}
               0 &1 \\
               0 &0
             \end{mat}\cdot c_1
             +
             \begin{mat}
               1 &0 \\
               1 &0
             \end{mat}\cdot c_2
             +
             \begin{mat}
               0 &0 \\
               0 &1
             \end{mat}\cdot c_3   
         \end{equation*}
         Using the upper right entries we see that $c_1=0$.
         The upper left entries give that $c_2=0$, and the lower left entries 
         show that $c_3=0$. 
     \end{exparts}
   \end{answer}
  \item \label{exer:VerifBasesCosPlusSin} 
    Check \nearbyexample{ex:BasisForCosPlusSin}.
    \begin{answer}
      We will show that the second is a basis; the first is similar.
      We will show this straight from the definition of a basis, 
      because this example appears before 
      \nearbytheorem{th:BasisIffUniqueRepWRT}.

      To see that it is linearly independent,
      we set up
      \( c_1\cdot(\cos\theta-\sin\theta)+c_2\cdot(2\cos\theta+3\sin\theta)=
        0\cos\theta+0\sin\theta \).
      Taking \( \theta=0 \) and \( \theta=\pi/2 \) gives this system
      \begin{equation*}
        \begin{linsys}{2}
          c_1\cdot 1    &+  &c_2\cdot 2   &=  &0  \\
          c_1\cdot (-1) &+  &c_2\cdot 3   &=  &0  
        \end{linsys}
        \grstep{\rho_1+\rho_2}
        \begin{linsys}{2}
          c_1  &+  &2c_2   &=  &0  \\
               &+  &5c_2   &=  &0 
        \end{linsys}
      \end{equation*}
      which shows that $c_1=0$ and $c_2=0$.    

      The calculation for span is also easy; for any $x,y\in\Re$, 
      we have that 
      \( c_1\cdot(\cos\theta-\sin\theta)+c_2\cdot(2\cos\theta+3\sin\theta)=
        x\cos\theta+y\sin\theta \)
      gives that \( c_2=x/5+y/5 \) and that \( c_1=3x/5-2y/5 \),
      and so the span is the entire space.  
    \end{answer}
  \recommended \item 
    Find the span of each set and then find a basis for that span.
    \begin{exparts*}
       \partsitem $\set{1+x,1+2x}$ in $\polyspace_2$
       \partsitem $\set{2-2x,3+4x^2}$ in $\polyspace_2$
    \end{exparts*}
    \begin{answer}
      \begin{exparts}
         \partsitem Asking which $a_0+a_1x+a_2x^2$ can be expressed as
           $c_1\cdot (1+x)+c_2\cdot (1+2x)$ 
           gives rise to three linear equations,
           describing the coefficients of $x^2$, $x$, and the constants.
           \begin{equation*} 
             \begin{linsys}{2}
               c_1 &+ &c_2  &= &a_0 \\
               c_1 &+ &2c_2 &= &a_1 \\
                   &  &0    &= &a_2
             \end{linsys}
           \end{equation*}
           Gauss's Method with back-substitution shows, 
           provided that $a_2=0$, that $c_2=-a_0+a_1$
           and $c_1=2a_0-a_1$.
           Thus, with $a_2=0$, we can compute appropriate
           $c_1$ and $c_2$ for any $a_0$ and $a_1$.
           So the span is the entire set of linear polynomials
           $\set{a_0+a_1x\suchthat a_0,a_1\in\Re}$.
           Parametrizing that set 
           $\set{a_0\cdot 1+a_1\cdot x\suchthat a_0,a_1\in\Re}$
           suggests a basis $\sequence{1,x}$ 
           (we've shown that it spans; checking linear independence is easy). 
        \partsitem With
          \begin{equation*}
            a_0+a_1x+a_2x^2
            =c_1\cdot(2-2x)+c_2\cdot(3+4x^2)
            =(2c_1+3c_2)+(-2c_1)x+(4c_2)x^2
          \end{equation*}
          we get this system.
           \begin{equation*} 
             \begin{linsys}{2}
               2c_1  &+ &3c_2  &= &a_0 \\
               -2c_1 &  &      &= &a_1 \\
                     &  &4c_2  &= &a_2
              \end{linsys}
             \grstep{\rho_1+\rho_2}
             \repeatedgrstep{(-4/3)\rho_2+\rho_3}
             \begin{linsys}{2}
               2c_1  &+ &3c_2  &= &a_0\hfill\hbox{} \\
                     &  &3c_2  &= &a_0+a_1\hfill\hbox{} \\
                     &  &0     &= &(-4/3)a_0-(4/3)a_1+a_2
              \end{linsys}
           \end{equation*}
           Thus, the only quadratic polynomials $a_0+a_1x+a_2x^2$ with 
           associated $c$'s are the ones such that 
           $0=(-4/3)a_0-(4/3)a_1+a_2$.
           Hence the span is this.
           \begin{equation*}
             \set{(-a_1+(3/4)a_2)+a_1x+a_2x^2\suchthat a_1,a_2\in\Re}
           \end{equation*}
           Parametrizing gives
           $\set{a_1\cdot (-1+x)+a_2\cdot ((3/4)+x^2)\suchthat a_1,a_2\in\Re}$,
           which suggests $\sequence{-1+x,(3/4)+x^2}$
           (checking that it is linearly independent is routine).
      \end{exparts} 
    \end{answer}
  \recommended \item 
    Find a basis for each of these subspaces of the space 
    $\polyspace_3$ of cubic polynomials. 
    \begin{exparts}
      \partsitem  The subspace of cubic polynomials $p(x)$ 
        such that $p(7)=0$
      \partsitem  The subspace of polynomials $p(x)$ such
        that $p(7)=0$ and $p(5)=0$
      \partsitem  The subspace of polynomials $p(x)$ such
        that $p(7)=0$, $p(5)=0$, and~$p(3)=0$
      \partsitem  The space of polynomials $p(x)$ such
        that $p(7)=0$, $p(5)=0$, $p(3)=0$, and~$p(1)=0$
    \end{exparts}
    \begin{answer}
       \begin{exparts}
        \partsitem The subspace is this.
          \begin{equation*}
             \set{a_0+a_1x+a_2x^2+a_3x^3 \suchthat a_0+7a_1+49a_2+343a_3=0 }
          \end{equation*}
          Rewriting $a_0=-7a_1-49a_2-343a_3$ gives this.
          \begin{equation*}
             \set{(-7a_1-49a_2-343a_3)+a_1x+a_2x^2+a_3x^3 
               \suchthat a_1,a_2,a_3\in\Re }
          \end{equation*}
          On breaking out the
          parameters, this suggests \( \sequence{-7+x,-49+x^2,-343+x^3} \)  
          for the basis (it is easily verified).
        \partsitem The given subspace is the collection of cubics
          $p(x)=a_0+a_1x+a_2x^2+a_3x^3$ such that $a_0+7a_1+49a_2+343a_3=0$
          and $a_0+5a_1+25a_2+125a_3=0$.     
          Gauss's Method 
          \begin{equation*}
            \begin{linsys}{4}
              a_0  &+  &7a_1  &+  &49a_2  &+  &343a_3  &=  &0  \\
              a_0  &+  &5a_1  &+  &25a_2  &+  &125a_3  &=  &0    
            \end{linsys}
            \grstep{-\rho_1+\rho_2}
            \begin{linsys}{4}
              a_0  &+  &7a_1  &+  &49a_2  &+  &343a_3  &=  &0  \\
                   &   &-2a_1 &-  &24a_2  &-  &218a_3  &=  &0    
            \end{linsys}
          \end{equation*}
          gives that $a_1=-12a_2-109a_3$ and that $a_0=35a_2+420a_3$.
          Rewriting $(35a_2+420a_3)+(-12a_2-109a_3)x+a_2x^2+a_3x^3$
          as $a_2\cdot(35-12x+x^2)+a_3\cdot(420-109x+x^3)$
          suggests this for a basis  $\sequence{35-12x+x^2,420-109x+x^3}$.
          The above shows that it spans the space.
          Checking it is linearly independent is routine.
          (\textit{Comment.} 
          A worthwhile check is to verify that both polynomials in the
          basis have both seven and five as roots.)
        \partsitem Here there are three conditions on the cubics,
          that $a_0+7a_1+49a_2+343a_3=0$, that $a_0+5a_1+25a_2+125a_3=0$,
          and that $a_0+3a_1+9a_2+27a_3=0$.
          Gauss's Method 
          \begin{equation*}
            \begin{linsys}{4}
              a_0  &+  &7a_1  &+  &49a_2  &+  &343a_3  &=  &0  \\
              a_0  &+  &5a_1  &+  &25a_2  &+  &125a_3  &=  &0  \\    
              a_0  &+  &3a_1  &+  &9a_2   &+  &27a_3   &=  &0    
            \end{linsys}
            \grstep[-\rho_1+\rho_3]{-\rho_1+\rho_2}
            \repeatedgrstep{-2\rho_2+\rho_3}
            \begin{linsys}{4}
              a_0  &+  &7a_1  &+  &49a_2  &+  &343a_3  &=  &0  \\
                   &   &-2a_1 &-  &24a_2  &-  &218a_3  &=  &0  \\    
                   &   &      &   &8a_2   &+  &120a_3  &=  &0    
            \end{linsys}
          \end{equation*}
          yields the single free variable $a_3$, with 
          $a_2=-15a_3$, $a_1=71a_3$, and $a_0=-105a_3$.
          The parametrization is this.
          \begin{multline*} 
            \set{(-105a_3)+(71a_3)x+(-15a_3)x^2+(a_3)x^3\suchthat a_3\in\Re} \\
            =
            \set{a_3\cdot(-105+71x-15x^2+x^3)\suchthat a_3\in\Re}
          \end{multline*}
          Therefore, a natural candidate for the basis is 
          $\sequence{-105+71x-15x^2+x^3}$.
          It spans the space by the work above.
          It is clearly linearly independent because it is a one-element
          set (with that single element not the zero object of the space).
          Thus, any cubic through the three points $(7,0)$, $(5,0)$, and
          $(3,0)$ is a multiple of this one.
          (\textit{Comment.}
          As in the prior question, 
          a worthwhile check is to verify that plugging seven, five, and
          three into this polynomial yields zero each time.)
        \partsitem This is the trivial subspace of $\polyspace_3$.
          Thus, the  basis is empty $\sequence{}$.
      \end{exparts}
      \noindent\textit{Remark.}
      Alternatively, we could have derived the polynomial in the third item
      by multiplying out $(x-7)(x-5)(x-3)$.
     \end{answer}
  \item 
     We've seen that the result of reordering a basis can be another basis.
     Must it be? 
     \begin{answer}
       Yes.
       Linear independence and span are unchanged by reordering.
     \end{answer}
  \item  
    Can a basis contain a zero vector?
    \begin{answer}
      No linearly independent set contains a zero vector.  
    \end{answer}
  \recommended \item
    Let \( \sequence{\vec{\beta}_1,\vec{\beta}_2,\vec{\beta}_3} \)
    be a basis for a vector space.
    \begin{exparts}
      \partsitem Show that 
        \( \sequence{c_1\vec{\beta}_1,c_2\vec{\beta}_2,c_3\vec{\beta}_3} \)
        is a basis when \( c_1, c_2, c_3\neq 0 \).
        What happens when at least one \( c_i \) is $0$?
      \partsitem Prove that
        \( \sequence{\vec{\alpha}_1,\vec{\alpha}_2,\vec{\alpha}_3} \)
        is a basis where \( \vec{\alpha}_i=\vec{\beta}_1+\vec{\beta}_i \).
    \end{exparts}
    \begin{answer}
      \begin{exparts}
         \partsitem To show that it is linearly independent, note that if
           \( d_1(c_1\vec{\beta}_1)+d_2(c_2\vec{\beta}_2)+d_3(c_3\vec{\beta}_3)
               =\zero \)
           then
           \( (d_1c_1)\vec{\beta}_1+(d_2c_2)\vec{\beta}_2+(d_3c_3)\vec{\beta}_3
               =\zero \), which in turn 
           implies that each \( d_ic_i \) is zero.
           But with \( c_i\neq 0 \) that means that each \( d_i \) is zero.
           Showing that it spans the space is much the same; 
           because $\sequence{\vec{\beta}_1,\vec{\beta}_2,\vec{\beta}_3}$
           is a basis, and so spans the space, we can for any $\vec{v}$ write  
           \( \vec{v}=d_1\vec{\beta}_1+d_2\vec{\beta}_2+d_3\vec{\beta}_3 \),
           and then
           \( \vec{v}=(d_1/c_1)(c_1\vec{\beta}_1)
               +(d_2/c_2)(c_2\vec{\beta}_2)+(d_3/c_3)(c_3\vec{\beta}_3) \).

           If any of the scalars are zero then the result is not a basis,
           because it is not linearly independent.
         \partsitem Showing that 
           $\sequence{2\vec{\beta}_1,\vec{\beta}_1+\vec{\beta}_2,
             \vec{\beta}_1+\vec{\beta}_3}$ is linearly independent is easy. 
           To show that it spans the space, assume that
           \( \vec{v}=d_1\vec{\beta}_1+d_2\vec{\beta}_2+d_3\vec{\beta}_3 \).
           Then, we can represent the same \( \vec{v} \) with respect to
           \( \sequence{2\vec{\beta}_1,\vec{\beta}_1+\vec{\beta}_2,
                         \vec{\beta}_1+\vec{\beta}_3} \)
           in this way
           $\vec{v}=(1/2)(d_1-d_2-d_3)(2\vec{\beta}_1)
           +d_2(\vec{\beta}_1+\vec{\beta}_2)+d_3(\vec{\beta}_1+\vec{\beta}_3)$.
      \end{exparts}   
    \end{answer}
  \item 
    Find one vector $\vec{v}$ that will make each into a basis
    for the space.
    \begin{exparts*}
      \partsitem $\sequence{\colvec[r]{1 \\ 1},\vec{v}}$ in $\Re^2$
      \partsitem $\sequence{\colvec[r]{1 \\ 1 \\ 0},
                            \colvec[r]{0 \\ 1 \\ 0},\vec{v}}$ in $\Re^3$
      \partsitem $\sequence{x,1+x^2,\vec{v}}$ in $\polyspace_2$
    \end{exparts*} 
    \begin{answer}
      Each forms a linearly independent set if we omit $\vec{v}$.
      To preserve linear independence, we must expand the span of each.
      That is, we must determine the span of each (leaving $\vec{v}$ out),
      and then pick a $\vec{v}$ lying outside of that span.
      Then to finish, we must check that the result spans the entire given
      space.
      Those checks are routine.
      \begin{exparts}
        \partsitem Any vector that is not a multiple of the given one, 
          that is, any vector that is not on the line $y=x$ will do here.
          One is $\vec{v}=\vec{e}_1$.
        \partsitem By inspection, we notice that the vector $\vec{e}_3$ is
          not in the span of the set of the two given vectors.
          The check that the resulting set is a basis for $\Re^3$ is 
          routine.
        \partsitem For any member of the span 
          $\set{c_1\cdot(x)+c_2\cdot(1+x^2)\suchthat c_1,c_2\in\Re}$,
          the coefficient of $x^2$ equals the constant term.
          So we expand the span if we add a quadratic without this property,
          say, $\vec{v}=1-x^2$.
          The check that the result is a basis for $\polyspace_2$ is easy.  
      \end{exparts}
    \end{answer}
  \recommended \item  
    Where
    \( \sequence{\vec{\beta}_1,\dots,\vec{\beta}_n } \)
    is a basis, show that in this equation
    \begin{equation*}
       c_1\vec{\beta}_1+\dots+c_k\vec{\beta}_k
       =
       c_{k+1}\vec{\beta}_{k+1}+\dots+c_n\vec{\beta}_n
    \end{equation*}
    each of the \( c_i \)'s is zero.
    Generalize.
    \begin{answer}
      To show that each scalar is zero, simply subtract
      \( c_1\vec{\beta}_1+\dots+c_k\vec{\beta}_k
          -c_{k+1}\vec{\beta}_{k+1}-\dots-c_n\vec{\beta}_n=\zero \).
      The obvious generalization is that in any equation involving only the
      \( \vec{\beta} \)'s, and in which each \( \vec{\beta} \) appears only
      once, each scalar is zero.
      For instance, an equation with a combination of 
      the even-indexed basis vectors
      (i.e., $\vec{\beta}_2$, $\vec{\beta}_4$, etc.) on the right and the
      odd-indexed basis vectors on the left also gives the conclusion that
      all of the coefficients are zero. 
    \end{answer}
  \item  
    A basis contains some of the vectors from a vector space; can it 
    contain them all?
    \begin{answer}
      No; no linearly independent set contains the zero vector.
    \end{answer}
  \item  
    \nearbytheorem{th:BasisIffUniqueRepWRT} shows that, with respect to a
    basis, every linear combination is unique.
    If a subset is not a basis, can linear combinations be not unique?
    If so, must they be?
    \begin{answer}
      Here is a subset of $\Re^2$ that is not a basis, and two different
      linear combinations of its elements that sum to the same vector.
      \begin{equation*}
        \set{\colvec[r]{1 \\ 2},\colvec[r]{2 \\ 4}}
        \qquad
        2\cdot\colvec[r]{1 \\ 2}+0\cdot\colvec[r]{2 \\ 4}
        =0\cdot\colvec[r]{1 \\ 2}+1\cdot\colvec[r]{2 \\ 4}
      \end{equation*}
      Thus, when a subset is not a basis, it can be the case that its
      linear combinations are not unique.

      But just because a subset is not a basis does not imply that its
      combinations must be not unique.
      For instance, this set 
      \begin{equation*}
        \set{\colvec[r]{1 \\ 2}}
      \end{equation*}
      does have the property that
      \begin{equation*}
        c_1\cdot\colvec[r]{1 \\ 2}
        =
        c_2\cdot\colvec[r]{1 \\ 2}
      \end{equation*}
      implies that $c_1=c_2$.
      The idea here is that this subset fails to be a basis because it fails
      to span the space; the proof of the theorem establishes that
      linear combinations are unique if and only if the subset is linearly
      independent.
     \end{answer}
  \recommended \item
    A square matrix is \definend{symmetric}\index{matrix!symmetric}%
    \index{symmetric matrix} 
    if for all indices \( i \) and
    \( j \), entry \( i,j \) equals entry \( j,i \).
    \begin{exparts}
      \partsitem Find a basis for the vector space of
        symmetric \( \nbyn{2} \) matrices.
      \partsitem Find a basis for the space of symmetric \( \nbyn{3} \)
        matrices.
      \partsitem Find a basis for the space of symmetric \( \nbyn{n} \)
        matrices.
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem Describing the vector space as
          \begin{equation*}
             \set{\begin{mat}
                     a  &b  \\
                     b  &c
                  \end{mat}  \suchthat a,b,c\in\Re}
          \end{equation*}
          suggests this for a basis.
          \begin{equation*}
            \sequence{
              \begin{mat}[r]
                1  &0  \\
                0  &0
              \end{mat},
              \begin{mat}[r]
                0  &0  \\
                0  &1
              \end{mat},
              \begin{mat}[r]
                0  &1  \\
                1  &0
              \end{mat}  }
          \end{equation*}
          Verification is easy.
        \partsitem This is one possible basis.
          \begin{equation*}
            \sequence{
              \begin{mat}[r]
                1  &0  &0  \\
                0  &0  &0  \\
                0  &0  &0
              \end{mat},
              \begin{mat}[r]
                0  &0  &0  \\
                0  &1  &0  \\
                0  &0  &0
              \end{mat},
              \begin{mat}[r]
                0  &0  &0  \\
                0  &0  &0  \\
                0  &0  &1
              \end{mat},
              \begin{mat}[r]
                0  &1  &0  \\
                1  &0  &0  \\
                0  &0  &0
              \end{mat},
              \begin{mat}[r]
                0  &0  &1  \\
                0  &0  &0  \\
                1  &0  &0
              \end{mat},
              \begin{mat}[r]
                0  &0  &0  \\
                0  &0  &1  \\
                0  &1  &0
              \end{mat}  }
          \end{equation*}
         \partsitem As in the prior two questions, we can form a basis from two
           kinds of  matrices.
           First are the matrices with a single one on the diagonal and all
           other entries zero (there are \( n \) of those matrices).
           Second are the matrices with two opposed off-diagonal entries
           are ones and all other entries are zeros.
           (That is, all entries in $M$ are zero except that 
           $m_{i,j}$ and $m_{j,i}$ are one.) 
      \end{exparts}  
    \end{answer}
  \recommended \item  
     We can show that every basis for $\Re^3$ contains the same
     number of vectors.
     \begin{exparts}
       \partsitem Show that no linearly independent subset of $\Re^3$
          contains more than three vectors.
       \partsitem Show that 
          no spanning subset of $\Re^3$ contains fewer than three vectors.
          \textit{Hint:} 
          recall how to calculate the span of a set and show that
          this method 
          cannot yield all of $\Re^3$ when we apply it to fewer than
          three vectors.
     \end{exparts}
     \begin{answer}
       \begin{exparts}
         \partsitem Any four vectors from $\Re^3$ are linearly related because
           the vector equation
           \begin{equation*}
             c_1\colvec{x_1 \\ y_1 \\ z_1}
             +c_2\colvec{x_2 \\ y_2 \\ z_2}
             +c_3\colvec{x_3 \\ y_3 \\ z_3}
             +c_4\colvec{x_4 \\ y_4 \\ z_4}
             =\colvec[r]{0 \\ 0 \\ 0}
           \end{equation*}
           gives rise to a linear system
           \begin{equation*}
             \begin{linsys}{4}
                x_1c_1  &+  &x_2c_2  &+  &x_3c_3  &+  &x_4c_4  &=  &0 \\
                y_1c_1  &+  &y_2c_2  &+  &y_3c_3  &+  &y_4c_4  &=  &0 \\
                z_1c_1  &+  &z_2c_2  &+  &z_3c_3  &+  &z_4c_4  &=  &0
             \end{linsys}
           \end{equation*}
           that is homogeneous (and so has a solution) and has 
           four unknowns but only three equations,
           and therefore has nontrivial solutions.
           (Of course, this argument applies to any subset of $\Re^3$ 
           with four or more vectors.) 
         \partsitem We shall do just the two-vector case.
           Given $x_1$, \ldots, $z_2$, 
           \begin{equation*}
             S=\set{\colvec{x_1 \\ y_1 \\ z_1},
             \colvec{x_2 \\ y_2 \\ z_2}} 
           \end{equation*}
           to decide which vectors
           \begin{equation*}
             \colvec{x \\ y \\ z}
           \end{equation*}
           are in the span of $S$, set up 
           \begin{equation*}
             c_1\colvec{x_1 \\ y_1 \\ z_1}
             +c_2\colvec{x_2 \\ y_2 \\ z_2}
             =\colvec{x \\ y \\ z}
           \end{equation*}
           and row reduce the resulting system.
           \begin{equation*}
             \begin{linsys}{2}
                x_1c_1  &+  &x_2c_2   &=  &x \\
                y_1c_1  &+  &y_2c_2   &=  &y \\
                z_1c_1  &+  &z_2c_2   &=  &z
             \end{linsys}
           \end{equation*}
           There are two
           variables $c_1$ and $c_2$ but three equations, so 
           when Gauss's Method finishes, on the bottom
           row there will be some relationship of the form
           $0=m_1x+m_2y+m_3z$. 
           Hence, vectors in the span of the two-element set $S$
           must satisfy some restriction.
           Hence the span is not all of $\Re^3$.
       \end{exparts}  
    \end{answer}
  \item 
    One of the exercises in the Subspaces subsection shows that the set 
    \begin{equation*}
      \set{\colvec{x \\ y \\ z}\suchthat x+y+z=1}
    \end{equation*}
    is a vector space under these operations.
    \begin{equation*}
      \colvec{x_1 \\ y_1 \\ z_1}+\colvec{x_2 \\ y_2 \\ z_2}
      =\colvec{x_1+x_2-1 \\ y_1+y_2 \\ z_1+z_2}
      \qquad
      r\colvec{x \\ y \\ z}=\colvec{rx-r+1 \\ ry \\ rz}
    \end{equation*}
    Find a basis.
    \begin{answer}
      We have (using these odball operations with care)
      \begin{multline*}
        \set{\colvec{1-y-z \\ y \\ z}\suchthat y,z\in\Re}
         =\set{\colvec{-y+1 \\ y \\ 0}
               +\colvec{-z+1 \\ 0 \\ z}
              \suchthat y,z\in\Re}                           \\
         =\set{y\cdot\colvec{0 \\ 1 \\ 0}
               +z\cdot\colvec{0 \\ 0 \\ 1}
              \suchthat y,z\in\Re}
      \end{multline*}
      and so a natural candidate for a basis is this.
      \begin{equation*}
        \sequence{\colvec[r]{0 \\ 1 \\ 0},\colvec[r]{0 \\ 0 \\ 1}}
      \end{equation*}
      To check linear independence we set up
      \begin{equation*}
         c_1\colvec[r]{0 \\ 1 \\ 0}+c_2\colvec[r]{0 \\ 0 \\ 1}
         =\colvec[r]{1 \\ 0 \\ 0}
      \end{equation*}
      (the vector on the right is the zero object in this space).
      That yields the linear system
      \begin{equation*}
        \begin{linsys}{3}
          (-c_1+1)  &+  &(-c_2+1)  &-  &1  &=  &1  \\
             c_1    &   &          &   &   &=  &0  \\
                    &   &c_2       &   &   &=  &0
        \end{linsys}
      \end{equation*}
      with only the solution $c_1=0$ and $c_2=0$.
      Checking the span is similar. 
   \end{answer}
\end{exercises}














\subsection{Dimension}
The previous subsection defines a basis of a vector space and
shows that a space can have many different bases.
% For example, following the definition of a basis, we saw three
% different bases for $\Re^2$.
So we cannot talk about ``the'' basis for a vector space.
True, some vector spaces have bases that strike us as more natural
than others, for instance, $\Re^2$'s basis $\stdbasis_2$ 
% or $\Re^3$'s basis $\stdbasis_3$
or $\polyspace_2$'s basis $\sequence{1,x,x^2}$.
But for 
the vector space $\set{a_2x^2+a_1x+a_0\suchthat 2a_2-a_0=a_1}$, 
no particular basis leaps out at us as the natural one.
We cannot, in general, associate with a space any single basis that
best describes it.

We can however find something about the bases that
is uniquely associated with the space.
This subsection shows that 
any two bases for a space have the same number of elements.
So with each space we can associate a number, 
the number of vectors in any of its bases.

Before we start, we first
limit our attention to spaces where at least one basis has only finitely
many members.

\begin{definition} \label{df:FiniteDimensional}
%<*df:FiniteDimensional>
A vector space is \definend{finite-dimensional\/}%
\index{vector space!finite dimensional}\index{finite-dimensional vector space}
if it has a basis with only finitely many vectors.
%</df:FiniteDimensional>
\end{definition}

\noindent One space that is not finite-dimensional is
the set of polynomials with real coefficients
\nearbyexample{ex:PolysOfAllFiniteDegrees}; 
this space is not spanned by any finite subset since that would contain
a polynomial of largest degree but this space has polynomials of all degrees.
Such spaces are interesting and important but we will focus in a
different direction.
From now on we will study only finite-dimensional vector spaces.
In the rest of this book we shall take `vector space' to mean 
`finite-dimensional vector space'.

\begin{remark}
One reason for sticking to finite-dimensional spaces is so that 
the representation of a vector with respect to a 
basis is a finitely-tall vector and we can easily write it.
Another reason is that 
the statement `any infinite-dimensional vector space has a basis'
is equivalent to a statement called the Axiom of Choice
\cite{Blass84} and so covering this would 
move us far past this book's scope.
(A discussion of the Axiom of Choice is in the
Frequently Asked Questions list for \texttt{sci.math}, and
another accessible one is \cite{Rucker}.)  
\end{remark}


To prove the main theorem we shall use a technical result, the Exchange Lemma.
We first illustrate it with an example.

\begin{example}
Here is a basis for $\Re^3$ and a vector given as a linear combination
of members of that basis.
\begin{equation*}
  B=\sequence{\colvec[r]{1 \\ 0  \\ 0},
              \colvec[r]{1 \\ 1 \\ 0},
              \colvec[r]{0 \\ 0 \\ 2}}
  \qquad
  \colvec[r]{1 \\ 2 \\ 0}
  =(-1)\cdot\colvec[r]{1 \\ 0  \\ 0}
   +2\colvec[r]{1 \\ 1 \\ 0}
   +0\cdot\colvec[r]{0 \\ 0 \\ 2}
\end{equation*}
Two of the basis vectors have non-zero coefficients.
Pick one, for instance the first.
Replace it with the vector that we've expressed as the combination
\begin{equation*}
  \hat{B}=\sequence{\colvec[r]{1 \\ 2  \\ 0},
              \colvec[r]{1 \\ 1 \\ 0},
              \colvec[r]{0 \\ 0 \\ 2}}
\end{equation*}
and the result is another basis for \( \Re^3 \).
\end{example}

\begin{lemma}[Exchange Lemma] \label{lm:ExchangeLemma}
%<*lm:ExchangeLemma>
Assume that 
\( B=\sequence{\vec{\beta}_1,\dots,\vec{\beta}_n} \) is a basis for a
vector space, and that for the vector \( \vec{v} \)
the relationship \( \vec{v}=\lincombo{c}{\vec{\beta}} \)
has \( c_i\neq 0 \).
Then exchanging \( \vec{\beta}_i \) for \( \vec{v} \) yields another
basis for the space.
%</lm:ExchangeLemma>
\end{lemma}

\begin{proof}
%<*pf:ExchangeLemma0>
Call the outcome of the exchange 
\( \hat{B}=\sequence{\vec{\beta}_1,\dots,\vec{\beta}_{i-1},\vec{v},
                       \vec{\beta}_{i+1},\dots,\vec{\beta}_n}  \).

We first show that $\hat{B}$ is linearly independent.
Any relationship 
\( d_1\vec{\beta}_1+\dots+d_i\vec{v}+\dots+d_n\vec{\beta}_n=\zero \)
among the members of $\hat{B}$, after substitution for $\vec{v}$,
\begin{equation*}
  d_1\vec{\beta}_1+\dots
    +d_i\cdot(c_1\vec{\beta}_1+\dots+c_i\vec{\beta}_i+\dots+c_n\vec{\beta}_n)
    +\dots+d_n\vec{\beta}_n
  =\zero
\tag*{($*$)}\end{equation*}
gives a linear relationship among the members of $B$.
The basis $B$ is linearly independent so the coefficient $d_ic_i$ of 
$\vec{\beta}_i$ is zero.
Because we assumed that $c_i$ is nonzero, $d_i=0$.
Using this in equation~$(*)$ gives that all of the other $d$'s are also
zero.
Therefore $\hat{B}$ is linearly independent.
%</pf:ExchangeLemma0>

%<*pf:ExchangeLemma1>
We finish by showing that $\hat{B}$ has the same span as $B$.
Half of this argument, that $\spanof{\smash{\hat{B}}}\subseteq\spanof{B}$, 
is easy; we can write any member 
$d_1\vec{\beta}_1+\dots+d_i\vec{v}+\dots+d_n\vec{\beta}_n$
of $\spanof{\smash{\hat{B}}}$ as
$
  d_1\vec{\beta}_1+\dots
    +d_i\cdot(c_1\vec{\beta}_1+\dots+c_n\vec{\beta}_n)
    +\dots+d_n\vec{\beta}_n
$,
which is a linear combination of linear combinations of members of $B$, and
hence is in $\spanof{B}$.
For the $\spanof{B}\subseteq\spanof{\smash{\hat{B}}}$ half of the argument,
recall that if
$\vec{v}=c_1\vec{\beta}_1+\dots+c_n\vec{\beta}_n$ with $c_i\neq 0$
then we can rearrange the equation to
$\vec{\beta}_i=(-c_1/c_i)\vec{\beta}_1+\dots+(1/c_i)\vec{v}+\dots
   +(-c_n/c_i)\vec{\beta}_n$.
Now, consider any member
$d_1\vec{\beta}_1+\dots+d_i\vec{\beta}_i+\dots+d_n\vec{\beta}_n$
of $\spanof{B}$, substitute for $\vec{\beta}_i$ its expression as a linear
combination of the members of $\hat{B}$, and recognize, 
as in the first half of this argument, that the result is a linear
combination of linear combinations of members of $\hat{B}$, and hence is in 
$\spanof{\smash{\hat{B}}}$.
%</pf:ExchangeLemma1>
\end{proof}

\begin{theorem} \label{th:AllBasesSameSize}
%<*th:AllBasesSameSize>
In any finite-dimensional vector space, all 
bases have the same number of elements.\index{basis}
%</th:AllBasesSameSize>
\end{theorem}

\begin{proof}
%<*pf:AllBasesSameSize0>
Fix a vector space with at least one finite basis.
Choose, from among all of this space's bases,
one \( B=\sequence{\vec{\beta}_1,\dots,\vec{\beta}_n} \) of minimal size.
We will show that any other basis
\( D=\smash{\sequence{\vec{\delta}_1,\vec{\delta}_2,\ldots}} \)
also has the same number of members, $n$.
Because \( B \) has minimal size, \( D \) has no fewer than \( n \) vectors.
We will argue that it cannot have more than \( n \) vectors.
%</pf:AllBasesSameSize0>

%<*pf:AllBasesSameSize1>
The basis \( B \) spans the space and \( \vec{\delta}_1 \) is in the space,
so \( \vec{\delta}_1 \) is a nontrivial linear combination of elements of
\( B \).
By the Exchange Lemma, we can swap \( \vec{\delta}_1 \) for a
vector from \( B \), resulting in a basis \( B_1 \), where one element is
\( \vec{\delta}_1 \) and all of the \( n-1 \) other elements 
are \( \vec{\beta} \)'s.
%</pf:AllBasesSameSize1>

%<*pf:AllBasesSameSize2>
The prior paragraph forms the basis step for an induction argument.
The inductive step starts with a basis \( B_k \) (for \( 1\leq k<n \))
containing \( k \) members of \( D \) and \( n-k \) members of \( B \).
We know that \( D \) has at least \( n \) members so there is a
\( \vec{\delta}_{k+1} \).
Represent it as a linear combination of elements of \( B_k \).
The key point:~in that representation, at least one of the nonzero scalars
must be associated with a \( \vec{\beta}_i \) or else that
representation would be a
nontrivial linear relationship among elements of the linearly independent
set \( D \).
Exchange \( \vec{\delta}_{k+1} \) for \( \vec{\beta}_i \) to get a new basis
\( B_{k+1} \) with one \( \vec{\delta} \) more and one \( \vec{\beta} \)
fewer than the previous basis \( B_k \).
%</pf:AllBasesSameSize2>

%<*pf:AllBasesSameSize3>
Repeat that until no \( \vec{\beta} \)'s remain, so
that \( B_n \) contains  
$\vec{\delta}_1,\dots,\vec{\delta}_n$.
Now, \( D \) cannot have more than these \( n \) vectors because
any \( \vec{\delta}_{n+1} \) that remains would be in the span of
\( B_n \) (since it is a basis) 
and hence would be a linear combination of the other $\vec{\delta}$'s, 
contradicting that $D$ is linearly independent.
%</pf:AllBasesSameSize3>
\end{proof}

\begin{definition}  \label{df:Dimension}
%<*df:Dimension>
The \definend{dimension}\index{dimension}\index{vector space!dimension}
of a vector space is the number of vectors in any of its bases.
%</df:Dimension>
\end{definition}

\begin{example}
Any basis for \( \Re^n \) has \( n \) vectors since the standard basis 
\( \stdbasis_n \) has \( n \) vectors.
Thus, this definition of `dimension' generalizes the most familiar use of 
term, that $\Re^n$ is $n$-dimensional.
\end{example}

\begin{example}
The space \( \polyspace_n \) of polynomials of degree at most $n$ 
has dimension \( n+1 \).
We can show this by exhibiting any basis\Dash $\sequence{1,x,\dots,x^n}$ 
comes to mind\Dash and counting its members.
\end{example}

\begin{example}
The space of functions 
$\set{a\cdot\cos\theta+b\cdot\sin\theta\suchthat a,b\in\Re}$  
of the real variable $\theta$ has dimension~$2$ since this space has the 
basis $\sequence{\cos\theta,\sin\theta}$.
\end{example}

\begin{example}
A trivial space is zero-dimensional since its basis is empty.
\end{example}

Again, although we sometimes say `finite-dimensional' for emphasis, from now on
we take all vector spaces to be finite-dimensional.
So in the next result the word `space' 
means `finite-dimensional vector space'.

\begin{corollary} \label{cor:NoLiSetGreatDim}
%<*co:NoLiSetGreatDim>
No linearly independent set can have a size greater than the dimension of the
enclosing space.
%</co:NoLiSetGreatDim>
\end{corollary}

\begin{proof}
%<*pf:NoLiSetGreatDim>
The proof of \nearbytheorem{th:AllBasesSameSize} 
never uses that \( D \) spans the space,
only that it is linearly independent.
%</pf:NoLiSetGreatDim>
\end{proof}


\begin{example} \label{ex:RefSubSpDiagram}
Recall the diagram from Example I.\ref{ex:SubspRThree} showing 
the subspaces of \( \Re^3 \).
Each subspace is described with a minimal spanning set, a basis.
The whole space has a basis with three members, 
the plane subspaces have bases with two members,
the line subspaces have bases with one member,
and the trivial subspace has a basis with zero members.
We could not in that section show that these are 
\( \Re^3 \)'s only subspaces.
We can show it now.
The prior corollary proves that 
the only subspaces of \( \Re^3 \) are either three-\hbox{},
two-\hbox{}, one-\hbox{}, or zero-dimensional.
There are no subspaces somehow, say, between lines and planes.
\end{example}


\begin{corollary} \label{cor:LIExpBas}
%<*co:LIExpBas>
Any linearly independent set can be expanded to make a basis.
%</co:LIExpBas>
\end{corollary}

\begin{proof}
%<*pf:LIExpBas>
If a linearly independent set 
is not already a basis then it must not span the space.
Adding to the set a vector that is not in the span 
will preserve linear independence by 
Lemma II.\ref{lm:AddVecLiSetIsLiIffVecNotInSpan}.
Keep adding until the resulting set does span the space, 
which the prior corollary
shows will happen after only a finite number of steps.
%</pf:LIExpBas>
\end{proof}

\begin{corollary}  \label{co:SpanningSetShrinksToABasis}
%<*co:SpanningSetShrinksToABasis>
Any spanning set can be shrunk to a basis.
%</co:SpanningSetShrinksToABasis>
\end{corollary}

\begin{proof}
%<*pf:SpanningSetShrinksToABasis>
Call the spanning set \( S \).
If \( S \) is empty then it is already a basis (the space must be a trivial
space).
If \( S=\set{\zero} \) then it can be shrunk to the empty basis,
thereby making it linearly independent, without changing its span.

Otherwise, $S$ contains a vector $\vec{s}_1$ with $\vec{s}_1\neq\zero$
and we can form a basis \( B_1=\sequence{\vec{s}_1} \).
If \( \spanof{B_1}=\spanof{S} \) then we are done.
If not then there is a \( \vec{s}_2\in\spanof{S} \) such that
\( \vec{s}_2\not\in\spanof{B_1} \).
Let \( B_2=\sequence{\vec{s}_1,\vec{s_2}} \);
by Lemma~II.\ref{lm:AddVecLiSetIsLiIffVecNotInSpan} this is linearly independent
so if \( \spanof{B_2}=\spanof{S} \) then we are done.

We can repeat this process until the spans are equal,
which must happen in at most finitely many steps.
%</pf:SpanningSetShrinksToABasis>
\end{proof}


\begin{corollary} \label{cor:NVecsRNSpanIffLI}
%<*co:NVecsRNSpanIffLI>
In an \( n \)-dimensional space, a set composed of \( n \) vectors is linearly 
independent if and
only if it spans the space.
%</co:NVecsRNSpanIffLI>
\end{corollary}

\begin{proof}
%<*pf:NVecsRNSpanIffLI>
First we will
show that a subset with \( n \) vectors is linearly independent if and only
if it is a basis.
The `if' is trivially true\Dash bases are linearly independent.
`Only if' holds
because a linearly independent set can be expanded to a basis, but a
basis has \( n \) elements, so this 
expansion is actually the set that we began with.

To finish, we will show that any subset with \( n \) vectors spans the space 
if and only if it is a basis.
Again, `if' is trivial.
`Only if'
holds because any spanning set can be shrunk to a basis, but a basis has
\( n \) elements and so this shrunken set is just the one we started with.
%</pf:NVecsRNSpanIffLI>
\end{proof}

The main result of this subsection, that all of 
the bases in a finite-dimensional
vector space have the same number of elements, is the single most important
result in this book. 
As \nearbyexample{ex:RefSubSpDiagram}
shows, it describes what vector spaces and subspaces there can be.

One immediate consequence brings us 
back to when we considered the two things that could be meant
by the term `minimal spanning set'.
At that point we defined `minimal' as linearly independent
but we noted that 
another reasonable interpretation of the term is  
that a spanning set is `minimal' when it has the fewest
number of elements of any set with the same span.
Now that 
we have shown that all bases have the same number of elements, we know
that the two senses of `minimal' are equivalent.


\begin{exercises}
  \item[{\em Assume that all spaces are finite-dimensional unless otherwise
    stated.}]
  \recommended \item  
    Find a basis for, and the dimension of, \(  \polyspace_2 \).
    \begin{answer}
      One basis is \( \sequence{1,x,x^2} \), and so
      the dimension is three.
    \end{answer}
  \item 
    Find a basis for, and the dimension of, the solution set of
    this system.
    \begin{equation*}
      \begin{linsys}{4}
         x_1  &-  &4x_2  &+  &3x_3  &-  &x_4  &=  &0  \\
        2x_1  &-  &8x_2  &+  &6x_3  &-  &2x_4 &=  &0  
      \end{linsys}
    \end{equation*}
    \begin{answer}
      The solution set is
      \begin{equation*}
        \set{\colvec{4x_2-3x_3+x_4 \\ x_2 \\ x_3 \\ x_4}
               \suchthat x_2,x_3,x_4\in\Re}
      \end{equation*}
      so a natural basis is this
      \begin{equation*}
       \sequence{\colvec[r]{4 \\ 1 \\ 0 \\ 0},
                   \colvec[r]{-3 \\ 0 \\ 1 \\ 0},
                   \colvec[r]{1 \\ 0 \\ 0 \\ 1}  }
      \end{equation*}  
      (checking linear independence is easy).
      Thus the dimension is three.
    \end{answer}
  \recommended \item Find a basis for, and the dimension of, each space.
    \begin{exparts}
      \partsitem
        $
          \set{\colvec{x \\ y \\ z \\ w}\in\Re^4
               \suchthat x-w+z=0}
        $
      \partsitem the set of $\nbym{5}{5}$ matrices whose only nonzero entries 
        are on the diagonal (e.g., in entry $1,1$ and $2,2$, etc.)
      \partsitem 
        $\set{a_0+a_1x+a_2x^2+a_3x^3\suchthat 
            \text{$a_0+a_1=0$ and $a_2-2a_3=0$}}\subseteq\polyspace_3$ 
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem
          Parametrize to get this description of the space.
          \begin{equation*}
            \set{\colvec{w-z \\ y \\ z \\ w}
                 =\colvec{0 \\ 1 \\ 0 \\ 0}y+
                 \colvec{-1 \\ 0 \\ 1 \\ 0}z+
                 \colvec{1 \\ 0 \\ 0 \\ 1}w
                 \suchthat y,z,w\in\Re}
          \end{equation*}
          That gives the space as the span of the three-vector set.
          To show the three vector set makes a basis we check that it is 
          linearly independent.
          \begin{equation*}
             \colvec{0 \\ 0 \\ 0 \\ 0}
                 =\colvec{0 \\ 1 \\ 0 \\ 0}c_1+
                 \colvec{-1 \\ 0 \\ 1 \\ 0}c_2+
                 \colvec{1 \\ 0 \\ 0 \\ 1}c_3
          \end{equation*}
          The second components give that $c_1=0$, and the third and fourth 
          components give that $c_2=0$ and~$c_3=0$.
          So one basis is this.
          \begin{equation*}
              \sequence{
                 \colvec{0 \\ 1 \\ 0 \\ 0},
                 \colvec{-1 \\ 0 \\ 1 \\ 0},
                 \colvec{1 \\ 0 \\ 0 \\ 1} 
              } 
          \end{equation*}
          The dimension is the number of vectors in a basis: $3$.
        \partsitem
          The natural parametrization is this.
          \begin{multline*}
            \set{
              \begin{mat}
                a &0 &0 &0 &0 \\
                0 &b &0 &0 &0 \\
                0 &0 &c &0 &0 \\
                0 &0 &0 &d &0 \\
                0 &0 &0 &0 &e 
              \end{mat}
              \suchthat a,\ldots,e\in\Re}   \\
            \set{
              \begin{mat}
                1 &0 &0 &0 &0 \\
                0 &0 &0 &0 &0 \\
                0 &0 &0 &0 &0 \\
                0 &0 &0 &0 &0 \\
                0 &0 &0 &0 &0 
              \end{mat}\cdot a
              +\begin{mat}
                0 &0 &0 &0 &0 \\
                0 &1 &0 &0 &0 \\
                0 &0 &0 &0 &0 \\
                0 &0 &0 &0 &0 \\
                0 &0 &0 &0 &0 
              \end{mat}\cdot b
              +\cdots
              \suchthat a,\ldots,e\in\Re}
          \end{multline*}
          Checking that the five-element set is linearly independent is trivial.
          So this is a basis; the dimension is $5$.
          \begin{equation*}
            \sequence{
              \begin{mat}
                1 &0 &0 &0 &0 \\
                0 &0 &0 &0 &0 \\
                0 &0 &0 &0 &0 \\
                0 &0 &0 &0 &0 \\
                0 &0 &0 &0 &0 
              \end{mat},
              \begin{mat}
                0 &0 &0 &0 &0 \\
                0 &1 &0 &0 &0 \\
                0 &0 &0 &0 &0 \\
                0 &0 &0 &0 &0 \\
                0 &0 &0 &0 &0 
              \end{mat},
              \,\ldots\,,
              \begin{mat}
                0 &0 &0 &0 &0 \\
                0 &0 &0 &0 &0 \\
                0 &0 &0 &0 &0 \\
                0 &0 &0 &0 &0 \\
                0 &0 &0 &0 &1 
              \end{mat}
          }
          \end{equation*}
        \partsitem
          The restrictions form a two-equations, four-unknowns linear system.
          Parametrizing that system to express the leading variables in 
          terms of those that are
          free gives $a_0=-a_1$, $a_1=a_1$, $a_2=2a_3$, and~$a_3=a_3$.
          \begin{multline*}
            \set{-a_1+a_1x+2a_3x^2+a_3x^3\suchthat a_1,a_3\in\Re}         \\
            =\set{(-1+x)\cdot a_1+(2x^2+x^3)\cdot a_3\suchthat a_1,a_3\in\Re}
          \end{multline*}
          That description shows that the space is the span of the two-element 
          set $\set{-1+x,x^2+x^3}$.  
          We will be done if we show the set is linearly independent.
          This relationship
          \begin{equation*}
            0+0x+0x^2+0x^3=(-1+x)\cdot c_1+(2x^2+x^3)\cdot c_2
          \end{equation*}
          gives that $c_1=0$ from the constant terms, and $c_2=0$ from the 
          cubic terms.
          One basis for the space is $\sequence{-1+x,2x^2+x^3}$.
          This is a two-dimensional space.

      \end{exparts}
    \end{answer}
  \item
    Find a basis for, and the dimension of, \( \matspace_{\nbyn{2}} \),
    the vector space of \( \nbyn{2} \) matrices.
    \begin{answer}
      For this space
      \begin{multline*}
        \set{\begin{mat}
               a  &b  \\
               c  &d
             \end{mat} \suchthat a,b,c,d\in\Re}            \\
        =\set{a\cdot\begin{mat}[r]
             1  &0  \\
             0  &0
           \end{mat}
           +\dots+
           d\cdot\begin{mat}[r]
             0  &0  \\
             0  &1
           \end{mat} \suchthat a,b,c,d\in\Re}
      \end{multline*}
      this is a natural basis.
      \begin{equation*}
        \sequence{
           \begin{mat}[r]
             1  &0  \\
             0  &0
           \end{mat},
           \begin{mat}[r]
             0  &1  \\
             0  &0
           \end{mat},
           \begin{mat}[r]
             0  &0  \\
             1  &0
           \end{mat},
           \begin{mat}[r]
             0  &0  \\
             0  &1
           \end{mat}  }
      \end{equation*}
      The dimension is four. 
    \end{answer}
  \item 
    Find the dimension of the vector space of matrices
    \begin{equation*}
      \begin{mat}
        a  &b  \\
        c  &d
      \end{mat}
    \end{equation*}
    subject to each condition.
    \begin{exparts*}
      \item $a, b, c, d\in\Re$ 
      \item $a-b+2c=0$ and~$d\in\Re$
      \item $a+b+c=0$, $a+b-c=0$, and~$d\in\Re$
    \end{exparts*}
    \begin{answer}
     \begin{exparts}
      \partsitem As in the prior exercise, the space $\matspace_{\nbyn{2}}$ 
        of matrices without restriction has this basis
        \begin{equation*}
         \sequence{
           \begin{mat}[r]
             1  &0  \\
             0  &0
           \end{mat},
           \begin{mat}[r]
             0  &1  \\
             0  &0
           \end{mat},
           \begin{mat}[r]
             0  &0  \\
             1  &0
           \end{mat},
           \begin{mat}[r]
             0  &0  \\
             0  &1
           \end{mat}  }
        \end{equation*}
        and so the dimension is four.
      \partsitem For this space
        \begin{multline*}
         \set{\begin{mat}
               a  &b  \\
               c  &d
             \end{mat} \suchthat \text{$a=b-2c$ and $d\in\Re$}}     \\
         =\set{b\cdot\begin{mat}[r]
             1  &1  \\
             0  &0
           \end{mat}
           +c\cdot\begin{mat}[r]
             -2  &0  \\
              1  &0
           \end{mat}
           +d\cdot\begin{mat}[r]
             0  &0  \\
             0  &1
           \end{mat} \suchthat b,c,d\in\Re}
        \end{multline*}
        this is a natural basis.
        \begin{equation*}
          \sequence{
            \begin{mat}[r]
              1  &1  \\
              0  &0
            \end{mat},
            \begin{mat}[r]
              -2  &0  \\
               1  &0
            \end{mat},
            \begin{mat}[r]
              0  &0  \\
              0  &1
            \end{mat}  }
        \end{equation*}
        The dimension is three. 
      \partsitem Gauss's Method applied to the two-equation linear system gives
        that $c=0$ and that $a=-b$.
        Thus, we have this description
        \begin{equation*}
         \set{\begin{mat}
               -b  &b  \\
                0  &d
             \end{mat} \suchthat b,d\in\Re}
         =\set{b\cdot\begin{mat}[r]
             -1  &1  \\
             0  &0
           \end{mat}
           +d\cdot\begin{mat}[r]
             0  &0  \\
             0  &1
           \end{mat} \suchthat b,d\in\Re}
        \end{equation*}
        and so this is a natural basis.
        \begin{equation*}
          \sequence{
            \begin{mat}[r]
              -1  &1  \\
               0  &0
            \end{mat},
            \begin{mat}[r]
              0  &0  \\
              0  &1
            \end{mat}  }
        \end{equation*}
        The dimension is two. 
     \end{exparts} 
    \end{answer}
  \recommended \item
    Find the dimension of this subspace of $\Re^2$.
    \begin{equation*}
      S=\set{\colvec{a+b \\ a+c}\suchthat a,b,c\in\Re}
    \end{equation*}
    \begin{answer}
      We cannot simply count the parameters.
      That is, the answer is not~$3$.
      Instead, observe that we can express every member
      $\binom{x}{y}\in\Re^2$ in the form
      \begin{equation*}
        \colvec{x \\ y}=\colvec{a+b \\ a+c}  
      \end{equation*}
      with the choice of $a=0$, $b=x$, and $c=y$ 
      (other choices are possible).
      So $S$ is the set $S=\Re^2$.
      It has dimension~$2$.
    \end{answer}
  \recommended \item 
    Find the dimension of each.
    \begin{exparts}
      \partsitem  The space of cubic polynomials $p(x)$ such
        that $p(7)=0$
      \partsitem  The space of cubic polynomials $p(x)$ such
        that $p(7)=0$ and~$p(5)=0$
      \partsitem  The space of cubic polynomials $p(x)$ such
        that $p(7)=0$, $p(5)=0$, and~$p(3)=0$
      \partsitem  The space of cubic polynomials $p(x)$ such
        that $p(7)=0$, $p(5)=0$, $p(3)=0$, and~$p(1)=0$
    \end{exparts}
    \begin{answer}
      The bases for these spaces are developed 
      in the answer set of the prior subsection.
      \begin{exparts}
        \partsitem One basis is \( \sequence{-7+x,-49+x^2,-343+x^3} \).
          The dimension is three.
        \partsitem One basis is $\sequence{35-12x+x^2,420-109x+x^3}$ so the
          dimension is two.
        \partsitem A basis is $\set{-105+71x-15x^2+x^3}$.
          The dimension is one.
        \partsitem This is the trivial subspace of $\polyspace_3$ and so the 
          basis is empty.
          The dimension is zero.
      \end{exparts}  
    \end{answer}
  \item 
     What is the dimension of the span of the set 
     $\set{\cos^2\theta,\sin^2\theta,\cos2\theta,\sin2\theta}$?
     This span is a subspace of the space of all real-valued functions of 
     one real variable.
     \begin{answer}
       First recall that $\cos2\theta=\cos^2\theta-\sin^2\theta$, and so 
       deletion of $\cos2\theta$ from this set leaves the span unchanged.
       What's left, the set  
       $\set{\cos^2\theta,\sin^2\theta,\sin2\theta}$, is linearly independent
       (consider the relationship
       $c_1\cos^2\theta+c_2\sin^2\theta+c_3\sin2\theta=Z(\theta)$
       where $Z$ is the zero function, and then take
       $\theta=0$, $\theta=\pi/4$, and $\theta=\pi/2$ to conclude that
       each $c$ is zero).
       It is therefore a basis for its span.
       That shows that the span is a dimension three vector space.
     \end{answer}
  \item  
    Find the dimension of \( \C^{47} \), the vector space
    of $47$-tuples of complex numbers.
    \begin{answer}
      Here is a basis
      \begin{equation*}
        \sequence{(1+0i,0+0i,\dots,0+0i),\,
                  (0+1i,0+0i,\dots,0+0i),(0+0i,1+0i,\dots,0+0i),\ldots }
      \end{equation*}   
      and so the dimension is \( 2\cdot 47=94 \).
    \end{answer}
  \item  
    What is the dimension of the vector space $\matspace_{\nbym{3}{5}}$ 
    of \( \nbym{3}{5} \) matrices?
    \begin{answer}
      A basis is
      \begin{equation*}
        \sequence{
          \begin{mat}[r]
            1  &0  &0  &0  &0  \\
            0  &0  &0  &0  &0  \\
            0  &0  &0  &0  &0
          \end{mat},
          \begin{mat}[r]
            0  &1  &0  &0  &0  \\
            0  &0  &0  &0  &0  \\
            0  &0  &0  &0  &0
          \end{mat},
          \ldots,
          \begin{mat}[r]
            0  &0  &0  &0  &0  \\
            0  &0  &0  &0  &0  \\
            0  &0  &0  &0  &1
          \end{mat}  }
      \end{equation*}
      and thus the dimension is \( 3\cdot 5=15 \).  
    \end{answer}
  \recommended \item 
    Show that this is a basis for $\Re^4$.
    \begin{equation*}
      \sequence{\colvec[r]{1 \\ 0 \\ 0 \\ 0},
        \colvec[r]{1 \\ 1 \\ 0 \\ 0},
        \colvec[r]{1 \\ 1 \\ 1 \\ 0},
        \colvec[r]{1 \\ 1 \\ 1 \\ 1} }
    \end{equation*}
    (We can use the results of this subsection to simplify this job.)
    \begin{answer}
       In a four-dimensional space a set of four vectors is linearly
       independent if and only if it spans the space.
       The form of these vectors makes linear independence easy to show
       (look at the equation of fourth components, then at the equation of 
       third components, etc.).  
    \end{answer}
  \item 
     Refer to \nearbyexample{ex:RefSubSpDiagram}.
     \begin{exparts}
       \partsitem Sketch a similar subspace diagram for $\polyspace_2$.
       \partsitem Sketch one for $\matspace_{\nbyn{2}}$.
     \end{exparts}
     \begin{answer}
      \begin{exparts}
       \partsitem The diagram for $\polyspace_2$ has four levels.
          The top level has the only three-dimensional subspace, 
          $\polyspace_2$ itself.
          The next level contains the two-dimensional subspaces
          (\emph{not} just the linear polynomials; any two-dimensional
          subspace, like those polynomials of the form $ax^2+b$).
          Below that are the one-dimensional subspaces.
          Finally, of course, is the only zero-dimensional subspace,
          the trivial subspace.
        \partsitem For $\matspace_{\nbyn{2}}$, the diagram has five levels,
          including subspaces of dimension four through zero.
     \end{exparts}
    \end{answer}
  \recommended \item \label{exer:DimDomToR}
    Where \( S \) is a set, the functions
    \( \map{f}{S}{\Re} \) form a vector space under the natural operations:
    the sum $f+g$ is the function given by 
    \( f+g\,(s)=f(s)+g(s) \) and the scalar product is
    \( r\cdot f \, (s)=r\cdot f(s) \).
    What is the dimension of the space resulting for each domain?
    \begin{exparts*}
      \partsitem \( S=\set{1} \)
      \partsitem \( S=\set{1,2} \)
      \partsitem \( S=\set{1,\ldots,n} \)
    \end{exparts*}
    \begin{answer}
      \begin{exparts*}
        \partsitem One
        \partsitem Two
        \partsitem \( n \)
      \end{exparts*}  
    \end{answer}
  \item  
    (See \nearbyexercise{exer:DimDomToR}.)
    Prove that this is an infinite-dimensional space:
    the set of all functions \( \map{f}{\Re}{\Re} \) under the natural
    operations.
    \begin{answer}
      We need only produce an infinite linearly independent set.
      One is such sequence is \( \sequence{f_1,f_2,\ldots} \) where
      \( \map{f_i}{\Re}{\Re} \) is
      \begin{equation*}
         f_i(x)=\begin{cases}
                   1  &\text{if \( x=i \)}  \\
                   0  &\text{otherwise}
                \end{cases}
      \end{equation*}  
      the function that has value $1$ only at $x=i$.
    \end{answer}
  \item  
    (See \nearbyexercise{exer:DimDomToR}.)
    What is the dimension of the vector space of functions
    $\map{f}{S}{\Re}$, under the natural operations, where the
    domain $S$ is the empty set?
    \begin{answer}
      A function is a set of ordered pairs
      $(x,f(x))$.
      So there is only one function with an empty domain, namely the empty set.
      A vector space with only one element a trivial vector space 
      and has dimension zero.
    \end{answer}
  \item  
    Show that 
    any set of four vectors in \( \Re^2 \) is linearly dependent.
    \begin{answer}
      Apply \nearbycorollary{cor:NoLiSetGreatDim}.
    \end{answer}
  \item  
    Show that
    \( \sequence{\vec{\alpha}_1,\vec{\alpha}_2,\vec{\alpha}_3}\subset\Re^3 \)
    is a basis if and only if there is no plane through the origin containing
    all three vectors.
    \begin{answer}
      A plane has the
      form $\set{\vec{p}+t_1\vec{v}_1+t_2\vec{v}_2\suchthat t_1,t_2\in\Re}$.
      (The first chapter also calls this a `$2$-flat', and contains
      a discussion of why this is equivalent to the
      description often taken in Calculus as the set of points $(x,y,z)$
      subject to a condition of the form $ax+by+cz=d$).
      When the plane passes through the origin we can take the particular
      vector $\vec{p}$ to be $\zero$.
      Thus, in the language we have developed in this chapter, a plane through
      the origin is the span of a set of two vectors.

      Now for the statement.
      Asserting that the three are not coplanar is the same as asserting that
      no vector lies in the span of the other two\Dash no vector is a linear
      combination of the other two.
      That's simply an assertion that the three-element set is linearly
      independent.
      By \nearbycorollary{cor:NVecsRNSpanIffLI}, that's equivalent to an
      assertion that the set is a basis for $\Re^3$ (more precisely, 
      any sequence made from the set's elements is a basis). 
    \end{answer}
  \item 
    % \begin{exparts}
      % \partsitem Prove that any subspace of a finite dimensional space
      %   has a basis. 
        Prove that any subspace of a finite dimensional space is 
        finite dimensional.
    % \end{exparts}
    \begin{answer}
      Let the space $V$ be finite dimensional and let
      $S$ be a subspace of $V$.

      If $S$ is not finite dimensional then it has a linearly independent 
      set that is infinite (start with the empty set and iterate adding 
      vectors that are not linearly dependent on the set; this process
      can continue for infinitely many steps or else $S$ would be 
      finite dimensional).
      But any linearly independent subset of~$S$ is a linearly independent
      subset of~$V$, contradicting \nearbycorollary{cor:NoLiSetGreatDim}
      % \begin{exparts}
      %   \partsitem The empty set is a linearly independent subset of $S$.
      %     By \nearbycorollary{cor:LIExpBas}, it can be expanded to a basis
      %     for the vector space $S$. 
      %   \partsitem Any basis for the subspace $S$ is a linearly independent 
      %     set in the superspace $V$.
      %     Hence it can be expanded to a basis for the superspace, which is
      %     finite dimensional.
      %     Therefore it has only finitely many members.  
      % \end{exparts}
    \end{answer}
  \item  
    Where is the finiteness of \( B \) used in
    \nearbytheorem{th:AllBasesSameSize}?
    \begin{answer}
      It ensures that we exhaust the \( \vec{\beta} \)'s.
      That is, it justifies the first sentence of the last paragraph. 
    \end{answer}
  \item
    Prove that if \( U \) and \( W \) are both three-dimensional
    subspaces of \( \Re^5 \) then \( U\intersection W \) is non-trivial.
    Generalize.
    \begin{answer}
       Let \( B_U \) be a basis for \( U \) and let \( B_W \)
       be a basis for \( W \).
       Consider the concatenation of the two basis sequences.
       If there is a repeated element then the intersection
       \( U\intersection W \) is nontrivial.
       Otherwise, 
       the set \( B_U\union B_W \) is linearly dependent as it is a
       six member subset of the five-dimensional space \( \Re^5 \).
       In either case some member of \( B_W \) is in the span of \( B_U \), and
       thus \( U\intersection W \) is more than just the trivial space
       \( \set{\zero\,} \).

       Generalization:
       if \( U,W \) are subspaces of a vector space of dimension \( n \) and
       if \( \dim(U)+\dim(W)>n \) then they have a nontrivial
       intersection.  
    \end{answer}
  \item 
    A basis for a space consists of elements of that space.
    So we are naturally led to how the property `is a basis' 
    interacts with operations $\subseteq$ and $\cap$ and $\cup$.
    (Of course, a basis is actually a sequence in that it is ordered, 
    but there is a natural extension of these operations.)
    \begin{exparts}
      \partsitem Consider first how bases might be related by $\subseteq$.
        Assume that \( U,W \) are subspaces of some vector space and
        that \( U\subseteq W \).
        Can there exist bases \( B_U \) for \( U \) and \( B_W \) for \( W \)
        such that \( B_U\subseteq B_W \)?
        Must such bases exist?

        For any basis \( B_U \) for \( U \), must there be a basis \( B_W \)
        for \( W \) such that \( B_U\subseteq B_W \)?

        For any basis \( B_W \) for \( W \), must there be a basis \( B_U \)
        for \( U \) such that \( B_U\subseteq B_W \)?

        For any bases \( B_U, B_W \) for \( U \) and \( W \), must  \( B_U \)
        be a subset of \( B_W \)?
      \partsitem Is the $\cap$ of bases a basis?
        For what space?
      \partsitem Is the $\cup$ of bases a basis?
        For what space?
      \partsitem What about the complement operation?
     \end{exparts}
     (\textit{Hint.} 
     Test any conjectures against some subspaces of \( \Re^3 \).)
     \begin{answer}
       First, note that a set is a basis for some space if and only
       if it is linearly independent, because in that case 
       it is a basis for its own span.
       \begin{exparts}
         \partsitem The answer to the question in the second paragraph
            is ``yes''
           (implying ``yes'' answers for both questions in the first
           paragraph).
           If \( B_U \) is a basis for \( U \) then
           \( B_U \) is a linearly independent subset of \( W \).
           Apply \nearbycorollary{cor:LIExpBas} to expand it to a basis for 
           \( W \).
           That is the desired \( B_W \).

           The answer to the question in the third paragraph is ``no'', which
           implies a ``no'' answer to the question of the fourth paragraph.
           Here is an example of a basis for a superspace with no sub-basis
           forming a basis for a subspace: in \( W=\Re^2 \), consider the
           standard basis \( \stdbasis_2 \).
           No sub-basis of $\stdbasis_2$ forms a basis for the 
           subspace \( U \) 
           of $\Re^2$ that is the line \( y=x \).
         \partsitem It is a basis (for its span) because the
           intersection of linearly
           independent sets is linearly independent (the intersection is a
           subset of each of the linearly independent sets).

           It is not, however, a basis for the intersection of the spaces.
           For instance, these are bases for \( \Re^2 \):
           \begin{equation*}
             B_1=\sequence{\colvec[r]{1 \\ 0},\colvec[r]{0 \\ 1}}
             \quad\text{and}\quad
             B_2=\sequence[r]{\colvec{2 \\ 0},\colvec[r]{0 \\ 2}}
           \end{equation*}
           and \( \Re^2\intersection\Re^2=\Re^2 \), but
           \( B_1\intersection B_2 \) is empty.
           All we can say is that the $\cap$ of the bases is a basis
           for a subset of the intersection of the spaces.
         \partsitem The $\cup$ of bases need not be a basis: in \( \Re^2 \)
           \begin{equation*}
             B_1=\sequence{\colvec[r]{1 \\ 0},\colvec[r]{1 \\ 1}}
             \quad\text{and}\quad
             B_2=\sequence{\colvec[r]{1 \\ 0},\colvec[r]{0 \\ 2}}
           \end{equation*}
           \( B_1\union B_2 \) is not linearly independent.
           A necessary and sufficient condition for a $\cup$ of two bases
           to be a basis 
           \begin{equation*}
             B_1\union B_2 \text{ is linearly independent }
             \quad\iff\quad
             \spanof{B_1\intersection B_2}=\spanof{B_1}\intersection
                                            \spanof{B_2}
           \end{equation*}
           it is easy enough to prove (but perhaps hard to apply).
         \partsitem The complement of a basis cannot be a basis
           because it contains the zero vector.
       \end{exparts}  
     \end{answer}
  \recommended \item 
    Consider how `dimension' interacts with `subset'.
    Assume \( U \) and \( W \) are both subspaces of some vector space, and
    that \( U\subseteq W \).
    \begin{exparts}
      \partsitem Prove that \( \dim (U)\leq\dim (W) \).
      \partsitem Prove that equality of dimension holds
        if and only if \( U=W \).
      \partsitem Show that the prior item does not hold if they are 
        infinite-dimensional.
    \end{exparts}
    \begin{answer}
       \begin{exparts}
          \partsitem A basis for \( U \) is a linearly independent set
            in \( W \)
            and so can be expanded via \nearbycorollary{cor:LIExpBas}
            to a basis for \( W \).
            The second basis has at least as many members as the first.
          \partsitem One direction is clear:~if \( V=W \) then they have the 
            same dimension.
            For the converse, let \( B_U \) be a basis for \( U \).
            It is a linearly independent subset of \( W \) and so can be
            expanded to a basis for \( W \).
            If \( \dim(U)=\dim(W) \) then this basis for \( W \) has no more
            members than does \( B_U \) and so equals \( B_U \).
            Since \( U \) and \( W \) have the same bases, they are equal.
          \partsitem Let \( W \) be the space of finite-degree polynomials and
            let \( U \) be the subspace of polynomials that have only
            even-powered terms.
            \begin{equation*}
               \set{a_0+a_1x^2+a_2x^4+\dots+a_nx^{2n}\suchthat
                a_0,\ldots,a_n\in\Re} 
            \end{equation*}
            Both spaces have infinite dimension, but \( U \) is a proper
            subspace.
       \end{exparts}  
     \end{answer}
  \puzzle \item 
    \cite{Wohascum47}
    For any vector $\vec{v}$ in $\Re^n$ and any permutation $\sigma$
    of the numbers $1$, $2$, \ldots, $n$
    (that is, $\sigma$ is a rearrangement of those numbers into a new order), 
    define $\sigma(\vec{v})$ to be the vector whose components are 
    $v_{\sigma(1)}$, $v_{\sigma(2)}$, \ldots, and $v_{\sigma(n)}$
    (where $\sigma(1)$ is the first number in the rearrangement, etc.).
    Now fix $\vec{v}$ and let $V$ be the span of 
    $\set{\sigma(\vec{v})\suchthat \mbox{$\sigma$ permutes $1$, \ldots, $n$}}$.
    What are the possibilities for the dimension of $V$?
    \begin{answer}
      The possibilities for the dimension of $V$ are $0$, $1$, $n-1$,
      and $n$.

      To see this, first consider the case when all the coordinates of 
      $\vec{v}$ are equal.
      \begin{equation*}
         \vec{v}=\colvec{z \\ z \\ \vdots \\ z}
      \end{equation*}
      Then $\sigma(\vec{v})=\vec{v}$ for every permutation $\sigma$, so
      $V$ is just the span of $\vec{v}$, which has dimension $0$ or $1$
      according to whether $\vec{v}$ is $\zero$ or not.

      Now suppose not all the coordinates of $\vec{v}$ are equal; let $x$
      and $y$ with $x\neq y$ be among the coordinates of $\vec{v}$.
      Then we can find permutations $\sigma_1$ and $\sigma_2$ such that
      \begin{equation*}
        \sigma_1(\vec{v})=\colvec{x \\ y \\ a_3 \\ \vdots \\ a_n}
        \quad\text{and}\quad
        \sigma_2(\vec{v})=\colvec{y \\ x \\ a_3 \\ \vdots \\ a_n}
      \end{equation*}
      for some $a_3,\ldots,a_n\in\Re$.
      Therefore,
      \begin{equation*}
        \frac{1}{y-x}\bigl(\sigma_1(\vec{v})-\sigma_2(\vec{v})\bigr)=
        \colvec[r]{-1 \\ 1 \\ 0 \\ \vdotswithin{-1} \\ 0}
      \end{equation*}
      is in $V$.
      That is, $\vec{e}_2-\vec{e}_1\in V$, where $\vec{e}_1$, $\vec{e}_2$,
      \ldots, $\vec{e}_n$ is the standard basis for $\Re^n$.
      Similarly, $\vec{e}_3-\vec{e}_2$, \ldots, $\vec{e}_n-\vec{e}_1$
      are all in $V$.
      It is easy to see that the vectors
      $\vec{e}_2-\vec{e}_1$, $\vec{e}_3-\vec{e}_2$, \ldots, 
      $\vec{e}_n-\vec{e}_1$ are linearly independent (that is, form a linearly
      independent set), so $\dim V\geq n-1$.

      Finally, we can write
      \begin{align*}
        \vec{v} &=x_1\vec{e}_1+x_2\vec{e}_2+\dots+x_n\vec{e}_n  \\
                &=(x_1+x_2+\dots+x_n)\vec{e}_1+x_2(\vec{e}_2-\vec{e}_1)+\dots
                   +x_n(\vec{e}_n-\vec{e}_1)
      \end{align*}
      This shows that if $x_1+x_2+\dots+x_n=0$ then $\vec{v}$ is in the span
      of $\vec{e}_2-\vec{e}_1$, \ldots, $\vec{e_n}-\vec{e}_1$ (that is, is in
      the span of the set of those vectors); similarly, each $\sigma(\vec{v})$
      will be in this span, so $V$ will equal this span and $\dim V=n-1$.
      On the other hand, if $x_1+x_2+\cdots+x_n\neq 0$ then the above equation
      shows that $\vec{e}_1\in V$ and thus $\vec{e}_1,\dots,\vec{e}_n\in V$,
      so $V=\Re^n$ and $\dim V=n$.
    \end{answer}
\end{exercises}
\index{basis|)}



























\subsection{Vector Spaces and Linear Systems}
We will now reconsider linear systems and Gauss's Method, 
aided by the tools and terms of this chapter.
We will make three points.

For the first,
recall the insight from the Chapter One that
Gauss's Method works by taking linear
combinations of rows\Dash
if two matrices are
related by row operations $A\longrightarrow\cdots\longrightarrow B$ then
each row of $B$ is a linear combination of the rows of $A$. 
Therefore, the right setting in which to study row operations in general,
and Gauss's Method in particular, is the following vector space.

\begin{definition} \label{df:RowSpace}
%<*df:RowSpace>
The \definend{row space\/}\index{matrix!row space}\index{row space}
of a matrix is the span of the set of its rows.
The \definend{row rank\/}\index{row!rank}\index{matrix!row rank}\index{row rank}
is the dimension of this space, the number of linearly independent rows.
%</df:RowSpace>
\end{definition}

\begin{example}
If
\begin{equation*}
  A=\begin{mat}[r]
          2  &3  \\
          4  &6
       \end{mat}
\end{equation*}
then \( \rowspace{A} \) is this subspace of the space of
two-component row vectors.
\begin{equation*}
   \set{c_1\cdot\rowvec{2 &3}+c_2\cdot\rowvec{4  &6}
          \suchthat c_1,c_2\in\Re }
\end{equation*}
The second row vector is linearly dependent on the first and so we can
simplify the above description to 
$\set{c\cdot\rowvec{2 &3}\suchthat c\in\Re }$.
\end{example}

\begin{lemma}     \label{le:RowSpUnchByGR}
%<*lm:RowSpUnchByGR>
If two matrices \( A \) and \( B \) are related by a row operation
\begin{equation*}
  A\grstep{\rho_i\leftrightarrow\rho_j}B 
  \quad\text{or}\quad
  A\grstep{k\rho_i}B 
  \quad\text{or}\quad
  A\grstep{k\rho_i+\rho_j}B
\end{equation*}
(for $i\neq j$ and $k\neq 0$) then their row spaces are equal.
Hence, row-equivalent matrices have the same row space and therefore
the same row rank.
%</lm:RowSpUnchByGR>
\end{lemma}

\begin{proof}
%<*pf:RowSpUnchByGR0>
Corollary One.III.\ref{cor:RowsOfEqMatsLinCombos} 
shows that when \mbox{$A\longrightarrow B$} then each row of $B$
is a linear combination of the rows of \( A \).
That is, in the above terminology, each row of \( B \) 
is an element of the row space of $A$.
Then $\rowspace{B}\subseteq\rowspace{A}$ follows because a member of the
set $\rowspace{B}$ is a linear combination of the rows of $B$, so it
is a combination of combinations of the rows of $A$,
and by the Linear Combination Lemma is also a member of $\rowspace{A}$.
%</pf:RowSpUnchByGR0>

%<*pf:RowSpUnchByGR1>
For the other set containment, 
recall Lemma One.III.\ref{le:RowOpsRev}, that row operations are reversible
so
\mbox{$A\longrightarrow B$} if and only if
\mbox{$B\longrightarrow A$}.
Then 
$\rowspace{A}\subseteq\rowspace{B}$ follows as in the previous paragraph. 
%</pf:RowSpUnchByGR1>
% Thus the two sets are equal.
\end{proof}

Of course, Gauss's Method performs the row operations systematically,
with the goal of echelon form.

\begin{lemma}  \label{le:RowsEchMatLI}
%<*lm:RowsEchMatLI>
The nonzero rows of an echelon form matrix make up a linearly independent
set.
%</lm:RowsEchMatLI>
\end{lemma}

\begin{proof}
%<*pf:RowsEchMatLI>
Lemma~One.III.\ref{le:EchFormNoLinCombo}
says that no nonzero row of an echelon form matrix is
a linear combination of the other rows.
This result just restates that in this chapter's terminology.
%</pf:RowsEchMatLI>
\end{proof}

Thus, in the language of this chapter,
Gaussian reduction works by eliminating linear dependences among rows, 
leaving the span unchanged, until no 
nontrivial linear relationships remain among the nonzero rows.
In short, Gauss's Method produces a basis for the row space.

\begin{example}
From any matrix, we can produce a basis for the row space by
performing Gauss's Method and taking the nonzero rows of the resulting
echelon form matrix.
For instance,
\begin{equation*}
  \begin{mat}[r]
    1  &3  &1  \\
    1  &4  &1  \\
    2  &0  &5
  \end{mat}
  \grstep[-2\rho_1+\rho_3]{-\rho_1+\rho_2}
  \repeatedgrstep{6\rho_2+\rho_3}
  \begin{mat}[r]
    1  &3  &1  \\
    0  &1  &0  \\
    0  &0  &3
  \end{mat}
\end{equation*}
produces the basis $\sequence{\rowvec{1 &3 &1},
            \rowvec{0 &1 &0},
            \rowvec{0 &0 &3} }$ for the row space.
This is a basis for the row space of both the starting and ending matrices, 
since the two row spaces are equal.
\end{example}

Using this technique, we can also find bases for spans 
not directly involving row vectors.

\begin{definition} \label{df:ColumnSpace}
%<*df:ColumnSpace>
The \definend{column space}\index{column!space}\index{matrix!column space}
of a matrix is the span of the set of its columns.
The \definend{column rank}\index{column!rank}\index{rank!column}
is the dimension of the column space, the number of linearly independent
columns.
%</df:ColumnSpace>
\end{definition}

Our interest in column spaces stems from our study of linear systems.
An example is that this system
\begin{equation*}
  \begin{linsys}{3}
    c_1  &+  &3c_2  &+  &7c_3  &=  &d_1  \\
   2c_1  &+  &3c_2  &+  &8c_3  &=  &d_2  \\
         &   &c_2   &+  &2c_3  &=  &d_3  \\
   4c_1  &   &      &+  &4c_3  &=  &d_4   
  \end{linsys}
\end{equation*}
has a solution if and only if the vector of \( d \)'s is a linear combination
of the other column vectors,
\begin{equation*}
  c_1\colvec[r]{1 \\ 2 \\ 0 \\ 4}
  +c_2\colvec[r]{3 \\ 3 \\ 1 \\ 0}
  +c_3\colvec[r]{7 \\ 8 \\ 2 \\ 4}
  =\colvec{d_1 \\ d_2 \\ d_3 \\ d_4}
\end{equation*}
meaning that the vector of \( d \)'s is in the column space of the
matrix of coefficients.

\begin{example}   \label{ex:BasisForColSpace}
Given this matrix,
\begin{equation*}
  \begin{mat}[r]
    1  &3  &7  \\
    2  &3  &8  \\
    0  &1  &2  \\
    4  &0  &4
  \end{mat}
\end{equation*}
to get a basis for the column space,
temporarily turn the columns into rows and reduce.
\begin{equation*}
    \begin{mat}[r]
       1  &2  &0  &4  \\
       3  &3  &1  &0  \\
       7  &8  &2  &4
    \end{mat}
  \grstep[-7\rho_1+\rho_3]{-3\rho_1+\rho_2}
  \repeatedgrstep{-2\rho_2+\rho_3}
  \begin{mat}[r]
     1  &2  &0  &4  \\
     0  &-3 &1  &-12\\
     0  &0  &0  &0
  \end{mat}
\end{equation*}
Now turn the rows back to columns.
\begin{equation*}
  \sequence{
     \colvec[r]{1 \\ 2 \\ 0 \\ 4},
     \colvec[r]{0 \\ -3 \\ 1 \\ -12} }
\end{equation*}
The result is a basis for the column space of the given matrix.
\end{example}

\begin{definition} \label{df:Transpose}
%<*df:Transpose>
The \definend{transpose}\index{transpose}\index{matrix!transpose}
of a matrix is the result of interchanging its rows and
columns, so that
column~\( j \) of the matrix \( A \) is row~\( j \)
of \( \trans{A} \) and vice versa.
%</df:Transpose>
\end{definition}
\noindent So we can summarize the prior example as ``transpose,
reduce, and transpose back.''

We can even, at the price of tolerating the as-yet-vague idea of vector
spaces being ``the same,''
use Gauss's Method to find bases for spans in other types of vector spaces.

\begin{example}
To get a basis for the span of
\( \set{x^2+x^4,2x^2+3x^4,-x^2-3x^4} \) in the space
\( \polyspace_4 \), think of these three polynomials 
as ``the same'' as the row vectors \( \rowvec{0 &0 &1 &0 &1} \),
\( \rowvec{0 &0 &2 &0 &3} \), and
\( \rowvec{0 &0 &-1 &0 &-3} \), apply Gauss's Method
\begin{equation*}
  \begin{mat}[r]
    0  &0  &1  &0  &1  \\
    0  &0  &2  &0  &3  \\
    0  &0  &-1 &0  &-3
  \end{mat}
  \grstep[\rho_1+\rho_3]{-2\rho_1+\rho_2}
  \repeatedgrstep{2\rho_2+\rho_3}
  \begin{mat}[r]
    0  &0  &1  &0  &1  \\
    0  &0  &0  &0  &1  \\
    0  &0  &0  &0  &0
  \end{mat}
\end{equation*}
and translate back to get the basis
\( \sequence{x^2+x^4,x^4} \).
(As mentioned earlier, 
we will make the phrase ``the same'' precise at the start of the next
chapter.)
\end{example}

Thus, the first point for this subsection is
that the tools of this chapter give us a more conceptual understanding of 
Gaussian reduction.

For the second point
observe that row operations on a matrix can change its column space.
\begin{equation*}
  \begin{mat}[r]
    1  &2  \\
    2  &4
  \end{mat}
  \grstep{-2\rho_1+\rho_2}
  \begin{mat}[r]
    1  &2  \\
    0  &0
  \end{mat}
\end{equation*}
The column space of the left-hand matrix contains vectors with
a second component that is nonzero
but the column space of the right-hand matrix 
contains only vectors whose second component is zero, so the two spaces 
are different.
This observation makes next result surprising.

\begin{lemma} \label{le:RowOpsNoChngColRnk}
%<*lm:RowOpsNoChngColRnk>
Row operations do not change the column rank.
%</lm:RowOpsNoChngColRnk>
\end{lemma}

\begin{proof}
%<*pf:RowOpsNoChngColRnk>
Restated, if $A$ reduces to $B$ 
then the column rank of $B$ equals the column rank of $A$.

This proof will be finished if we show that row operations do not affect 
linear relationships among columns, because the column rank is the size
of the largest set of unrelated columns.
That is, we will show that a relationship exists among columns
(such as that the fifth column is twice the
second plus the fourth) if and only if that relationship exists after
the row operation. 
But this is exactly the first theorem of this book,
Theorem One.I.\ref{th:GaussMethod}:~in a relationship
among columns,
\begin{equation*}
  c_1\cdot\colvec{a_{1,1} \\ a_{2,1} \\ \vdots \\ a_{m,1}}
   +\dots+
  c_n\cdot \colvec{a_{1,n} \\ a_{2,n} \\ \vdots \\ a_{m,n}}
   =\colvec[r]{0 \\ 0 \\ \vdotswithin{0} \\ 0}
\end{equation*}
row operations leave unchanged the set of solutions \( (c_1,\ldots,c_n) \).
%</pf:RowOpsNoChngColRnk>
\end{proof}

Another way to make the point that 
Gauss's Method has something to say about the column space as well as about 
the row space is with Gauss-Jordan reduction.
It ends with
the reduced echelon form of a matrix, as here.
\begin{equation*}
  \begin{mat}[r]
    1  &3  &1  &6  \\
    2  &6  &3  &16 \\
    1  &3  &1  &6
  \end{mat}
  \grstep{}\cdots\grstep{}
  \begin{mat}[r]
    1  &3  &0  &2  \\
    0  &0  &1  &4  \\
    0  &0  &0  &0
  \end{mat}
\end{equation*}
Consider the row space and the column space of this result.

The first point made earlier in this subsection says
that to get a basis for the row space we can just collect the rows with 
leading entries.
However, because this is in reduced echelon form, 
a basis for the column space is just as easy:~collect the
columns containing the leading entries,
\( \sequence{\vec{e}_1,\vec{e}_2} \).
% (Linear independence is obvious.
% As to span, the other columns are in the span 
% since they all have a third component of zero.)
Thus, for a reduced echelon form matrix
we can find bases for the row and column spaces
in essentially the same way, by taking the parts of the 
matrix, the rows or columns, containing the leading entries.

\begin{theorem} \label{th:RowRankEqualsColumnRank}
%<*th:RowRankEqualsColumnRank>
For any matrix, 
the row rank and column rank are equal.
%</th:RowRankEqualsColumnRank>
\end{theorem}

\begin{proof}
%<*pf:RowRankEqualsColumnRank0>
Bring the matrix to reduced echelon form.
Then the 
row rank equals the number of leading entries since that equals the
number of nonzero rows.
Then also, the number of leading entries
equals the column rank because the set of columns containing leading
entries consists of some of the \( \vec{e}_i \)'s from a standard
basis, and that set is linearly independent and spans the set of
columns.
Hence, in the reduced echelon form matrix, the row rank equals the column
rank, because each equals the number of leading entries.
%</pf:RowRankEqualsColumnRank0>

%<*pf:RowRankEqualsColumnRank1>
But \nearbylemma{le:RowSpUnchByGR} and \nearbylemma{le:RowOpsNoChngColRnk} 
show that the row rank and column rank are not changed
by using row operations to get to reduced echelon form.
Thus the row rank and the column rank of the original matrix are also equal.
%</pf:RowRankEqualsColumnRank1>
\end{proof}

\begin{definition} \label{df:Rank}
%<*df:Rank>
The \definend{rank\/}\index{rank} of a matrix is its row rank or column rank.
%</df:Rank>
\end{definition}

So the second point that we have made 
in this subsection is that the column space and row
space of a matrix have the same dimension.

Our final point is that the concepts that
we've seen arising naturally in the study of
vector spaces are exactly the ones that we have studied with linear systems.

\begin{theorem}  \label{th:RankVsSoltnSp}
%<*tm:RankVsSoltnSp>
For linear systems with \( n \) unknowns and with matrix of coefficients
\( A \), the statements
\begin{tfae}
   \item the rank of \( A \) is \( r \)
   \item the vector space of solutions of the associated homogeneous system has
     dimension \( n-r \)
\end{tfae}
are equivalent.
%</tm:RankVsSoltnSp>
\end{theorem}

\par\noindent So if the system has at least one particular solution 
then for the set of
solutions, the number of parameters equals $n-r$, the number of variables
minus the rank of the matrix of coefficients. 

\begin{proof}
%<*pf:RankVsSoltnSp>
The rank of \( A \) is \( r \) if and only if Gaussian reduction on \( A \)
ends with \( r \) nonzero rows.
That's true if and only if echelon form matrices row equivalent to \( A \)
have \( r \)-many leading variables.
That in turn holds if and only if there are \( n-r \) free variables.
%</pf:RankVsSoltnSp>
\end{proof}

\begin{corollary} \label{co:EquivToNonsingular}
%<*co:EquivToNonsingular>
Where the matrix $A$ is \( \nbyn{n} \), these statements 
\begin{tfae}
  \item the rank of \( A \) is \( n \)
  \item \( A \) is nonsingular
  \item the rows of \( A \) form a linearly independent set
  \item the columns of \( A \) form a linearly independent set
  \item any linear system whose matrix of coefficients is \( A \) has one and
    only one solution
\end{tfae}
are equivalent.
%</co:EquivToNonsingular>
\end{corollary}

\begin{proof}
%<*pf:EquivToNonsingular>
Clearly \( \text{(1)}\iff\text{(2)}\iff\text{(3)}\iff\text{(4)} \).
The last, \( \text{(4)}\iff\text{(5)} \), holds because a set of \( n \)
column vectors is linearly independent if and only if it is a basis for
\( \Re^n \), but the system
\begin{equation*}
  c_1\colvec{a_{1,1} \\ a_{2,1} \\ \vdots \\ a_{m,1}}
   +\dots+
  c_n\colvec{a_{1,n} \\ a_{2,n} \\ \vdots \\ a_{m,n}}
   =\colvec{d_1 \\ d_2 \\ \vdots \\ d_m}
\end{equation*}
has a unique solution for all choices of \( d_1,\dots,d_n\in\Re \) if and only
if the vectors of \( a \)'s on the left form a basis.
%</pf:EquivToNonsingular>
\end{proof}

\begin{remark}\cite{Munkres}  \label{rem:MunkresCommment}
Sometimes the results of this subsection are 
mistakenly remembered to say that the general solution
of an \( n \)~unknowns system of \( m \)~equations uses \( n-m \) parameters.
The number of equations is not the relevant figure, rather, what matters
is the number of independent equations, the number of equations in a maximal
independent set.
Where there are \( r \) independent equations, the general solution involves
\( n-r \) parameters.
\end{remark}


\begin{exercises}
  \item 
    Transpose each.
    \begin{exparts*}
      \partsitem \( \begin{mat}[r]
                 2  &1  \\
                 3  &1
               \end{mat}  \)
      \partsitem \( \begin{mat}[r]
                 2  &1  \\
                 1  &3
               \end{mat}  \)
      \partsitem \( \begin{mat}[r]
                 1  &4  &3 \\
                 6  &7  &8
               \end{mat}  \)
      \partsitem \( \colvec[r]{0 \\ 0 \\ 0} \)
      \partsitem \( \rowvec{-1 &-2} \)
    \end{exparts*}
    \begin{answer}  
      \begin{exparts*}
        \partsitem \( \begin{mat}[r]
                   2  &3  \\
                   1  &1
                 \end{mat}  \)
        \partsitem \( \begin{mat}[r]
                   2  &1  \\
                   1  &3
                 \end{mat}  \)
        \partsitem \( \begin{mat}[r]
                   1  &6  \\
                   4  &7  \\
                   3  &8
                 \end{mat}  \)
        \partsitem \( \rowvec{0 &0 &0} \)
        \partsitem \( \colvec[r]{-1 \\ -2} \)
      \end{exparts*}  
    \end{answer}
  \recommended \item 
    Decide if the vector is in the row space of the matrix.
    \begin{exparts*}
       \partsitem \( \begin{mat}[r]
                  2  &1  \\
                  3  &1
                \end{mat}  \),
             \( \rowvec{1 &0} \)
       \partsitem \( \begin{mat}[r]
                  0  &1  &3  \\
                 -1  &0  &1  \\
                 -1  &2  &7
                \end{mat}  \),
             \( \rowvec{1 &1 &1} \)
    \end{exparts*}
    \begin{answer}  
       \begin{exparts}
         \partsitem Yes.
           To see if there are $c_1$ and $c_2$ such that
           \( c_1\cdot\rowvec{2 &1}+c_2\cdot\rowvec{3 &1}=\rowvec{1 &0} \)
           we solve
           \begin{equation*}
              \begin{linsys}{2}
                 2c_1  &+  &3c_2  &=  &1  \\
                  c_1  &+  &c_2   &=  &0  
              \end{linsys}
           \end{equation*}
           and get \( c_1=-1 \) and \( c_2=1 \). 
           Thus the vector is in the row space.
         \partsitem No.
           The equation
           \( c_1\rowvec{0 &1 &3}
              +c_2\rowvec{-1 &0 &1}
              +c_3\rowvec{-1 &2 &7}
              =\rowvec{1 &1 &1} \)
           has no solution.
           \begin{equation*}
             \begin{amat}[r]{3}
               0  &-1  &-1  &1  \\
               1  &0   &2   &1  \\
               3  &1   &7   &1
             \end{amat}
             \grstep{\rho_1\leftrightarrow\rho_2}
             \grstep{-3\rho_1+\rho_2}
             \repeatedgrstep{\rho_2+\rho_3}
             \begin{amat}[r]{3}
               1  &0   &2   &1  \\
               0  &-1  &-1  &1  \\
               0  &0   &0   &-1
             \end{amat}
           \end{equation*}
           Thus, the vector is not in the row space.
      \end{exparts}  
    \end{answer}
  \recommended \item 
    Decide if the vector is in the column space.
    \begin{exparts*}
      \partsitem \( \begin{mat}[r]
                 1  &1  \\
                 1  &1
               \end{mat}  \),
            \( \colvec[r]{1 \\ 3} \)
      \partsitem \( \begin{mat}[r]
                 1  &3  &1 \\
                 2  &0  &4 \\
                 1  &-3 &-3
               \end{mat}  \),
            \( \colvec[r]{1 \\ 0 \\ 0} \)
    \end{exparts*}
    \begin{answer}
       \begin{exparts}
          \partsitem No.
            To see if there are \( c_1,c_2\in\Re \) such that
            \begin{equation*}
              c_1\colvec[r]{1 \\ 1}
              +c_2\colvec[r]{1 \\ 1}
              =\colvec[r]{1 \\ 3}
            \end{equation*}
            we can use Gauss's Method on the resulting linear system.
            \begin{equation*}
              \begin{linsys}{2}
                 c_1  &+  &c_2  &=  &1  \\
                 c_1  &+  &c_2  &=  &3  
              \end{linsys}
              \grstep{-\rho_1+\rho_2}
              \begin{linsys}{2}
                 c_1  &+  &c_2  &=  &1  \\
                      &   &0    &=  &2  
              \end{linsys}
            \end{equation*}
            There is no solution and so the vector is not in the column space.
          \partsitem Yes.
            From this relationship
            \begin{equation*}
              c_1\colvec[r]{1 \\ 2 \\ 1}
              +c_2\colvec[r]{3 \\ 0 \\ -3}
              +c_3\colvec[r]{1 \\ 4 \\ 3}
              =\colvec[r]{1 \\ 0 \\ 0}
            \end{equation*}
            we get a linear system that, when we apply Gauss's Method,
            \begin{equation*}
              \begin{amat}[r]{3}
                1  &3  &1  &1  \\
                2  &0  &4  &0  \\
                1  &-3 &-3 &0
              \end{amat}
              \grstep[-\rho_1+\rho_3]{-2\rho_1+\rho_2}
              \repeatedgrstep{-\rho_2+\rho_3}
              \begin{amat}[r]{3}
                1  &3  &1  &1  \\
                0  &-6 &2  &-2 \\
                0  &0  &-6 &1
              \end{amat}
            \end{equation*}
            yields a solution.
            Thus, the vector is in the column space.
      \end{exparts}  
    \end{answer}
  \recommended \item  
    Decide if the vector is in the column space of the matrix.
    \begin{exparts*}
      \partsitem \( \begin{mat}[r]
                   2  &1  \\
                   2  &5
                 \end{mat} \),~\( \colvec[r]{1 \\ -3}  \)
      \partsitem \( \begin{mat}[r]
                   4  &-8 \\
                   2  &-4
                 \end{mat} \),~\( \colvec[r]{0 \\ 1}  \)
      \partsitem \( \begin{mat}[r]
                   1  &-1  &1  \\
                   1  &1   &-1 \\
                  -1  &-1  &1
                \end{mat} \),~\( \colvec[r]{2 \\ 0 \\ 0}  \)
    \end{exparts*}
    \begin{answer}
      \begin{exparts}
        \partsitem Yes;
          we are asking if there are scalars \( c_1 \) and \( c_2 \) such that
          \begin{equation*}
            c_1\colvec[r]{2 \\ 2}+c_2\colvec[r]{1 \\ 5}=\colvec[r]{1 \\ -3}
          \end{equation*}
          which gives rise to a linear system
          \begin{equation*}
            \begin{linsys}{2}
              2c_1  &+  &c_2  &=  &1  \\
              2c_1  &+  &5c_2 &=  &-3
            \end{linsys}
            \grstep{-\rho_1+\rho_2}
            \begin{linsys}{2}
              2c_1  &+  &c_2  &=  &1  \\
                    &   &4c_2 &=  &-4
            \end{linsys}
          \end{equation*}
          and Gauss's Method produces \( c_2=-1 \) and \( c_1=1 \). 
          That is, there is indeed such a pair of scalars and so the vector
          is indeed in the column space of the matrix.
        \partsitem No;
          we are asking if there are scalars $c_1$ and $c_2$
          such that 
          \begin{equation*}
            c_1\colvec[r]{4 \\ 2}+c_2\colvec[r]{-8 \\ -4}=\colvec[r]{0 \\ 1}
          \end{equation*}
          and one way to proceed is to consider the resulting linear system
          \begin{equation*}
            \begin{linsys}{2}
              4c_1  &-  &8c_2  &=  &0  \\
              2c_1  &-  &4c_2  &=  &1
            \end{linsys}
          \end{equation*}
          that is easily seen to have no solution.
          Another way to proceed is to note
          that any linear combination of the columns on the left
          has a second component half as big as its first component, 
          but the  vector on the right does not meet that criterion.
        \partsitem Yes; we can simply observe that the vector
          is the first column minus the second.
          Or, failing that, setting up the relationship among the columns 
          \begin{equation*}
            c_1\colvec[r]{1 \\ 1 \\ -1}
             +c_2\colvec[r]{-1 \\ 1 \\ -1}
             +c_3\colvec[r]{1 \\ -1 \\ 1}
             =\colvec[r]{2 \\ 0 \\ 0}
          \end{equation*}
          and considering the resulting linear system
          \begin{equation*}
            \begin{linsys}{3}
              c_1  &-  &c_2  &+  &c_3  &=  &2  \\
              c_1  &+  &c_2  &-  &c_3  &=  &0  \\
             -c_1  &-  &c_2  &+  &c_3  &=  &0    
            \end{linsys}
            \grstep[\rho_1+\rho_3]{-\rho_1+\rho_2}
            \begin{linsys}{3}
              c_1  &-  &c_2  &+  &c_3  &=  &2  \\
                   &   &2c_2 &-  &2c_3 &=  &-2  \\
                   &   &-2c_2&+  &2c_3 &=  &2    
            \end{linsys}
            \grstep{\rho_2+\rho_3}
            \begin{linsys}{3}
              c_1  &-  &c_2  &+  &c_3  &=  &2  \\
                   &   &2c_2 &-  &2c_3 &=  &-2  \\
                   &   &     &   &0    &=  &0    
            \end{linsys}
          \end{equation*}
          gives the additional information (beyond that there is at least one
          solution) that there are infinitely many solutions.
          Parametrizing gives $c_2=-1+c_3$ and $c_1=1$, and so taking $c_3$ to 
          be zero gives a particular solution of $c_1=1$, $c_2=-1$, and
          $c_3=0$ (which is, of course, the observation made at the start).
      \end{exparts}  
    \end{answer}
  \recommended \item  
    Find a basis for the row space of this matrix.
    \begin{equation*}
      \begin{mat}[r]
         2  &0  &3  &4  \\
         0  &1  &1  &-1 \\
         3  &1  &0  &2  \\
         1  &0  &-4 &-1
       \end{mat}
    \end{equation*}
    \begin{answer}
     % Using Sage:
     % M = matrix(QQ, [[2,0,3,4], [0,1,1,-1], [3,1,0,2], [1,0,-4,-1]])
     % M1 = M.with_added_multiple_of_row(2,0,-3/2)
     % M2 = M1.with_added_multiple_of_row(3,0,-1/2)
     % M3 = M2.with_added_multiple_of_row(2,1,-1)
     % M4 = M3.with_added_multiple_of_row(3,2,-1)
     A routine Gaussian reduction
     \begin{equation*}
       \begin{mat}[r]
         2  &0  &3 &4   \\
         0  &1  &1  &-1 \\
         3  &1  &0  &2  \\
         1  &0  &-4  &-1
       \end{mat}
       \grstep[-(1/2)\rho_1+\rho_4]{-(3/2)\rho_1+\rho_3}
       \grstep{-\rho_2+\rho_3}
       \repeatedgrstep{-\rho_3+\rho_4}
       \begin{mat}[r]
         2  &0  &3      &4   \\
         0  &1  &1      &-1 \\
         0  &0  &-11/2  &-3  \\
         0  &0  &0      &0
       \end{mat}
     \end{equation*}
     suggests this basis
     $\sequence{\rowvec{2 &0 &3 &4},
         \rowvec{0 &1 &1 &-1},\rowvec{0 &0 &-11/2 &-3}}$.

      Another procedure, perhaps more convenient, is to swap rows first,
      % From Sage
      % M = matrix(QQ, [[2,0,3,4], [0,1,1,-1], [3,1,0,2], [1,0,-4,-1]])
      % M1 = M.with_swapped_rows(0,3)
      % M2 = M1.with_added_multiple_of_row(2,0,-3)
      % M3 = M2.with_added_multiple_of_row(3,0,-2)
      % M4 = M3.with_added_multiple_of_row(2,1,-1)
      % M5 = M4.with_added_multiple_of_row(3,2,-1)
      \begin{equation*}
        \grstep{\rho_1\swap\rho_4}
        \repeatedgrstep[-2\rho_1+\rho_4]{-3\rho_1+\rho_3}
        \repeatedgrstep{-\rho_2+\rho_3}
        \repeatedgrstep{-\rho_3+\rho_4}
        \begin{mat}[r]
           1  &0  &-4 &-1 \\
           0  &1  &1  &-1 \\
           0  &0  &11 &6  \\
           0  &0  &0  &0
         \end{mat}
      \end{equation*}
      leading to the basis
      \( \sequence{\rowvec{1 &0 &-4 &-1},\rowvec{0 &1 &1 &-1},
                   \rowvec{0 &0 &11 &6}} \).  
    \end{answer}
  \recommended \item 
    Find the rank of each matrix.
    \begin{exparts*}
      \partsitem \(
        \begin{mat}[r]
          2  &1  &3  \\
          1  &-1 &2  \\
          1  &0  &3
        \end{mat}  \)
      \partsitem \(
        \begin{mat}[r]
          1  &-1 &2  \\
          3  &-3 &6  \\
         -2  &2  &-4
        \end{mat}  \)
      \partsitem \(
        \begin{mat}[r]
          1  &3  &2  \\
          5  &1  &1  \\
          6  &4  &3
        \end{mat}  \)
      \partsitem \(
        \begin{mat}[r]
          0  &0  &0  \\
          0  &0  &0  \\
          0  &0  &0
        \end{mat}  \)
    \end{exparts*}
    \begin{answer}
      \begin{exparts}
         \partsitem This reduction
           \begin{equation*}
             \mbox{}
             \grstep[-(1/2)\rho_1+\rho_3]{-(1/2)\rho_1+\rho_2}
             \repeatedgrstep{-(1/3)\rho_2+\rho_3}
             \begin{mat}[r]
               2  &1     &3     \\
               0  &-3/2  &1/2   \\
               0  &0     &4/3
             \end{mat}
           \end{equation*}
           shows that the row rank, and hence the rank, is three.
         \partsitem Inspection of the columns shows that the others
           are multiples of the first (inspection of the rows shows the same
           thing).
           Thus the rank is one.
           
           Alternatively, the reduction
           \begin{equation*}
             \begin{mat}[r]
               1  &-1  &2  \\
               3  &-3  &6  \\   
               -2 &2   &-4
             \end{mat}
             \grstep[2\rho_1+\rho_3]{-3\rho_1+\rho_2}
             \begin{mat}[r]
               1  &-1  &2  \\
               0  &0   &0  \\   
               0  &0   &0 
             \end{mat}
           \end{equation*}
           shows the same thing.
         \partsitem This calculation
           \begin{equation*}
             \begin{mat}[r]
               1  &3  &2  \\
               5  &1  &1  \\
               6  &4  &3  
             \end{mat}
             \grstep[-6\rho_1+\rho_3]{-5\rho_1+\rho_2}
             \repeatedgrstep{-\rho_2+\rho_3}
             \begin{mat}[r]
               1  &3   &2  \\
               0  &-14 &-9 \\
               0  &0   &0  
             \end{mat}
           \end{equation*}
           shows that the rank is two.
         \partsitem The rank is zero.
       \end{exparts}  
     \end{answer}
  \item Give a basis for the column space of this matrix.
    Give the matrix's rank. 
    \begin{equation*}
      \begin{mat}
        1 &3 &-1 &2 \\
        2 &1 &1  &0 \\
        0 &1 &1  &4
      \end{mat}
    \end{equation*}
  \begin{answer}
    We want a basis for this span.
    \begin{equation*}
      \spanof{\colvec{1 \\ 2 \\ 0},
              \colvec{3 \\ 1 \\ 1},
              \colvec{-1 \\ 1 \\ 1},
              \colvec{2 \\ 0 \\ 4}}\subseteq\Re^3
    \end{equation*}
    The most straightforward approach 
    is to transpose those columns to rows, use Gauss's Method
    to find a basis for the span of the rows, and then transpose them back to 
    columns. 
    \begin{equation*}
      \begin{mat}
        1 &2 &0 \\
        3 &1 &1 \\
       -1 &1 &1 \\
        2 &0 &4
      \end{mat}
      \grstep[\rho_1+\rho_3 \\ -2\rho_1+\rho_4]{-3\rho_1+\rho_2}
      \grstep[-(4/5)\rho_2+\rho_4]{(3/5)\rho_2+\rho_3}
      \grstep{-2\rho_3+\rho_4}
      \begin{mat}
        1 &2  &0 \\
        0 &-5 &1 \\
        0 &0  &8/5 \\
        0 &0  &0
      \end{mat}
    \end{equation*}
    Discard the zero vector as showing that there was  a redundancy among the 
    starting vectors, to get this basis for the column space.
    \begin{equation*}
      \sequence{
        \colvec{1 \\ 2 \\ 0},
        \colvec{0 \\ -5 \\ 1},
        \colvec{0 \\ 0 \\ 8/5}
        }
    \end{equation*}
    The matrix's rank is the dimension of its column space, so it is three.
    (It is also equal to the dimension of its row space.)  
  \end{answer}
  \recommended \item 
    Find a basis for the span of each set.
    \begin{exparts}
      \partsitem \(
        \set{\rowvec{1 &3},
          \rowvec{-1 &3},
          \rowvec{1 &4},
          \rowvec{2 &1}  }\subseteq\matspace_{\nbym{1}{2}} \)
      \partsitem \(
         \set{\colvec[r]{1 \\2 \\1},
           \colvec[r]{3 \\ 1 \\ -1},
           \colvec[r]{1 \\ -3 \\ -3}  }\subseteq\Re^3  \)
      \partsitem \(  \set{1+x,1-x^2,3+2x-x^2}\subseteq\polyspace_3  \)
      \partsitem \( \set{
          \begin{mat}[r]
            1  &0  &1  \\
            3  &1  &-1
          \end{mat},
          \begin{mat}[r]
            1  &0  &3  \\
            2  &1  &4
          \end{mat},
          \begin{mat}[r]
           -1  &0  &-5 \\
           -1  &-1 &-9
          \end{mat}  }  \subseteq\matspace_{\nbym{2}{3}}  \)
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem This reduction
          \begin{equation*}
             \begin{mat}[r]
               1  &3  \\
              -1  &3  \\
               1  &4  \\
               2  &1
             \end{mat}
             \grstep[-\rho_1+\rho_3 \\ -2\rho_1+\rho_4]{\rho_1+\rho_2}
             \grstep[(5/6)\rho_2+\rho_4]{-(1/6)\rho_2+\rho_3}
             \begin{mat}[r]
               1  &3  \\
               0  &6  \\
               0  &0  \\
               0  &0
             \end{mat}
          \end{equation*}
          gives \( \sequence{\rowvec{1 &3},\rowvec{0 &6}} \).
        \partsitem Transposing and reducing
          \begin{equation*}
             \begin{mat}[r]
               1  &2  &1  \\
               3  &1  &-1 \\
               1  &-3 &-3
             \end{mat}
             \grstep[-\rho_1+\rho_3]{-3\rho_1+\rho_2}
             \begin{mat}[r]
               1  &2  &1  \\
               0  &-5 &-4 \\
               0  &-5 &-4
             \end{mat}
             \grstep{-\rho_2+\rho_3}
             \begin{mat}[r]
               1  &2  &1  \\
               0  &-5 &-4 \\
               0  &0  &0
             \end{mat}
          \end{equation*}
          and then transposing back gives this basis.
          \begin{equation*}
            \sequence{\colvec[r]{1 \\ 2 \\ 1},
              \colvec[r]{0 \\ -5 \\ -4}   }
          \end{equation*}
        \partsitem Notice first that the surrounding space is as 
          $\polyspace_3$, not $\polyspace_2$. 
          Then, taking the first polynomial $1+1\cdot x+0\cdot x^2+0\cdot x^3$ 
          to be ``the same'' as the row vector $\rowvec{1 &1 &0 &0}$, etc., 
          leads to
          \begin{equation*}
            \begin{mat}[r]
              1  &1  &0  &0 \\
              1  &0  &-1 &0 \\
              3  &2  &-1 &0
            \end{mat}
            \grstep[-3\rho_1+\rho_3]{-\rho_1+\rho_2}
            \repeatedgrstep{-\rho_2+\rho_3}
            \begin{mat}[r]
              1  &1  &0  &0 \\
              0  &-1 &-1 &0 \\
              0  &0  &0  &0
            \end{mat}
          \end{equation*}
          which yields the basis \( \sequence{1+x,-x-x^2} \).
        \partsitem Here ``the same'' gives
          \begin{equation*}
            \begin{mat}[r]
              1  &0  &1  &3  &1  &-1  \\
              1  &0  &3  &2  &1  &4   \\
             -1  &0  &-5  &-1 &-1 &-9
            \end{mat}
            \grstep[\rho_1+\rho_3]{-\rho_1+\rho_2}
            \repeatedgrstep{2\rho_2+\rho_3}
            \begin{mat}[r]
              1  &0  &1  &3  &1  &-1  \\
              0  &0  &2  &-1 &0  &5   \\
              0  &0  &0  &0  &0  &0
            \end{mat}
          \end{equation*}
          leading to this basis.
          \begin{equation*}
            \sequence{
            \begin{mat}[r]
              1  &0  &1  \\
              3  &1  &-1
            \end{mat},
            \begin{mat}[r]
              0  &0  &2  \\
             -1  &0  &5
            \end{mat}  }
          \end{equation*}
      \end{exparts}   
     \end{answer}
  \item Give a basis for the span of each set, in the natural vector space.
    \begin{exparts}
      \partsitem
         $\set{\colvec{1 \\ 1 \\ 3},
              \colvec{-1 \\ 2 \\ 0},
              \colvec{0  \\ 12 \\ 6}}$
      \partsitem $\set{x+x^2, 2-2x, 7, 4+3x+2x^2}$
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem
          Transpose the columns to rows, bring to echelon form 
          (and then lose any zero rows), and transpose back to columns.
          \begin{equation*}
            \begin{mat}
              1 &1  &3 \\
             -1 &2  &0 \\
              0 &12 &6
            \end{mat}
            \grstep{\rho_1+\rho_2}
            \grstep{-4\rho_2+\rho_3}
            \begin{mat}
              1 &1  &3 \\
              0 &3  &3 \\
              0 &0  &-6
            \end{mat}
          \end{equation*}
          One basis for the span is this.
          \begin{equation*}
            \sequence{
              \colvec{1 \\ 1 \\ 3},
              \colvec{0 \\ 3 \\ 3},
              \colvec{0 \\ 0 \\ -6}
          }
          \end{equation*}
       \partsitem
          As in the prior part we think of those as rows, to take advantage of 
          the work we've done with Gauss's Method.
          \begin{multline*}
            \begin{mat}
              0 &1  &1 \\
              2 &-2 &0 \\
              7 &0 &0 \\
              4 &3  &2
            \end{mat}
            \grstep{\rho_1\leftrightarrow\rho_2}
            \grstep[2\rho_1+\rho_4]{-(7/2)\rho_1+\rho_3}
            \grstep[-7\rho_2+\rho_4]{-7\rho_2+\rho_3}          \\
            \grstep{-(5/7)\rho_3+\rho_4}
            \begin{mat}
              2 &-2  &0 \\
              0 &1 &1 \\
              0 &0  &-7 \\
              0 &0 &0
            \end{mat}
          \end{multline*}
          One basis for the span of that set is 
          $\sequence{2-2x, x+x^2, -5x^2}$.
      \end{exparts}
    \end{answer}
  \item 
    Which matrices have rank zero?
    Rank one?
    \begin{answer}
      Only the zero matrices have rank of zero.
      The only matrices of rank one have the form
      \begin{equation*}
        \begin{mat}
           k_1\cdot \rho  \\
            \vdots  \\
           k_m\cdot \rho
        \end{mat}
      \end{equation*}
      where \( \rho \) is some nonzero row vector, and not all of the 
      \( k_i \)'s are zero.
      (\textit{Remark.}
      We can't simply say that all of the rows are multiples of the first
      because the first row might be the zero row.
      \textit{Another Remark.}
      The above also applies with `column' replacing `row'.)  
    \end{answer}
  \recommended \item
    Given \( a,b,c\in\Re \), what choice of \( d \) will cause this matrix 
    to have the rank of one?
    \begin{equation*}
      \begin{mat}
         a  &b  \\
         c  &d
      \end{mat}
    \end{equation*}
    \begin{answer}
      If \( a\neq 0 \) then a choice of \( d=(c/a)b \) will make the second
      row be a multiple of the first, specifically, \( c/a \) times the first.
      If \( a=0 \) and \( b=0 \) then any non-\( 0 \) choice for \( d \) 
      will ensure that
      the second row is nonzero.
      If \( a=0 \) and \( b\neq 0 \) and \( c=0 \) then any choice for \( d \)
      will do, since the matrix will automatically have rank one (even with
      the choice of $d=0$).
      Finally, if \( a=0 \) and \( b\neq 0 \) and \( c\neq 0 \) then
      no choice for \( d \) will suffice because the matrix is sure to have 
      rank two.   
    \end{answer}
  \item 
    Find the column rank of this matrix.
    \begin{equation*}
      \begin{mat}[r]
        1  &3  &-1  &5  &0  &4  \\
        2  &0  &1   &0  &4  &1
      \end{mat}
    \end{equation*}
    \begin{answer}
      The column rank is two.
      One way to see this is by inspection\Dash the column space consists of
      two-tall columns and so can have a dimension of at least two, and we
      can easily find two columns that together form a linearly independent
      set (the fourth and fifth columns, for instance).
      Another way to see this is to recall that 
      the column rank equals the row rank, and to perform Gauss's Method, 
      which leaves two nonzero rows.
    \end{answer}
  \item 
    Show that a linear system with at least one solution has at most 
    one solution if
    and only if the matrix of coefficients has rank equal to the number of
    its columns.
    \begin{answer}
      We apply \nearbytheorem{th:RankVsSoltnSp}.  
      The number of columns of a matrix of coefficients $A$ of a linear 
      system equals the number $n$ of unknowns.
      A linear system with at least one solution has at most one solution
      if and only if the space of solutions of the associated homogeneous
      system has dimension zero (recall:~in the 
      `$\text{General}=\text{Particular}+\text{Homogeneous}$' equation
       $\vec{v}=\vec{p}+\vec{h}$, provided that such a $\vec{p}$ exists,
       the solution $\vec{v}$ is unique if and only if the vector $\vec{h}$
       is unique, namely $\vec{h}=\zero$).
       But that means, by the theorem, that $n=r$.
    \end{answer}
  \recommended \item
    If a matrix is \( \nbym{5}{9} \), which set must be dependent, its set of
    rows or its set of columns?
    \begin{answer}
      The set of columns must be dependent because the rank of the matrix
      is at most five while there are nine columns.  
    \end{answer}
  \item 
    Give an example to show that, despite that they have the same 
    dimension, the row space and column space of a matrix need not be equal.
    Are they ever equal?
    \begin{answer}
      There is little danger of their being equal since the row space
      is a set of row vectors while the column space is a set of
      columns (unless the matrix is \( \nbyn{1} \), in which case 
      the two spaces must be equal).

      \textit{Remark.}
      Consider
      \begin{equation*}
        A=\begin{mat}[r]
            1  &3  \\
            2  &6
          \end{mat}
      \end{equation*}
      and note that the row space is the set of 
      all multiples of \( \rowvec{1 &3} \)
      while the column space consists of multiples of
      \begin{equation*}
        \colvec[r]{1 \\ 2}
      \end{equation*}
      so we also cannot argue that the two spaces must be simply 
      transposes of each other.  
    \end{answer}
  \item 
    Show that the set
    \( \set{(1,-1,2,-3),(1,1,2,0),(3,-1,6,-6)} \) does not have the
    same span as \( \set{(1,0,1,0),(0,2,0,3)} \).
    What, by the way, is the vector space?
    \begin{answer}
      First, the vector space is the set of four-tuples of real numbers, under
      the natural operations.
      Although this is not the set of four-wide row vectors, the difference
      is slight\Dash it is ``the same'' as that set.
      So we will treat the four-tuples like four-wide vectors.

      With that, one way to see that \( (1,0,1,0) \) is not in the span 
      of the first set is to note that this reduction
      \begin{equation*}
        \begin{mat}[r]
          1  &-1  &2  &-3  \\
          1  &1   &2  &0   \\
          3  &-1  &6  &-6
        \end{mat}
        \grstep[-3\rho_1+\rho_3]{-\rho_1+\rho_2}
        \repeatedgrstep{-\rho_2+\rho_3}
        \begin{mat}[r]
          1  &-1  &2  &-3  \\
          0  &2   &0  &3   \\
          0  &0   &0  &0
        \end{mat}
      \end{equation*}
      and this one
      \begin{equation*}
        \begin{mat}[r]
          1  &-1  &2  &-3  \\
          1  &1   &2  &0   \\
          3  &-1  &6  &-6  \\
          1  &0   &1  &0
        \end{mat}
        \grstep[-3\rho_1+\rho_3 \\ -\rho_1+\rho_4]{-\rho_1+\rho_2}
        \repeatedgrstep[-(1/2)\rho_2+\rho_4]{-\rho_2+\rho_3}
        \repeatedgrstep{\rho_3\leftrightarrow\rho_4}
        \begin{mat}[r]
          1  &-1  &2  &-3  \\
          0  &2   &0  &3   \\
          0  &0   &-1 &3/2 \\
          0  &0   &0  &0   
        \end{mat}
      \end{equation*}
      yield matrices differing in rank.
      This means that addition of $(1,0,1,0)$ to the set of the first three
      four-tuples increases the rank, and hence the span, of that set.
      Therefore $(1,0,1,0)$ is not already in the span.
    \end{answer}
  \recommended \item 
    Show that this set of column vectors
    \begin{equation*}
      \set{\colvec{d_1 \\ d_2 \\ d_3}
           \suchthat
           \text{there are $x$, $y$, and $z$ such that:\ }
           \begin{linsys}{3}
             3x  &+  &2y  &+  &4z  &=   &d_1   \\
              x  &   &    &-  &z   &=   &d_2   \\
             2x  &+  &2y  &+  &5z  &=   &d_3   
           \end{linsys}
           }
    \end{equation*}
    is a subspace of \( \Re^3 \).
    Find a basis.
    \begin{answer}
      It is a subspace because it is the column space of the matrix 
      \begin{equation*}
        \begin{mat}[r]
          3  &2  &4  \\
          1  &0  &-1 \\
          2  &2  &5
        \end{mat}
      \end{equation*}
      of coefficients.
      To find a basis for the column space, 
      \begin{equation*}
        \set{c_1\colvec[r]{3 \\ 1 \\ 2}
             +c_2\colvec[r]{2 \\ 0 \\ 2}
             +c_3\colvec[r]{4 \\ -1 \\ 5}
             \suchthat c_1,c_2,c_3\in\Re}
      \end{equation*}
      we eliminate linear relationships among the three column vectors 
      from the spanning set by transposing, reducing,
      \begin{equation*}
         \begin{mat}[r]
           3  &1  &2  \\
           2  &0  &2  \\
           4  &-1 &5
         \end{mat}
         \grstep[-(4/3)\rho_1+\rho_3]{-(2/3)\rho_1+\rho_2}
         \repeatedgrstep{-(7/2)\rho_2+\rho_3}
         \begin{mat}[r]
           3  &1     &2  \\
           0  &-2/3  &2/3  \\
           0  &0     &0
         \end{mat}
      \end{equation*}
      omitting the zero row, and transposing back.
      \begin{equation*}
        \sequence{ \colvec[r]{3 \\ 1 \\ 2},
                   \colvec[r]{0 \\ -2/3 \\ 2/3}  }
      \end{equation*}  
    \end{answer}
  \item 
    Show that the transpose operation is 
    linear:\index{transpose!is linear}
    \begin{equation*}
      \trans{(rA+sB)}  = r\trans{A}+s\trans{B}
    \end{equation*}
    for \( r,s\in\Re \) and \( A,B\in\matspace_{\nbym{m}{n}} \).
    \begin{answer}
      We can do this as a straightforward calculation.
      \begin{align*}
        \trans{(rA+sB)}
        &=\trans{\begin{mat}
             ra_{1,1}+sb_{1,1}  &\ldots &ra_{1,n}+sb_{1,n} \\
                          &\vdots            \\
             ra_{m,1}+sb_{m,1}  &\ldots &ra_{m,n}+sb_{m,n}
                \end{mat}  }  \\
        &=\begin{mat}
           ra_{1,1}+sb_{1,1}  &\ldots &ra_{m,1}+sb_{m,1}  \\
                            &\vdots                           \\
           ra_{1,n}+sb_{1,n}  &\ldots &ra_{m,n}+sb_{m,n}
         \end{mat}           \\
        &=\begin{mat}
           ra_{1,1}  &\ldots &ra_{m,1}  \\
                    &\vdots                       \\
           ra_{1,n}  &\ldots &ra_{m,n}
         \end{mat}           
        +\begin{mat}
           sb_{1,1}  &\ldots &sb_{m,1}  \\
                    &\vdots                       \\
           sb_{1,n}  &\ldots &sb_{m,n}
         \end{mat}           \\
        &=r\trans{A}+s\trans{B}
      \end{align*}  
    \end{answer}
  \recommended \item  
    In this subsection we have shown that 
    Gaussian reduction finds a basis for the row space.
    \begin{exparts}
      \partsitem Show that 
        this basis is not unique\Dash different reductions may yield
        different bases.
      \partsitem Produce matrices with equal row spaces but unequal numbers of
        rows.
      \partsitem Prove that two matrices have equal row spaces if and
        only if after
        Gauss-Jordan reduction they have the same nonzero rows.
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem These reductions give different bases.
          \begin{equation*}
            \begin{mat}[r]
              1  &2  &0  \\
              1  &2  &1
            \end{mat}
            \grstep{-\rho_1+\rho_2}
            \begin{mat}[r]
              1  &2  &0  \\
              0  &0  &1
            \end{mat}
            \qquad
            \begin{mat}[r]
              1  &2  &0  \\
              1  &2  &1
            \end{mat}
            \grstep{-\rho_1+\rho_2}
            \repeatedgrstep{2\rho_2}
            \begin{mat}[r]
              1  &2  &0  \\
              0  &0  &2
            \end{mat}
          \end{equation*}
        \partsitem An easy example is this.
          \begin{equation*}
            \begin{mat}[r]
              1  &2  &1  \\
              3  &1  &4
            \end{mat}
            \qquad
            \begin{mat}[r]
              1  &2  &1  \\
              3  &1  &4  \\
              0  &0  &0
            \end{mat}
          \end{equation*}
          This is a less simplistic example.
          \begin{equation*}
            \begin{mat}[r]
              1  &2  &1  \\
              3  &1  &4
            \end{mat}
            \qquad
            \begin{mat}[r]
              1  &2  &1  \\
              3  &1  &4  \\
              2  &4  &2  \\
              4  &3  &5
            \end{mat}
          \end{equation*}
        \partsitem Assume that \( A \) and \( B \) are matrices with equal
          row spaces.
          Construct a matrix \( C \) with the rows of \( A \) above the rows
          of \( B \), and another matrix \( D \) with the rows of \( B \)
          above the rows of \( A \).
          \begin{equation*}
            C=\begin{mat}
                 A \\ B
              \end{mat}
            \qquad
            D=\begin{mat}
                 B \\ A
              \end{mat}
          \end{equation*}
          Observe that \( C \) and \( D \) are row-equivalent (via a sequence
          of row-swaps) and so Gauss-Jordan reduce to the same reduced
          echelon form matrix.

          Because the row spaces are equal, the rows of \( B \)
          are linear combinations of the rows of \( A \) so Gauss-Jordan
          reduction on \( C \) simply turns the rows of \( B \) to zero rows
          and thus the nonzero rows of $C$ are just the nonzero rows obtained
          by Gauss-Jordan reducing \( A \).
          The same can be said for the matrix \( D \)\Dash Gauss-Jordan
          reduction on \( D \) gives the same non-zero rows as are produced
          by reduction on \( B \) alone.
          Therefore, 
          \( A \) yields the same nonzero rows as \( C  \), which yields
          the same nonzero rows as \( D \), which yields the same nonzero
          rows as \( B \).
      \end{exparts}  
    \end{answer}
  \item 
    Why is there not a problem with \nearbyremark{rem:MunkresCommment} 
    in the case that \( r \) is bigger than \( n \)?
    \begin{answer}
      It cannot be bigger.  
    \end{answer}
  \item  
    Show that the row rank of an \( \nbym{m}{n} \) matrix is at most
    \( m \).
    Is there a better bound?
    \begin{answer}
      The number of rows in a maximal linearly independent set cannot
      exceed the number of rows.
      A better bound (the bound that is, in general, the best possible) is
      the minimum of \( m \) and \( n \), because the row rank equals the
      column rank.
     \end{answer}
  \recommended \item
    Show that the rank of a matrix equals the rank of its transpose.
    \begin{answer}
      Because the rows of a matrix \( A \) are the columns
      of \( \trans{A} \) the dimension of the row space of \( A \) equals
      the dimension of the column space of \( \trans{A} \).
      But the dimension of the row space of \( A \) is the rank of \( A \) and
      the dimension of the column space of \( \trans{A} \) is the rank of
      \( \trans{A} \).
      Thus the two ranks are equal.
    \end{answer}
  \item 
    True or false:~the column space of a matrix equals the row space of its
    transpose.
    \begin{answer}
      False.
      The first is a set of columns while the second is a set of rows.

      This example, however,
      \begin{equation*}
         A=\begin{mat}[r]
             1  &2  &3  \\
             4  &5  &6
           \end{mat},
         \qquad
         \trans{A}=\begin{mat}[r]
             1  &4  \\
             2  &5  \\
             3  &6
           \end{mat}
      \end{equation*}
      indicates that as soon as we have a formal meaning for ``the same'',
      we can apply it here:
      \begin{equation*}
        \colspace{A}=\spanof{\set{
                        \colvec[r]{1 \\ 4},
                        \colvec[r]{2 \\ 5},
                        \colvec[r]{3 \\ 6} }  }
      \end{equation*}
      while
      \begin{equation*}
         \rowspace{\trans{A}}=\spanof{\set{
                                 \rowvec{1 &4},
                                 \rowvec{2 &5},
                                 \rowvec{3 &6} }  }
      \end{equation*}  
      are ``the same'' as each other.
    \end{answer}
  \recommended \item  
    We have seen that a row operation may change the column space.
    Must it?
    \begin{answer}
      No.
      Here, Gauss's Method does not change the column space.
      \begin{equation*}
        \begin{mat}[r]
          1  &0  \\
          3  &1
        \end{mat}
        \grstep{-3\rho_1+\rho_2}
        \begin{mat}[r]
          1  &0  \\
          0  &1
        \end{mat}
      \end{equation*} 
    \end{answer}
  \item
    Prove that a linear system has a solution if and only if that system's
    matrix of coefficients has the same rank as its augmented matrix.
    \begin{answer}
      A linear system
      \begin{equation*}
        c_1\vec{a}_1+\dots+c_n\vec{a}_n=\vec{d}
      \end{equation*}
      has a solution if and only if \( \vec{d} \) is in the span of
      the set \( \set{\vec{a}_1,\dots,\vec{a}_n} \).
      That's true if and only if the column rank of the augmented matrix
      equals the column rank of the matrix of coefficients.
      Since rank equals the column rank, the system has a solution if and
      only if the rank of its augmented matrix equals the rank of its matrix
      of coefficients.  
    \end{answer}
  \item 
    An \( \nbym{m}{n} \) matrix has 
    \definend{full row rank}\index{full row rank}\index{row rank!full} 
    if its row rank
    is \( m \), and it has 
    \definend{full column rank}\index{full column rank}\index{column!rank!full}
    if its column rank is \( n \).
    \begin{exparts}
      \partsitem Show that 
        a matrix can have both full row rank and full column rank
        only if it is square.
      \partsitem Prove that the linear system with matrix of
        coefficients \( A \)
        has a solution for any \( d_1 \), \ldots, \( d_n \)'s on the 
        right side if and only if \( A \) has full row rank.
      \partsitem Prove that a homogeneous system
        has a unique solution if and only if its matrix of coefficients
        $A$ has full column rank.
      \partsitem Prove that the statement 
       ``if a system with matrix of coefficients \( A \) has
        any solution then it has a unique solution'' holds
        if and only if \( A \) has full column rank.
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem Row rank equals column rank so each is at most the 
          minimum of the number of rows and columns.
          Hence both can be full only if the number of rows equals the number
          of columns.
          (Of course, the converse does not hold:~a 
          square matrix need not have full row rank or full column rank.)
        \partsitem If \( A \) has full row rank then, no matter what 
          the right-hand
          side, Gauss's Method on the augmented matrix ends with a leading
          one in each row and none of those leading ones in the
          furthest right column (the ``augmenting'' column).
          Back substitution then gives a solution.

          On the other hand, if the linear system lacks a solution for some
          right-hand side it can only be because Gauss's Method leaves some row
          so that it is all zeroes to the left of the ``augmenting'' bar 
          and has a nonzero entry on the right.
          Thus, if $A$ does not have a solution for some right-hand sides,
          then \( A \) does not have full row rank because some of its rows
          have been eliminated.
        \partsitem The matrix \( A \) has full column rank if and only
          if its columns
          form a linearly independent set.
          That's equivalent to the existence of only the trivial linear
          relationship among the columns, so the only solution of the
          system is where each variable is $0$.
        \partsitem The matrix \( A \) has full column rank if and only
          if the set of
          its columns is linearly independent, and so forms a basis for
          its span.
          That's equivalent to the existence of a unique linear representation
          of all vectors in that span.
          That proves it, 
          since any linear representation of a vector in the span is a 
          solution of the linear system.
      \end{exparts}  
    \end{answer}
  \item 
    How would the conclusion of \nearbylemma{le:RowSpUnchByGR} change
    if Gauss's Method were changed to allow multiplying a row by zero?
    \begin{answer}
      Instead of the row spaces being the same, the row space of $B$
      would be a subspace (possibly equal to) the row space of $A$.
    \end{answer}
  \recommended \item
    What is the relationship between \( \rank(A) \) and \( \rank(-A) \)?
    Between \( \rank(A) \) and \( \rank(kA) \)?
    What, if any, is the relationship between \( \rank(A) \), \( \rank(B) \),
    and \( \rank(A+B) \)?
    \begin{answer}
      Clearly \( \rank(A)=\rank(-A) \) as Gauss's Method allows us to
      multiply all rows of a matrix by \( -1 \).
      In the same way, when \( k\neq 0 \) we have \( \rank(A)=\rank(kA) \).

      Addition is more interesting.
      The rank of a sum can be smaller than the rank of the summands.
      \begin{equation*}
        \begin{mat}[r]
          1  &2  \\
          3  &4
        \end{mat}
        +
        \begin{mat}[r]
         -1  &-2  \\
         -3  &-4
        \end{mat}
        =
        \begin{mat}[r]
         0   &0   \\
         0   &0
        \end{mat}
      \end{equation*}
      The rank of a sum can be bigger than the rank of the summands.
      \begin{equation*}
        \begin{mat}[r]
          1  &2  \\
          0  &0
        \end{mat}
        +
        \begin{mat}[r]
          0  &0   \\
          3  &4
        \end{mat}
        =
        \begin{mat}[r]
         1   &2   \\
         3   &4
        \end{mat}
      \end{equation*}
      But there is an upper bound (other than the size of the matrices).
      In general, \( \rank(A+B)\leq\rank(A)+\rank(B) \).

      To prove this, note that we can perform Gaussian elimination on
      \( A+B \) in either of two ways:
      we can first add \( A \) to \( B \) and then apply the appropriate
      sequence of reduction steps
      \begin{equation*}
        (A+B)
        \grstep{\text{step}_1}
        \cdots
        \grstep{\text{step}_k}
        \text{echelon form}
      \end{equation*}
      or we can get the same results by performing \( \text{step}_1 \) through
      \( \text{step}_k \) separately on \( A \) and \( B \), and then adding.
      The largest rank that we can end with in the second case is clearly 
      the sum of the ranks.
      (The matrices above give examples of both possibilities,  
       $\rank(A+B)<\rank(A)+\rank(B)$ and $\rank(A+B)=\rank(A)+\rank(B)$,
       happening.)
    \end{answer}
    % Relationship between rank(A), rank(B) and rank(cA+dB)?
\end{exercises}























\subsectionoptional{Combining Subspaces}
\index{direct sum|(}
\textit{This subsection is optional.
It is required only for the last sections of
Chapter Three and Chapter Five and for occasional exercises.
You can pass it over without loss of continuity.}

% This chapter opened with the definition of a vector space, and the middle
% consisted of a first analysis of the idea.
% This subsection closes the chapter by finishing the analysis, in the sense
% that `analysis' means ``method of determining the \ldots{} essential
% features of something by separating it into parts'' \cite{MacmillanDict}.

One way to understand something is to see how to build it from 
component parts.
For instance, we sometimes think of \( \Re^3 \) 
put together
from the \( x \)-axis, the \( y \)-axis, and \( z \)-axis.
In this subsection we will describe 
how to decompose a vector space into a combination of
some of its subspaces.
In developing this idea of subspace combination, we will keep the $\Re^3$
example in mind as a prototype.

Subspaces are subsets and sets combine via union.
But taking the combination operation for subspaces to be the simple set union
operation isn't what we want.
For instance, the union of the 
\( x \)-axis, the \( y \)-axis, and \( z \)-axis is not all of $\Re^3$.
In fact this union is not
a subspace because it is not closed under addition: this vector 
\begin{equation*}
  \colvec[r]{1 \\ 0 \\ 0}
  +
  \colvec[r]{0 \\ 1 \\ 0}
  +
  \colvec[r]{0 \\ 0 \\ 1}
  =
  \colvec[r]{1 \\ 1 \\ 1}
\end{equation*}
is in none of the three axes and hence is not in the union.
Therefore
to combine subspaces, 
in addition to the members of those subspaces, we must at least also
include all of their linear combinations.

\begin{definition}
Where \( W_1,\dots, W_k \) are subspaces of a vector space, their
\definend{sum}\index{sum!of subspaces}\index{subspace!sum}
is the span of their union
$W_1+W_2+\dots +W_k=\spanof{W_1\union W_2\union \cdots W_k}$.
\end{definition}

\noindent Writing `\( + \)'  
fits with the conventional practice of using this symbol for a
natural accumulation operation.

\begin{example} \label{ex:RThreeSumAxes}
Our $\Re^3$ prototype works with this.
Any vector $\vec{w}\in\Re^3$ is a linear combination
$c_1\vec{v}_1+c_2\vec{v}_2+c_3\vec{v}_3$ where $\vec{v}_1$ is a member of
the $x$-axis, etc., in this way
\begin{equation*}
    \colvec{w_1 \\ w_2 \\ w_3}
    =1\cdot\colvec{w_1 \\ 0 \\ 0}
    +1\cdot\colvec{0 \\ w_2 \\ 0}
    +1\cdot\colvec{0 \\ 0 \\ w_3}
\end{equation*}
and so $\text{$x$-axis}+\text{$y$-axis}+\text{$z$-axis}=\Re^3$.
\end{example}

\begin{example}
A sum of subspaces can be less than the entire space. 
Inside of  \( \polyspace_4 \), 
let \( L \) be the subspace of linear polynomials
$\set{a+bx\suchthat a,b\in\Re}$
and let \( C \) be the subspace of purely-cubic polynomials
\( \set{cx^3\suchthat c\in\Re}\).
Then \mbox{$L+C$} is not all of $\polyspace_4$.
Instead, 
\( \mbox{$L+C$}=\set{a+bx+cx^3\suchthat a,b,c\in\Re} \).
\end{example}

\begin{example} \label{exam:RThreeIsSumXYAndYZ}
A space can be described as a combination of subspaces in more than one way.
Besides the decomposition 
$\Re^3=\text{$x$-axis}+\text{$y$-axis}+\text{$z$-axis}$,
we can also write
$\Re^3=\text{$xy$-plane}+\text{$yz$-plane}$.
To check this, note that any $\vec{w}\in\Re^3$ can be written
as a linear combination of a member of the $xy$-plane and a member of the
$yz$-plane; here are two such combinations.  
\begin{equation*}
  \colvec{w_1 \\ w_2 \\ w_3}
  =1\cdot\colvec{w_1 \\ w_2 \\ 0}
   +1\cdot\colvec{0 \\ 0 \\ w_3}
  \qquad
  \colvec{w_1 \\ w_2 \\ w_3}
  =1\cdot\colvec{w_1 \\ w_2/2 \\ 0}
   +1\cdot\colvec{0 \\ w_2/2 \\ w_3}
\end{equation*}
\end{example}

The above definition gives one way in which we can think of a space 
as a combination of some of its parts.
However, the prior example shows that there is at least one interesting
property of our benchmark model that is not captured by
the definition of the sum of subspaces.
In the familiar decomposition of $\Re^3$,
we often speak of a vector's `$x$~part' or `$y$~part' or 
`$z$~part'.
That is, in our prototype each vector has a unique decomposition into
pieces from the parts making up the whole space.
But in the decomposition used in \nearbyexample{exam:RThreeIsSumXYAndYZ}, we
cannot refer to the ``$xy$~part'' of a vector\Dash these three sums
\begin{equation*}
  \colvec[r]{1 \\ 2 \\ 3}
  =\colvec[r]{1 \\ 2 \\ 0}
  +\colvec[r]{0 \\ 0 \\ 3}
  =\colvec[r]{1 \\ 0 \\ 0}
  +\colvec[r]{0 \\ 2 \\ 3} 
  =\colvec[r]{1 \\ 1 \\ 0}
  +\colvec[r]{0 \\ 1 \\ 3} 
\end{equation*}
all describe the vector as comprised of something from the first plane plus
something from the second plane, but the ``$xy$~part'' is different in each.

%What's happening here, what allows the vector's second entry 
%$2$ to be taken from the $xy$-plane or
%from the $yz$-plane or from a combination, is that the two planes overlap.
%Their intersection is the $y$-axis and so the middle component 
%can shift around.
%In terms of our `analysis', the problem with the sum of subspaces operation is
%that, while the expression
%$\Re^3=\text{$xy$-plane}+\text{$yz$-plane}$ describes the space as a
%combination of its parts, those parts are not separate.

%\begin{lemma}
%The intersection of subspaces is a subspace.\index{subspace!intersection}
%\end{lemma}

%\begin{proof}
%To show that it is a subspace, we need only show that it is nonempty and
%closed under linear combinations.
%It is nonempty because it contains the zero vector.
%For closure, consider a linear combination of vectors from the
%intersection. 
%We can view that as a linear combination of vectors from the first subspace,
%because each vector in the intersection is in the first subspace.
%As all of the vectors are in the first subspace, 
%their combination sums to a vector that is also in the first subspace.
%In the same way, the combination sums to a vector that is in each subspace, 
%and so is in the intersection.
%\end{proof}

%Therefore, an intersection of subspaces is never empty; the smallest that it
%can be is the trivial subspace.
%So the above reference to subspaces that ``overlap'' must be taken to mean
%that their intersection is larger than the trivial subspace, 
%not just that their intersection is nonempty.

That is, when we consider how $\Re^3$ is put together from the three axes
we might mean ``in such a way that every vector 
has at least one decomposition,'' which gives the definition above.
But if we take it to mean 
``in such a way that every vector has one and only one
decomposition'' then we need another condition on combinations.
To see what this condition is, recall that
vectors are uniquely represented in terms of a basis.
We can use this to break a space into a sum of subspaces
such that any vector in the space breaks uniquely into a sum of members of
those subspaces.

\begin{example} \label{exam:BenchDirSum}
Consider $\Re^3$ with its standard basis 
$\stdbasis_3=\sequence{\vec{e}_1,\vec{e}_2,\vec{e}_3}$.
The subspace with the basis $B_1=\sequence{\vec{e}_1}$ is the $x$-axis,
the subspace with the basis $B_2=\sequence{\vec{e}_2}$ is the $y$-axis, and
the subspace with the basis $B_3=\sequence{\vec{e}_3}$ is the $z$-axis.
The fact that any member of $\Re^3$ is expressible as a sum of vectors from
these subspaces
\begin{equation*}
  \colvec{x \\ y \\ z}
   =\colvec{x \\ 0 \\ 0}
    +\colvec{0 \\ y \\ 0}
    +\colvec{0 \\ 0 \\ z}
\end{equation*}
reflects the fact that $\stdbasis_3$ spans the space\Dash this 
equation
\begin{equation*}
  \colvec{x \\ y \\ z}
   =c_1\colvec[r]{1 \\ 0 \\ 0}
    +c_2\colvec[r]{0 \\ 1 \\ 0}
    +c_3\colvec[r]{0 \\ 0 \\ 1}
\end{equation*}
has a solution for any $x,y,z\in\Re$.
And the fact that each such expression is unique reflects that fact that
$\stdbasis_3$ is linearly independent, so any 
equation like the one above has a unique
solution. 
\end{example}

\begin{example}
We don't have to take the basis vectors one at a time, 
we can conglomerate them into larger sequences.
Consider again the space $\Re^3$ and the vectors from the standard basis $\stdbasis_3$.
The subspace with the basis $B_1=\sequence{\vec{e}_1,\vec{e}_3}$
is the $xz$-plane.
The subspace with the basis $B_2=\sequence{\vec{e}_2}$ is the $y$-axis.
As in the prior example, the fact that any member of the space is a sum of
members of the two subspaces in one and only one way
\begin{equation*}
  \colvec{x \\ y \\ z}
   =\colvec{x \\ 0 \\ z}
    +\colvec{0 \\ y \\ 0}
\end{equation*}
is a reflection of the fact that these vectors form a
basis\Dash this equation
\begin{equation*}
  \colvec{x \\ y \\ z}
   =(c_1\colvec[r]{1 \\ 0 \\ 0}
    +c_3\colvec[r]{0 \\ 0 \\ 1})
    +c_2\colvec[r]{0 \\ 1 \\ 0}
\end{equation*}
has one and only one solution for any $x,y,z\in\Re$.
\end{example}

% These examples illustrate 
% the natural way to decompose a space into a sum of subspaces so that
% each vector decomposes uniquely into a sum of vectors from the parts.

\begin{definition}
The \definend{concatenation}\index{concatenation of sequences}%
\index{sequence!concatenation} 
of the sequences
$
   B_1=\sequence{\vec{\beta}_{1,1},\dots,\vec{\beta}_{1,n_1}}$, 
  \ldots,
  $B_k=\sequence{\vec{\beta}_{k,1},\dots,\vec{\beta}_{k,n_k}}$ 
adjoins them into a single sequence.
\begin{equation*}
 \cat{\cat{\cat{B_1}{B_2}}{\cdots}}{B_k}=
   \sequence{\vec{\beta}_{1,1},\dots,\vec{\beta}_{1,n_1},
            \vec{\beta}_{2,1},\dots,\vec{\beta}_{k,n_k} } 
\end{equation*}
\end{definition}

\begin{lemma} \label{le:UniqDecIffBasisDec}
Let $V$ be a vector space that is the sum of some of its subspaces
$V=W_1+\dots+W_k$.
Let $B_1$, \ldots, $B_k$ be bases for these subspaces.
The following are equivalent.
\begin{tfae}
  \item The expression of any $\vec{v}\in V$ as a combination  
     $\vec{v}=\vec{w}_1+\dots+\vec{w}_k$ 
     with $\vec{w}_i\in W_i$ is unique.
  \item The concatenation $\cat{\cat{B_1}{\cdots}}{B_k}$ is a basis for $V$.
  \item Among 
    nonzero vectors from different $W_i$'s every linear relationship is 
    trivial.
% ; that is, 
%     any set of nonzero vectors $\set{\vec{w}_1,\dots,\vec{w}_k}$
%     with $\vec{w}_i\in W_i$ is linearly independent.
\end{tfae}
\end{lemma}

\begin{proof}
We will show that $\text{(1)}\implies\text{(2)}$, 
that $\text{(2)}\implies\text{(3)}$, 
and finally that $\text{(3)}\implies\text{(1)}$.
For these arguments, observe that we can pass from a combination of 
$\vec{w}$'s to a combination of $\vec{\beta}$'s
\begin{align*}
  d_1\vec{w}_1+\dots+d_k\vec{w}_k                                 
    &= d_1(c_{1,1}\vec{\beta}_{1,1}+\dots+c_{1,n_1}\vec{\beta}_{1,n_1})   \\
    &\qquad\hbox{}+\dots
        +d_k(c_{k,1}\vec{\beta}_{k,1}+\dots+c_{k,n_k}\vec{\beta}_{k,n_k}) \\
    &= d_1c_{1,1}\cdot\vec{\beta}_{1,1}
        +\dots
        +d_kc_{k,n_k}\cdot\vec{\beta}_{k,n_k}   
  \tag{$*$}
\end{align*}
and vice versa
(we can move from the bottom to the top by taking each $d_i$ to be $1$).

For $\text{(1)}\implies\text{(2)}$,
assume that all decompositions are unique.
We will show that $\cat{\cat{B_1}{\cdots}}{B_k}$ spans the
space and is linearly independent.
It spans the space because the assumption 
that $V=W_1+\dots+W_k$
means that every $\vec{v}$ can be expressed as 
$\vec{v}=\vec{w}_1+\dots+\vec{w}_k$, which translates by equation~($*$) to 
an expression of $\vec{v}$ as a linear combination of the $\vec{\beta}$'s
from the concatenation.
For linear independence, consider this linear relationship.
\begin{equation*}
  \zero=c_{1,1}\vec{\beta}_{1,1}+\dots+c_{k,n_k}\vec{\beta}_{k,n_k}
\end{equation*}
Regroup as in ($*$) (that is, move from bottom to top) to get
the decomposition $\zero=\vec{w}_1+\dots+\vec{w}_k$.
Because the zero vector obviously
has the decomposition $\zero=\zero+\dots+\zero$,
the assumption that decompositions are unique shows that each
$\vec{w}_i$ is the zero vector.
This means that
$c_{i,1}\vec{\beta}_{i,1}+\dots+c_{i,n_i}\vec{\beta}_{i,n_i}=\zero$, and
since each $B_i$ is a basis we have the desired conclusion that all of
the $c$'s are zero.

For $\text{(2)}\implies\text{(3)}$ assume that the concatenation of the
bases is a basis for the entire space.
Consider a linear relationship among nonzero vectors from different
$W_i$'s.
This might or might not involve a vector from $W_1$, or
one from $W_2$, etc., so we write it 
$\zero=\mbox{}\cdots+d_i\vec{w}_i+\cdots\mbox{}$.
As in equation~($*$) expand the vector.
\begin{align*}
  \zero   
   &= \mbox{}\dots
      +d_i(c_{i,1}\vec{\beta}_{i,1}+\dots+c_{i,n_i}\vec{\beta}_{i,n_i})
      +\cdots\mbox{}                                               \\        
   &= \mbox{}\dots
      +d_ic_{i,1}\cdot\vec{\beta}_{i,1}
      +\dots+
      d_ic_{i,n_i}\cdot\vec{\beta}_{i,n_i}
      +\cdots\mbox{}                                                      
\end{align*}
The linear independence of $\cat{\cat{B_1}{\cdots}}{B_k}$ gives that each
coefficient $d_ic_{i,j}$ is zero.
Since $\vec{w}_i$ is nonzero vector, at least one of the 
$c_{i,j}$'s is not zero, and thus $d_i$ is zero.
This holds for each $d_i$, and therefore the linear relationship is trivial.

Finally, for $\text{(3)}\implies\text{(1)}$,
assume that among nonzero vectors from different $W_i$'s any linear
relationship is trivial.
Consider two decompositions of a vector 
$\vec{v}=\mbox{}\cdots+\vec{w}_i+\cdots\mbox{}$ 
and $\vec{v}=\mbox{}\cdots+\vec{u}_j+\cdots\mbox{}$
where $\vec{w}_i\in W_i$ and $\vec{u}_j\in W_j$.
Subtract one from the other to get a linear relationship, something like
this (if there is no $\vec{u}_i$ or $\vec{w}_j$ then leave those out).
\begin{equation*}
  \zero
   =\mbox{}\cdots+(\vec{w}_i-\vec{u}_i)
     +\cdots
     +(\vec{w}_j-\vec{u}_j)+\cdots\mbox{}
\end{equation*}
The case assumption that statement~(3) holds implies that the terms each
equal the zero vector
 $\vec{w}_i-\vec{u}_i=\zero$.
Hence decompositions are unique.
\end{proof}

\begin{definition}
A collection of subspaces \( \set{W_1,\ldots, W_k} \) is
\definend{independent}\index{subspace!independence}
if no nonzero vector from any \( W_i \) is a linear combination of
vectors from the other subspaces \( W_1,\dots, W_{i-1},W_{i+1},\dots, W_k \).
\end{definition}

\begin{definition}
A vector space \( V \) is the
\definend{direct sum}\index{subspace!direct sum}\index{direct sum!definition}
(or \definend{internal direct sum}\index{internal direct sum}) 
of its subspaces \( W_1,\dots, W_k \) if
\( V=W_1+W_2+\dots +W_k \)
and the collection \( \set{W_1,\dots, W_k} \) is independent.
We write \( V=W_1\directsum W_2\directsum \cdots\directsum W_k \).
\end{definition}

\begin{example}
Our prototype works: 
$\Re^3=\text{$x$-axis}\directsum\text{$y$-axis}\directsum\text{$z$-axis}$. 
\end{example}

\begin{example}
The space of \( \nbyn{2} \) matrices is this direct sum.
\begin{equation*}
  \set{\begin{mat}
         a  &0  \\
         0  &d
       \end{mat}  \suchthat a,d\in\Re }
  \,\mbox{}\directsum\mbox{}\,
  \set{\begin{mat}
         0  &b  \\
         0  &0
       \end{mat}  \suchthat b\in\Re }
  \,\mbox{}\directsum\mbox{}\,
  \set{\begin{mat}
         0  &0  \\
         c  &0
       \end{mat}  \suchthat c\in\Re }
\end{equation*}
It is the direct sum of subspaces in many other ways as well; 
direct sum decompositions are not unique.
\end{example}

\begin{corollary} \label{cor:DirSumDimsAdd}
The dimension of a direct sum is the sum of the dimensions of its summands.
\end{corollary}

\begin{proof}
In \nearbylemma{le:UniqDecIffBasisDec},
the number of basis vectors in the concatenation equals the sum of
the number of vectors in the sub-bases.
\end{proof}

The special case of two subspaces is worth its own mention.

\begin{definition}
When a vector space is the direct sum of two of its subspaces then they are
\definend{complements}.\index{subspace!complementary}%
\index{complementary subspaces}
\end{definition}

\begin{lemma}
\label{le:DirectSumTwoSp}
A vector space \( V \) is the 
direct sum\index{direct sum!of two subspaces}
of two of its subspaces \( W_1 \) and \( W_2 \) if and only if it is the
sum of the two \( V=W_1+W_2 \) and their intersection is trivial
\( W_1\intersection W_2=\set{\zero\,} \).
\end{lemma}

\begin{proof}
Suppose first that $V=W_1\directsum W_2$.
By definition, $V$ is the sum of the two $V=W_1+ W_2$.
To show that their intersection is trivial 
let $\vec{v}$ be a vector from $W_1\intersection W_2$
and consider the equation $\vec{v}=\vec{v}$.
On that equation's left side is a member of $W_1$
and on the right is a member of $W_2$, which we can think of as 
a linear combination of members of $W_2$.
But the two spaces are independent 
so the only way that a member of $W_1$ can be a linear
combination of vectors from $W_2$ is if that member 
is the zero vector $\vec{v}=\zero$.

For the other direction, suppose that $V$ is the sum of two spaces
with a trivial intersection.
To show that $V$ is a direct sum of the two we need only show that
the spaces are independent\Dash that no nonzero member of the first is 
expressible as a linear combination of members of the second, and vice versa.
This holds because any relationship 
$\vec{w}_1=c_1\vec{w}_{2,1}+\dots+c_k\vec{w}_{2,k}$ (with $\vec{w}_1\in W_1$
and $\vec{w}_{2,j}\in W_2$ for all $j$) shows that the vector on the left is 
also in $W_2$, since the right side is a combination of members of $W_2$.
The intersection of these two spaces is trivial, so $\vec{w}_1=\zero$.
The same argument works for any $\vec{w}_2$.
\end{proof}

%\begin{corollary}
%\label{cor:CompsIffDimsAndIntTriv}
%Two subspaces \( W_1,W_2 \) are complements in \( V \) if and only if
%\( \dim(W_1)+\dim(W_2)=\dim(V) \) and
%\( W_1\intersection W_2=\set{\zero\,} \).
%\end{corollary}

%\begin{proof}
%This follows immediately from 
%\nearbycorollary{cor:DimSumEqDimSummandsMinDimInter}.
%\end{proof}

\begin{example}
In $\Re^2$ the \( x \)-axis and the \( y \)-axis are complements,
that is, $\Re^2=\text{$x$-axis}\directsum\text{$y$-axis}$.
A space can have more than one pair of complementary subspaces;
another pair for~$\Re^2$ are the subspaces consisting of the 
lines \( y=x \) and \( y=2x \).
\end{example}

\begin{example}
In the space \( F=\set{a\cos\theta+b\sin\theta\suchthat a,b\in\Re} \),
the subspaces \( W_1=\set{a\cos\theta\suchthat a\in\Re} \) and
\( W_2=\set{b\sin\theta\suchthat b\in\Re} \) are complements.
The prior example noted that a space can be decomposed into more than
one pair of complements. 
In addition note that $F$ can has more than one pair of
complementary subspaces where the first in the pair is $W_1$\Dash another 
complement of \( W_1 \) is
\( W_3=\set{b\sin\theta+b\cos\theta \suchthat b\in\Re} \).
\end{example}

\begin{example}
In \( \Re^3 \), the \( xy \)-plane and the \( yz \)-planes are not
complements, which is the point of the discussion following
\nearbyexample{exam:RThreeIsSumXYAndYZ}.
One complement of the \( xy \)-plane is the \( z \)-axis.
% A complement of the \( yz \)-plane is the line through \( (1,1,1) \).
\end{example}

Here is a natural question that arises from \nearbylemma{le:DirectSumTwoSp}:~for
$k>2$ is
the simple sum $V=W_1+\dots+W_k$ also a direct sum if and only if
the intersection of the subspaces is trivial?

\begin{example}    \label{exam:DirSumThree}
If there are more than two subspaces then 
having a trivial intersection is not enough to 
guarantee unique decomposition (i.e., is not enough to ensure that the
spaces are independent). 
In \( \Re^3 \), let
\( W_1 \) be the \( x \)-axis, let \( W_2 \) be the \( y \)-axis, and let
$W_3$ be this.
\begin{equation*}
  W_3=\set{\colvec{q \\ q \\ r} \suchthat q,r\in\Re}
\end{equation*}
The check that \( \Re^3=W_1+W_2+W_3 \) is easy.
The intersection 
\( W_1\intersection W_2\intersection W_3 \) 
is trivial, but decompositions aren't unique.
\begin{equation*}
  \colvec{x \\ y \\ z}
  =\colvec{0 \\ 0 \\ 0}
  +\colvec{0 \\ y-x \\ 0}
  +\colvec{x \\ x \\ z}
  =\colvec{x-y \\ 0 \\ 0}
  +\colvec{0 \\ 0 \\ 0}
  +\colvec{y \\ y \\ z}
\end{equation*}
(This example also shows that this requirement is also not 
enough:~that all pairwise intersections of the subspaces be trivial.
See \nearbyexercise{exer:ThreeSubsPairwseNonTriv}.)
\end{example}

In this subsection we have seen two ways to regard a space as built up from
component parts.
Both are useful; in particular we will use the direct sum definition
at the end of the Chapter Five.

%In looking at how this overlap of subspaces interacts with the 
%`$+$' operation, we first consider the simplest case, the two-subspace case.
%To understand the relationship between the two subspaces $U$ and $W$, their
%sum $U+W$, and their intersection $U\intersection W$, we can ask about the
%bases.  
%Note that a basis for the intersection $U\intersection W$ is a linearly 
%independent subset of $U$ and also of $W$.
%Thus, we can think to expand it to make bases for the larger sets.

%\begin{lemma} \label{le:BasesSumTwoSubs}
%Let \( U \) and \( W \) be subspaces of a vector space.
%If the sequence \( \sequence{\vec{\beta}_1,\dots,\vec{\beta}_k} \) 
%is a basis for
%\( U\intersection W \), and
%the sequence \( \sequence{\vec{\mu}_1,\dots,\vec{\mu}_j,
%   \vec{\beta}_1,\dots,\vec{\beta}_k} \)
%is a basis for \( U \), and
%\( \sequence{\vec{\beta}_1,\dots,\vec{\beta}_k,
%   \vec{\omega}_1,\dots,\vec{\omega}_p} \)
%is a basis for \( W \), then
%\begin{equation*}
%   \sequence{\vec{\mu}_1,\dots,\vec{\mu}_j,
%             \vec{\beta}_1,\dots,\vec{\beta}_k,
%             \vec{\omega}_1,\dots,\vec{\omega}_p}
%\end{equation*}
%is a basis for \( U+W \).
%\end{lemma}

%\begin{proof}
%We write $B_{U\intersection W}$ for the basis for $U\intersection W$,
%we write $B_U$ for the basis for $U$, we write $B_W$ for the basis for $W$,
%and we write $B_{U+W}$ for the basis under consideration.

%To see that that $B_{U+W}$ spans $U+W$, observe that
%any vector $c\vec{u}+d\vec{w}$ from $U+W$ can be written as a linear
%combination of the vectors in $B_{U+W}$, simply by expressing $\vec{u}$ in 
%terms of $B_U$ and expressing $\vec{w}$ in terms of $B_W$.

%We finish by showing that $B_{U+W}$ is linearly independent. 
%Consider
%\begin{equation*}
%  c_1\vec{\mu}_1+\dots+c_{j+1}\vec{\beta}_1+\dots+c_{j+k+p}\vec{\omega}_p=
%    \zero
%\end{equation*}
%which can be rewritten in this way.
%\begin{equation*}
%  c_1\vec{\mu}_1+\dots+c_j\vec{\mu}_j=
%       -c_{j+1}\vec{\beta}_1-\dots-c_{j+k+p}\vec{\omega}_p
%\end{equation*}
%Note that the left side sums to a vector in \( U \) while 
%right side sums to a vector in \( W \), and thus both sides sum to
%a member of $U\intersection W$.
%Since the left side is a member of $U\intersection W$, 
%it is expressible in terms of the members of $B_{U\intersection W}$, 
%which gives the combination of $\vec{\mu}$'s from the left side above
%as equal to a combination of $\vec{\beta}$'s. 
%But, the fact that 
%the basis $B_U$ is linearly independent shows that any such combination
%is trivial, and in particular, the coefficients $c_1$, \ldots,
%$c_j$ fro the left side above are all zero.
%Similarly, the coefficients of the \( \vec{\omega} \)'s are all zero.
%This leaves the above equation as a linear relationship among the 
%\( \vec{\beta} \)'s, 
%but $B_{U\intersection W}$ is linearly independent, 
%and therefore all of the coefficients of the $\vec{\beta}$'s are also zero.
%\end{proof}

%\begin{example}
%Label the axes in \( \Re^4 \) with \( x \), \( y \), \( z \), and \( w \).
%If \( U \) is \( xyz \)-space
%(that is, the subspace spanned by the \( x \), \( y \), and
%\( z \) axes), and \( W \) is \( xyw \)-space, then this
%\begin{equation*}
%  B_{U\intersection W}
%     =\sequence{\colvec{1 \\ 0 \\ 0 \\ 0},
%                \colvec{0 \\ 2 \\ 0 \\ 0} }
%\end{equation*}
%is a basis for \( U\intersection W \).
%It can be extended to these bases
%\begin{equation*}
%  B_U=\sequence{\colvec{1 \\ 1 \\ 1 \\ 0},
%            \colvec{1 \\ 0 \\ 0 \\ 0},
%            \colvec{0 \\ 2 \\ 0 \\ 0} }
%  \quad\text{and}\quad
%  B_W=\sequence{\colvec{1 \\ 0 \\ 0 \\ 0},
%            \colvec{0 \\ 2 \\ 0 \\ 0},
%            \colvec{1 \\ 0 \\ 0 \\ -1} }
%\end{equation*}
%for \( U \) and \( W \).
%Then this 
%\begin{equation*}
%  B_{U+W}
%   =\sequence{\colvec{1 \\ 1 \\ 1 \\ 0},
%            \colvec{1 \\ 0 \\ 0 \\ 0},
%            \colvec{0 \\ 2 \\ 0 \\ 0},
%            \colvec{1 \\ 0 \\ 0 \\ -1} }
%\end{equation*}
%is a basis for \( U+W=\Re^4 \).
%\end{example}

%\begin{corollary}
%\label{cor:DimSumEqDimSummandsMinDimInter}
%For any subspaces \( U \) and \( W \) of a vector space,
%\begin{equation*}
%  \dim(U+W)=\dim(U)+\dim(W)-\dim(U\intersection W).
%\end{equation*}
%\end{corollary}

%\begin{proof}
%Using the notation from the lemma, $\dim (U\intersection W)=k$,
%and $\dim(U)=j+k$, and $\dim(W)=k+p$, and $\dim (U+W)=j+k+p$. 
%\end{proof}

%\begin{example}
%In the prior example, $\dim (U\intersection W)=2$,
%and $\dim(U)=3$, and $\dim (W)=3$, and $\dim (U+W)=\dim(\Re^4)=4$,
%and so the equation becomes $4=3+3-2$.     
%\end{example}

%\begin{example}
%In \nearbyexample{exam:RThreeIsSumXYAndYZ} the equation gives
%$\dim(\Re^3)=\dim(\text{$xy$-plane})+\dim(\text{$yz$-plane})
%    -\dim(\text{$y$-axis})$ which is, of course, simply $3=2+2-1$.
%\end{example}

%The lemma gives us a way to ensure that every vector $\vec{v}\in U+W$ has a 
%unique decomposition as the sum of a vector from $U$ and a vector from $W$.
%Recall that vectors are uniquely represented in terms of a basis.
%If $B_{U\intersection W}$ is the empty basis (that is, if the intersection of
%$U$ and $W$ is trivial) then in the representation
%$\vec{v}=c_1\vec{\mu}_1+\dots+c_{j+p}\vec{\omega}_{j+p}$ we can take
%$\vec{u}$ to be $c_1\vec{\mu}_1+\dots+c_{j}\vec{\mu}_{j}$
%and take $\vec{w}$ to be 
%$c_{j+1}\vec{\omega}_{j+1}+\dots+c_{j+p}\vec{\omega}_{j+p}$
%to have a unique $\vec{v}=\vec{u}+\vec{w}$.

%\begin{definition}
%\label{def:DirectSumTwoSp}
%A vector space \( V \) is the 
%\definend{direct sum}\index{subspace!direct sum}%
%\index{direct sum!of two subspaces}
%of two of its subspaces \( W_1 \) and \( W_2 \), written 
%$V=W_1\directsum W_2$, if
%\( W_1+W_2=V \) and~\( W_1\intersection W_2=\set{\zero\,} \).
%The two subspaces are
%\definend{complements} %
%\index{subspace!complementary}\index{complementary subspaces}
%in \( V \).
%\end{definition}

%\begin{corollary}
%\label{cor:CompsIffDimsAndIntTriv}
%Two subspaces \( W_1,W_2 \) are complements in \( V \) if and only if
%\( \dim(W_1)+\dim(W_2)=\dim(V) \) and
%\( W_1\intersection W_2=\set{\zero\,} \).
%\end{corollary}

%\begin{proof}
%This follows immediately from 
%\nearbycorollary{cor:DimSumEqDimSummandsMinDimInter}.
%\end{proof}

%\begin{example}
%The \( x \)-axis and the \( y \)-axis are complements in \( \Re^2 \),
%that is, $\Re^2=\text{$x$-axis}\directsum\text{$y$-axis}$.
%A space can have more than one pair of complementary subspaces;
%another pair here are the subspaces consisting of the 
%lines \( y=x \) and \( y=2x \).
%\end{example}

%\begin{example}
%In the space \( F=\set{a\cos\theta+b\sin\theta\suchthat a,b\in\Re} \),
%the subspaces \( W_1=\set{a\cos\theta\suchthat a\in\Re} \) and
%\( W_2=\set{b\sin\theta\suchthat b\in\Re} \) are complements.
%In addition to the fact that a space like $F$ can have more than one pair of
%complementary subspaces, inside of the space a single subspace like $W_1$ can
%have more than one subspace that is a complement to it---another 
%complement to \( W_1 \) is
%\( W_3=\set{b\sin\theta+c\cos\theta \suchthat b,c\in\Re} \).
%\end{example}

%\begin{example}
%In \( \Re^3 \), the \( xy \)-plane and the \( yz \)-planes are not
%complements, because they have a nontrivial intersection.
%One complement of the \( xy \)-plane is the \( z \)-axis.
%A complement of the \( yz \)-plane is the line thru \( (1,1,1) \).
%\end{example}

%\begin{lemma}
%For a space $V$ and two subspaces $W_1$, $W_2$, every vector $\vec{v}$ in the
%space has a decomposition $\vec{v}=\vec{u}+\vec{w}$ that is unique (where
%$\vec{u}\in U$ and $\vec{w}\in W$) if and only if the space is the direct sum
%of the subspaces.
%\end{lemma}

%\begin{proof}
%The `if' direction is handled as discussed before the definition.
%If $V=W_1\directsum W_2$ then the intersection of the two spaces is trivial
%and so \nearbylemma{le:BasesSumTwoSubs} shows that there is a basis for
%the sum $W_1+W_2=V$ consisting of a basis for $W_1$ with a basis for $W_2$
%adjoined.
%The representation of any $\vec{v}\in V$ with respect to this basis is unique,
%and we can regroup inside of this representation to have a unique expression
%$\vec{v}=\vec{w}_1+\vec{w}_2$.

%For the `only if' direction, we assume that decompositions exist and are
%unique.
%The fact that for every $\vec{v}\in V$ a decomposition exists shows that
%the space is the sum of the subspaces.
%To show that the intersection of the spaces is trivial,
%consider a member of the intersection \ldots
  
%\end{proof}

%We finish this subsection by going to the case of more than two subspaces.
%This is a bit trickier that the two-subspace case; the right notion of
%``do not overlap'' is perhaps not evident.
%That is, we are looking for a definition of a space as a direct sum of its
%subspaces where the first condition is that the space be the sum of those 
%subspaces, and the second condition is strong enough to give that each vector
%in the space has a unique decomposition into parts, one from each subspace.

%A natural first guess at the second condition, based on the material above,
%is to require that the spaces have a trivial intersection.
%The next example shows that this guess is, however, not enough to guarantee
%unique decomposition. 

%\begin{example}    \label{exam:DirSumThree}
%In \( \Re^3 \), let
%\( W_1 \) be the \( x \)-axis, let \( W_2 \) be the \( y \)-axis, and let
%\begin{equation*}
%  W_3=\set{\colvec{q \\ q \\ r} \suchthat q,r\in\Re}.
%\end{equation*}
%The check that \( \Re^3=W_1+W_2+W_3 \) is easy.
%The intersection of all three is trivial
%\( W_1\intersection W_2\intersection W_3=\set{\zero} \) but
%decompositions aren't unique.
%\begin{equation*}
%  \colvec{x \\ y \\ z}
%  =\colvec{0 \\ 0 \\ 0}
%  +\colvec{0 \\ y-x \\ 0}
%  +\colvec{x \\ x \\ z}
%  =\colvec{x-y \\ 0 \\ 0}
%  +\colvec{0 \\ 0 \\ 0}
%  +\colvec{y \\ y \\ z}
%\end{equation*}
%(This example also shows that this requirement is also not 
%enough:~that all pairwise intersections of the subspaces be trivial.
%See \nearbyexercise{exer:ThreeSubsPairwseNonTriv}.)
%\end{example}

%To ensure that each vector in the sum has a unique decomposition,
%we need a still stronger condition.

%\begin{definition}
%A collection of subspaces \( \set{W_1,\ldots, W_k} \) is
%{\em independent\/}\index{subspace!independence}
%if no nonzero vector from any \( W_i \) is a linear combination of
%vectors from the other subspaces \( W_1,\dots, W_{i-1},W_{i+1},\dots, W_k \).
%\end{definition}

%\begin{definition}
%A vector space \( V \) is the
%{\em direct sum\/}\index{subspace!direct sum}\index{direct sum!definition}
%(or {\em internal direct sum\/}) of its subspaces \( W_1,\dots, W_k \) if
%\( W_1+W_2+\dots +W_k=V \)
%and the collection \( \set{W_1,\dots, W_k} \) is independent.
%We write \( V=W_1\directsum W_2\directsum \dots\directsum W_k \).
%\end{definition}

%\begin{example}
%The benchmark example fits under this definition:
%$\Re^3=\text{$x$-axis}\directsum\text{$y$-axis}\directsum\text{$z$-axis}$ 
%\end{example}

%\begin{example}
%The space of \( \nbyn{2} \) matrices is this direct sum
%\begin{equation*}
%  \set{\begin{mat}
%         a  &0  \\
%         0  &d
%       \end{mat}  \suchthat a,d\in\Re }
%  \,\mbox{}\directsum\mbox{}\,
%  \set{\begin{mat}
%         0  &b  \\
%         0  &0
%       \end{mat}  \suchthat b\in\Re }
%  \,\mbox{}\directsum\mbox{}\,
%  \set{\begin{mat}
%         0  &0  \\
%         c  &0
%       \end{mat}  \suchthat c\in\Re }
%\end{equation*}
%(it is the direct sum of subspaces in many other ways also; as with the case
%of only two subspaces, direct sum decompositions into three or more 
%subspaces need not be unique).
%\end{example}

%Of course, following the work in this subsection, that definition will only be
%justified if we support it with a result saying that under those conditions,
%each vector in the summation has a unique decomposition into a sum of vectors,
%one from each summand.
%For that, 
%we will relate the decomposion a vector space into a direct sum
%to a decomposition of some basis for that space.
%We already know that uniqueness of decomposition relates to bases.

%\begin{definition}
%The sequence {\em concatenation\/}\index{concatenation} of
%\( B_1=\sequence{\vec{\beta}_{1,1},\dots,\vec{\beta}_{1,n_1}} \),
%\dots, \( B_k=\sequence{\vec{\beta}_{k,1},\dots,\vec{\beta}_{k,n_k}} \) is
%\(
% \cat{\cat{B_1}{\cdots}}{B_k}=
%  \sequence{\vec{\beta}_{1,1},\dots,\vec{\beta}_{1,n_1},
%            \vec{\beta}_{2,1},\dots,\vec{\beta}_{k,n_k} }.
%\)
%\end{definition}

%\begin{corollary}
%\label{cor:DirSumIffBasesCat}
%If \( V \) has subspaces \( W_1,\dots, W_k \)
%with bases \( B_1,\dots, B_k \)
%then \( W_1\directsum\dots\directsum W_k=V \) if and only if
%\( \cat{\cat{B_1}{\cdots}}{B_k} \) is a basis for \( V \).
%\end{corollary}

%\begin{proof}
%We will show that 
%the sum \( W_1+\dots+W_k \) equals \( V \) 
%if and only if \( \cat{\cat{B_1}{\cdots}}{B_k} \) spans \( V \), and that
%the set \( \set{W_1,\dots, W_k} \) is independent if and only if
%\( \cat{\cat{B_1}{\cdots}}{B_k} \) is linearly independent.
%These two halves prove the `if and only if' equivalence.

%First~(1).
%Left to right:
%assume the subspaces sum to \( V \) so any vector in \( V \) can be represented
%as \( \vec{v}=\vec{w}_1+\dots+\vec{w}_k \),
%rewrite each \( \vec{w}_i \) as a sum of basis elements and
%conclude \( \cat{\cat{B_1}{\cdots}}{B_k} \) spans \( V \).
%Right to left: if \( \cat{\cat{B_1}{\cdots}}{B_k} \) spans \( V \)
%then any \( \vec{v}\in V \) is a linear combination of vectors from
%\( \cat{\cat{B_1}{\cdots}}{B_k} \).
%Inside that combination commute to put vectors from \( B_1 \) first,
%followed from vectors from \( B_2 \), etc.
%Sum to first part to some \( \vec{w}_1\in W_1\), sum the second part to
%a \( \vec{w}_2\in W_2 \), etc.
%The result has the desired form.

%Now we prove item~(2).
%`If' is easy.
%For `only if' suppose the set of subspaces is independent and consider a linear
%relationship among vectors in
%\( \cat{\cat{B_1}{\cdots}}{B_k} \).
%Rewrite that relationship so all the vectors from \( B_1 \) are on one side and
%the other vectors are on the other side.
%This expresses some member of \( W_1 \) as a linear combination of members of
%the other subspaces, and independence of the set of subspaces then implies
%this linear combination of members of \( B_1 \) is \( \zero \).
%Thus, in this relationship, all the scalars associated with members of
%\( B_1 \) are \( 0 \).
%Similarly every scalar in the combination is \( 0 \).
%\end{proof}

%Hence, as mentioned above, each direct sum decomposition of a vector space is
%associated with a decomposition of one of that space's bases.

%\begin{corollary}
%\label{cor:DirectSumIffUniqueSum}
%Where \( W_1,\dots, W_k \) are subspaces,
%\( V=W_1\directsum\dots\directsum W_k \) if and
%only if every vector \( \vec{v}\in V \) has one and only one representation
%\( \vec{v}=\vec{w}_1+\dots+\vec{w}_k \) where \( \vec{w}_i\in W_i \).
%\end{corollary}

%\begin{proof}
%A chain of double implications works:
%\( V=W_1\directsum\dots\directsum W_k \)
%if and only if
%\( \cat{\cat{B_1}{\cdots}}{B_k} \) is a basis for \( V \),
%which is true if and only if each vector in \( V \)
%has a unique representation as a sum of vectors from
%\( \cat{\cat{B_1}{\cdots}}{B_k} \).
%\end{proof}

%\begin{corollary}
%The dimension of a direct sum is the sum of the dimensions of its summands.
%\end{corollary}


\begin{exercises}
  \recommended \item
    Decide if \( \Re^2 \) is the direct sum of each \( W_1 \) and \( W_2 \).
    \begin{exparts}
       \partsitem \( W_1=\set{\colvec{x \\ 0}\suchthat x\in\Re} \),
             \( W_2=\set{\colvec{x \\ x}\suchthat x\in\Re} \)
       \partsitem \( W_1=\set{\colvec{s \\ s}\suchthat s\in\Re} \),
             \( W_2=\set{\colvec{s \\ 1.1s}\suchthat s\in\Re} \)
       \partsitem \( W_1=\Re^2 \), \( W_2=\set{\zero} \)
       \partsitem \( W_1=W_2=\set{\colvec{t \\ t}\suchthat t\in\Re} \)
       \partsitem \( W_1=\set{\colvec[r]{1 \\ 0}+\colvec{x \\ 0}
                                \suchthat x\in\Re} \),
             \( W_2=\set{\colvec[r]{-1 \\ 0}+\colvec[r]{0 \\ y}\suchthat y\in\Re} \)
    \end{exparts}
    \begin{answer}
       With each of these we can apply \nearbylemma{le:DirectSumTwoSp}.
       \begin{exparts}
         \partsitem Yes.
           The plane is the sum of this $W_1$ and $W_2$ because for any 
           scalars $a$ and $b$
           \begin{equation*}
             \colvec{a \\ b}
             =\colvec{a-b \\ 0}
             +\colvec{b \\ b}
           \end{equation*}
           shows that the general vector is a sum of vectors from the two
           parts.
           And, these two subspaces are (different) lines through the origin,
           and so have a trivial intersection.
         \partsitem Yes.
           To see that any vector in the plane is a combination of vectors
           from these parts, consider this relationship.
           \begin{equation*}
             \colvec{a \\ b}=c_1\colvec[r]{1 \\ 1}+c_2\colvec[r]{1 \\ 1.1}
           \end{equation*}
           We could now simply note that the set
           \begin{equation*}
             \set{\colvec[r]{1 \\ 1},\colvec[r]{1 \\ 1.1}}
           \end{equation*}
           is a basis for the space (because it is clearly linearly
           independent, and has size two in $\Re^2$), and thus there is one and
           only one solution to the above equation, implying that all
           decompositions are unique.
           Alternatively, we can solve
           \begin{equation*}
             \begin{linsys}{2}
               c_1  &+  &c_2    &=  &a  \\
               c_1  &+  &1.1c_2 &=  &b
             \end{linsys}
             \grstep{-\rho_1+\rho_2}
             \begin{linsys}{2}
               c_1  &+  &c_2    &=  &a  \\
                    &   &0.1c_2 &=  &-a+b
             \end{linsys}
           \end{equation*}
           to get that $c_2=10(-a+b)$ and $c_1=11a-10b$, and so we have
           \begin{equation*}
             \colvec{a \\ b}=\colvec{11a-10b \\ 11a-10b}
                             +\colvec{-10a+10b \\ 1.1\cdot(-10a+10b)}
           \end{equation*}
           as required.
           As with the prior answer, each of the two subspaces is a line
           through the origin, and their intersection is trivial.
         \partsitem Yes.
           Each vector in the plane is a sum in this way
           \begin{equation*}
             \colvec{x \\ y}=\colvec{x \\ y}+\colvec[r]{0 \\ 0}
           \end{equation*}
           and the intersection of the two subspaces is trivial.
         \partsitem No.
           The intersection is not trivial.
         \partsitem No. 
           These are not subspaces.
      \end{exparts}  
    \end{answer}
  \recommended \item
    Show that \( \Re^3 \) is the direct sum of the \( xy \)-plane with    
    each of these. 
    \begin{exparts}
      \partsitem the \( z \)-axis
      \partsitem the line
        \begin{equation*}
          \set{\colvec{z \\ z \\ z} \suchthat z\in\Re }
        \end{equation*}
    \end{exparts}
    \begin{answer}
      With each of these we can use \nearbylemma{le:DirectSumTwoSp}.      
      \begin{exparts}
        \partsitem Any vector in \( \Re^3 \) can be decomposed as this sum.
          \begin{equation*}
            \colvec{x \\ y \\ z}
            =\colvec{x \\ y \\ 0}
            +\colvec{0 \\ 0 \\ z}
          \end{equation*}
          And, the intersection of the $xy$-plane and the $z$-axis is the
          trivial subspace.
        \partsitem Any vector in \( \Re^3 \) can be decomposed as
          \begin{equation*}
            \colvec{x \\ y \\ z}
            =\colvec{x-z \\ y-z \\ 0}
            +\colvec{z \\ z \\ z}
          \end{equation*}
          and the intersection of the two spaces is trivial.
      \end{exparts}  
    \end{answer}
  \item 
    Is \( \polyspace_2 \) the direct sum of
    \( \set{a+bx^2\suchthat a,b\in\Re} \) and
    \( \set{cx\suchthat c\in\Re} \)?
    \begin{answer}
      It is.
      Showing that these two are subspaces is routine.
      To see that the space is the direct sum of these two, just note that
      each member of \( \polyspace_2 \) has the unique decomposition
      \( m+nx+px^2=(m+px^2)+(nx) \).  
     \end{answer}
  \recommended \item 
    In \( \polyspace_n \), the 
    \definend{even}\index{even functions} polynomials are the members
    of this set
    \begin{equation*}
      \mathcal{E}=
      \set{p\in\polyspace_n \suchthat \text{\( p(-x)=p(x) \) for all \( x \)}}
    \end{equation*}
    and the 
    \definend{odd}\index{odd function}
    polynomials are the members of this set.
    \begin{equation*}
      \mathcal{O}=
      \set{p\in\polyspace_n \suchthat \text{\( p(-x)=-p(x) \) 
                                              for all \( x \)}}
    \end{equation*}
    Show that these are complementary subspaces.
    \begin{answer}
      To show that they are subspaces is routine.
      We will argue they are complements with \nearbylemma{le:DirectSumTwoSp}.
      The intersection \( \mathcal{E}\intersection\mathcal{O} \) is trivial
      because the only polynomial satisfying both conditions $p(-x)=p(x)$ and
      $p(-x)=-p(x)$ is the zero polynomial.
      To see that the entire space is the sum of the subspaces 
      \( \mathcal{E}+\mathcal{O}=\polyspace_n \), 
      note that the polynomials
      \( p_0(x)=1 \), \( p_2(x)=x^2 \), \( p_4(x)=x^4 \), etc., are 
      in \( \mathcal{E} \) and also note that the polynomials
      \( p_1(x)=x \), \( p_3(x)=x^3 \), etc., are in \( \mathcal{O} \).
      Hence any member of \( \polyspace_n \) is a combination of members of
      \( \mathcal{E} \) and \( \mathcal{O} \).  
    \end{answer}
  \item 
    Which of these subspaces of \( \Re^3 \) 
    \begin{center}
     \begin{tabular}{l}
      $W_1$: the \( x \)-axis,\quad
      $W_2$:~the \( y \)-axis,\quad
      $W_3$:~the \( z \)-axis,             \\
      $W_4$:~the plane \( x+y+z=0 \),\quad
      $W_5$:~the \( yz \)-plane
     \end{tabular}
    \end{center}
    can be combined to
    \begin{exparts*}
      \partsitem sum to \( \Re^3 \)? 
      \partsitem direct sum to \( \Re^3 \)?
    \end{exparts*}
    \begin{answer}
     Each of these is $\Re^3$.
     \begin{exparts}
      \partsitem 
        These are broken into some separate lines for readability.
        \begin{center}
          \begin{tabular}{l} 
            $W_1+W_2+W_3$, 
              $W_1+W_2+W_3+W_4$, 
              $W_1+W_2+W_3+W_5$,  \\ \quad %line too long
              $W_1+W_2+W_3+W_4+W_5$, 
              $W_1+W_2+W_4$, 
              $W_1+W_2+W_4+W_5$,     \\ \quad %line too long
              $W_1+W_2+W_5$,         
              $W_1+W_3+W_4$, 
              $W_1+W_3+W_5$, 
              $W_1+W_3+W_4+W_5$,     \\ \quad 
              $W_1+W_4$, 
              $W_1+W_4+W_5$,         
              $W_1+W_5$,             \\ 
              $W_2+W_3+W_4$, 
              $W_2+W_3+W_4+W_5$,     
              $W_2+W_4$, 
              $W_2+W_4+W_5$,         \\ 
              $W_3+W_4$, 
              $W_3+W_4+W_5$,         \\ 
              $W_4+W_5$ 
          \end{tabular}
         \end{center}
     \partsitem
       $W_1\directsum W_2\directsum W_3$,    
       $W_1\directsum W_4$,    
       $W_1\directsum W_5$,    
       $W_2\directsum W_4$,    
       $W_3\directsum W_4$    
     \end{exparts}
    \end{answer}
  \recommended \item
    Show that \( \polyspace_n=\set{a_0 \suchthat a_0\in\Re}\directsum\dots
                              \directsum\set{a_nx^n\suchthat a_n\in\Re} \).
    \begin{answer}
      Clearly each is a subspace.
      The bases \( B_i=\sequence{x^i} \) for the subspaces, when concatenated,
      form a basis for the whole space.  
    \end{answer}
  \item 
    What is \( W_1+W_2 \) if \( W_1\subseteq W_2 \)?
    \begin{answer}
       It is \( W_2 \).  
    \end{answer}
  \item 
    Does \nearbyexample{exam:BenchDirSum} generalize?
    That is, is this true or false:~if a vector space \( V \) has a basis
    \( \sequence{\vec{\beta}_1,\dots ,\vec{\beta}_n} \) then
    it is the direct sum of the spans of the one-dimensional subspaces
    \( V=\spanof{\set{\vec{\beta}_1}}\directsum\dots
           \directsum\spanof{\set{\vec{\beta}_n}} \)?
    \begin{answer}
      True by \nearbylemma{le:UniqDecIffBasisDec}.  
    \end{answer}
  \item 
    Can \( \Re^4 \) be decomposed as a direct sum in two different ways?
    Can \( \Re^1 \)?
    \begin{answer}
      Two distinct direct sum decompositions of \( \Re^4 \) are easy
      to find.
      Two such are
      \( W_1=\spanof{\set{\vec{e}_1,\vec{e}_2}} \) and
      \( W_2=\spanof{\set{\vec{e}_3,\vec{e}_4}} \), and also
      \( U_1=\spanof{\set{\vec{e}_1}} \) and
      \( U_2=\spanof{\set{\vec{e}_2,\vec{e}_3,\vec{e}_4}} \).
      (Many more are possible, 
      for example \( \Re^4 \) and its trivial subspace.)

      In contrast, any partition of \( \Re^1 \)'s single-vector basis will
      give one basis with no elements and another with a single element.
      Thus any decomposition involves \( \Re^1 \) and its trivial
      subspace. 
    \end{answer}
  \item 
     This exercise makes the notation of writing `$+$' between sets more
     natural. 
     Prove that, where \( W_1,\dots, W_k \) are subspaces of a vector space,
     \begin{equation*}
       W_1+\dots+W_k
       =\set{\vec{w}_1+\vec{w}_2+\dots+\vec{w}_k
             \suchthat \vec{w}_1\in W_1,\dots,\vec{w}_k\in W_k},
     \end{equation*}
     and so the sum of subspaces is the subspace of all sums.
     \begin{answer}
       Set inclusion one way is easy:
       \( \set{\vec{w}_1+\dots+\vec{w}_k\suchthat \vec{w}_i\in W_i} \)
       is a subset of \( \spanof{W_1\union \dots \union W_k}  \)
       because each \( \vec{w}_1+\dots+\vec{w}_k \) is a sum of vectors from 
       the union.

       For the other inclusion, to any linear combination of vectors from
       the union apply commutativity of vector addition to
       put vectors from \( W_1 \) first, followed by vectors from \( W_2 \), 
       etc.
       Add the vectors from \( W_1 \) to get a \( \vec{w}_1\in W_1 \),
       add the vectors from \( W_2 \) to get a \( \vec{w}_2\in W_2 \), etc.
       The result has the desired form.
     \end{answer}
  \item \label{exer:ThreeSubsPairwseNonTriv}
    (Refer to \nearbyexample{exam:DirSumThree}.
    This exercise 
    shows that the requirement that pairwise intersections be trivial
    is genuinely stronger than the requirement only that the intersection of
    all of the subspaces be trivial.)
    Give a vector space and three subspaces $W_1$, $W_2$, and $W_3$
    such that the space is the sum of the subspaces, 
    the intersection of all three subspaces
    $W_1\intersection W_2\intersection W_3$ is trivial, 
    but the pairwise intersections 
    $W_1\intersection W_2$, $W_1\intersection W_3$, and $W_2\intersection W_3$
    are nontrivial.
    \begin{answer}
      One example is to take the space to be $\Re^3$, and to take the
      subspaces to be the $xy$-plane, the $xz$-plane, and the $yz$-plane.
    \end{answer}
  \recommended \item 
    Prove that if \( V=W_1\directsum\dots\directsum W_k \) then
    \( W_i\intersection W_j \) is trivial whenever \( i\neq j \).
    This shows that the first half of the proof of 
    \nearbylemma{le:DirectSumTwoSp} extends to the case of more than two
    subspaces.
    (\nearbyexample{exam:DirSumThree} shows that this implication does
    not reverse; the other half does not extend.)
    \begin{answer}
      Of course, the zero vector is in all of the subspaces, so the 
      intersection contains at least that one vector..  
      By the definition of direct sum the set
      \( \set{W_1,\dots,W_k} \) is independent and so no nonzero vector
      of \( W_i \) is a multiple of a member of \( W_j \), when
      \( i\neq j \).
      In particular, no nonzero vector from \( W_i \) equals a member of
      \( W_j \).
    \end{answer}
  \item 
    Recall that no linearly independent set contains the zero vector.
    Can an independent set of subspaces contain the trivial subspace?
    \begin{answer}
      It can contain a trivial subspace;
      this set of subspaces of \( \Re^3 \) is independent:
      \( \set{\set{\zero},\text{\( x \)-axis}} \).
      No nonzero vector from the trivial space \( \set{\zero} \) 
      is a multiple of a
      vector from the \( x \)-axis, simply because the trivial space has
      no nonzero vectors to be candidates for such a multiple (and
      also no nonzero vector from the \( x \)-axis is a multiple of
      the zero vector from the trivial subspace).  
     \end{answer}
  \recommended \item 
    Does every subspace have a complement?
    \begin{answer}
      Yes.
      For any subspace of a vector space we can take any
      basis \( \sequence{\vec{\omega}_1,\dots,\vec{\omega}_k} \) for that
      subspace and extend it to a basis
      \( \sequence{\vec{\omega}_1,\dots,\vec{\omega}_k,
                     \vec{\beta}_{k+1},\dots,\vec{\beta}_n} \) for the whole
      space.
      Then the complement of the original subspace has this basis
      \( \sequence{\vec{\beta}_{k+1},\dots,\vec{\beta}_n} \).  
     \end{answer}
  \recommended \item 
    Let \( W_1, W_2 \) be subspaces of a vector space.
    \begin{exparts}
      \partsitem Assume that 
        the set \( S_1 \) spans \( W_1 \), and that the set \( S_2 \) spans 
        \( W_2 \).
        Can \( S_1\union S_2 \) span \( W_1+W_2 \)?
        Must it?
      \partsitem Assume that \( S_1 \) is a linearly independent 
        subset of \( W_1 \)
        and that \( S_2 \) is a linearly independent subset of \( W_2 \).
        Can \( S_1\union S_2 \) be a linearly independent subset of
        \( W_1+W_2 \)?
        Must it?
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem It must.
          We can write any member of \( W_1+W_2 \) as \( \vec{w}_1+\vec{w}_2 \)
          where \( \vec{w}_1\in W_1 \) and \( \vec{w}_2\in W_2 \).
          As \( S_1 \) spans \( W_1 \), the vector \( \vec{w}_1 \) is a
          combination of members of \( S_1 \).
          Similarly \( \vec{w}_2 \) is a combination of members of \( S_2 \).
        \partsitem An easy way to see that it can be linearly independent 
          is to take each to be the empty set.
          On the other hand, in the space $\Re^1$, 
          if \( W_1=\Re^1 \) and \( W_2=\Re^1 \) and $S_1=\set{1}$ and
          $S_2=\set{2}$, then their union $S_1\union S_2$ is not independent.
      \end{exparts}  
     \end{answer}
  \item \label{exer:BasesSumTwoSubs}
    When we decompose a vector space as a direct sum, the dimensions
    of the subspaces add to the dimension of the space.
    The situation with a space that is given as the sum of its subspaces 
    is not as simple.
    This exercise considers the two-subspace special case.
    \begin{exparts}
      \partsitem For these subspaces of \( \matspace_{\nbyn{2}} \) find
        \( W_1\intersection W_2 \), \( \dim(W_1\intersection W_2) \),
        \( W_1+W_2 \), and \( \dim(W_1+W_2) \).
        \begin{equation*}
          W_1=\set{\begin{mat}
                    0  &0  \\
                    c  &d
                  \end{mat} \suchthat c,d\in\Re  }
          \qquad
         W_2=\set{\begin{mat}
                    0  &b  \\
                    c  &0
                  \end{mat} \suchthat b,c\in\Re  }
       \end{equation*}
     \partsitem Suppose that \( U \) and \( W \) are subspaces 
       of a vector space.
       Suppose that the sequence 
       \( \sequence{\vec{\beta}_1,\dots,\vec{\beta}_k} \) 
       is a basis for
       \( U\intersection W \).
       Finally, suppose that the prior sequence has been expanded to give
       a sequence \( \sequence{\vec{\mu}_1,\dots,\vec{\mu}_j,
       \vec{\beta}_1,\dots,\vec{\beta}_k} \)
       that is a basis for \( U \), and a sequence
       \( \sequence{\vec{\beta}_1,\dots,\vec{\beta}_k,
          \vec{\omega}_1,\dots,\vec{\omega}_p} \)
       that is a basis for \( W \). 
       Prove that this sequence
       \begin{equation*}
         \sequence{\vec{\mu}_1,\dots,\vec{\mu}_j,
              \vec{\beta}_1,\dots,\vec{\beta}_k,
              \vec{\omega}_1,\dots,\vec{\omega}_p}
       \end{equation*}
       is a basis for the sum \( U+W \).
      \partsitem Conclude that 
          $\dim (U+W)=\dim(U)+\dim(W)-\dim(U\intersection W)$.
      \partsitem Let \( W_1 \) and \( W_2 \) be eight-dimensional 
        subspaces of a ten-di\-men\-sion\-al space.
       List all values possible for \( \dim(W_1\intersection W_2) \).
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem The intersection and sum are
          \begin{equation*}
             \set{\begin{mat}
                    0  &0  \\
                    c  &0
                  \end{mat} \suchthat c\in\Re  }
             \qquad
             \set{\begin{mat}
                    0  &b  \\
                    c  &d
                   \end{mat} \suchthat b,c,d\in\Re  }
          \end{equation*}
        which have dimensions one and three.
      \partsitem We write $B_{U\intersection W}$ for the basis for 
        $U\intersection W$,
        we write $B_U$ for the basis for $U$, 
        we write $B_W$ for the basis for $W$,
        and we write $B_{U+W}$ for the basis under consideration.

        To see that $B_{U+W}$ spans $U+W$, observe that we can write
        any vector $c\vec{u}+d\vec{w}$ from $U+W$ as a linear
        combination of the vectors in $B_{U+W}$, 
        simply by expressing $\vec{u}$ in 
        terms of $B_U$ and expressing $\vec{w}$ in terms of $B_W$.

         We finish by showing that $B_{U+W}$ is linearly independent. 
         Consider
         \begin{equation*}
           c_1\vec{\mu}_1+\dots+c_{j+1}\vec{\beta}_1+\dots
              +c_{j+k+p}\vec{\omega}_p=\zero
         \end{equation*}
         which can be rewritten in this way.
         \begin{equation*}
           c_1\vec{\mu}_1+\dots+c_j\vec{\mu}_j=
             -c_{j+1}\vec{\beta}_1-\dots-c_{j+k+p}\vec{\omega}_p
         \end{equation*}
         Note that the left side sums to a vector in \( U \) while 
         right side sums to a vector in \( W \), and thus both sides sum to
         a member of $U\intersection W$.
         Since the left side is a member of $U\intersection W$, 
         it is expressible in terms of the members of $B_{U\intersection W}$, 
         which gives the combination of $\vec{\mu}$'s from the left side above
         as equal to a combination of $\vec{\beta}$'s. 
         But, the fact that 
         the basis $B_U$ is linearly independent shows 
         that any such combination
         is trivial, and in particular, the coefficients $c_1$, \ldots,
         $c_j$ from the left side above are all zero.
         Similarly, the coefficients of the \( \vec{\omega} \)'s are all zero.
         This leaves the above equation as a linear relationship among the 
         \( \vec{\beta} \)'s, 
         but $B_{U\intersection W}$ is linearly independent, 
         and therefore all of the coefficients of the $\vec{\beta}$'s are also
         zero.
        \partsitem Just count the basis vectors in the prior 
          item:~$\dim(U+W)=j+k+p$, and $\dim(U)=j+k$, and $\dim(W)=k+p$, 
          and $\dim(U\intersection W)=k$.
        \partsitem  We know that
          \( \dim(W_1+W_2)=\dim(W_1)+\dim(W_2)-\dim(W_1\intersection W_2) \).
          Because \( W_1\subseteq W_1+W_2 \), 
          we know that \( W_1+W_2 \) must have dimension greater than that of
          $W_1$, that is, must have dimension eight, nine, or ten.
          Substituting gives us three possibilities
          $8=8+8-\dim(W_1\intersection W_2)$ or
          $9=8+8-\dim(W_1\intersection W_2)$ or
          $10=8+8-\dim(W_1\intersection W_2)$.
          Thus \( \dim(W_1\intersection W_2) \) must be either 
          eight, seven, or six.
          (Giving examples to show that 
          each of these three cases is possible
          is easy, for instance in \( \Re^{10} \).)  
      \end{exparts}
    \end{answer}
  \item 
    Let \( V=W_1\directsum\cdots\directsum W_k \) and for each index
    \( i \) suppose that \( S_i \) is a linearly independent subset of
    \( W_i \).
    Prove that the union of the \( S_i \)'s is linearly independent.
    \begin{answer}
      Expand each \( S_i \) to a basis $B_i$ for \( W_i \).
      The concatenation of those bases $\cat{\cat{B_1}{\cdots}}{B_k}$
      is a basis for \( V \) and thus its members form a linearly independent
      set.
      But the union $S_1\union\cdots\union S_k$ is a subset 
      of that linearly independent set, and thus is
      itself linearly independent.  
     \end{answer}
  \item 
    A matrix is \definend{symmetric}\index{matrix!symmetric}%
    \index{symmetric matrix}
    if for each pair of indices \( i \) and
    \( j \), the \( i,j \) entry equals the \( j,i \) entry.
    A matrix is \definend{antisymmetric}\index{matrix!antisymmetric}%
    \index{antisymmetric matrix} 
    if each \( i,j \) entry is the
    negative of the \( j,i \) entry.
    \begin{exparts}
      \partsitem Give a symmetric $\nbyn{2}$ matrix and an antisymmetric
        $\nbyn{2}$ matrix.
        (\textit{Remark.}
        For the second one, be careful about the entries on the diagonal.)
      \partsitem What is the relationship between a square symmetric matrix and
        its transpose?
        Between a square antisymmetric matrix and its transpose?
      \partsitem Show that \( \matspace_{\nbyn{n}} \) is the direct sum of 
        the space of symmetric matrices and the space of antisymmetric 
        matrices.
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem Two such are these.
          \begin{equation*}
            \begin{mat}[r]
              1  &2  \\
              2  &3
            \end{mat}
            \qquad
            \begin{mat}[r]
              0  &1  \\
              -1 &0
            \end{mat}
          \end{equation*}
          For the antisymmetric one, entries on the diagonal must be zero.
        \partsitem A square symmetric matrix equals its transpose.
          A square antisymmetric matrix equals the negative of its transpose.
        \partsitem Showing that the two sets are subspaces is easy.
         Suppose that \( A\in\matspace_{\nbyn{n}} \).
         To express $A$ as a sum of a symmetric and an antisymmetric matrix,
         we observe that
         \begin{equation*}
            A=(1/2)(A+\trans{A}) + (1/2)(A-\trans{A})
         \end{equation*}
         and note the first summand is symmetric while the second is 
         antisymmetric.
         Thus \( \matspace_{\nbyn{n}} \) is the sum of the two subspaces.
         To show that the sum is direct, 
         assume a matrix \( A \) is both symmetric
         \( A=\trans{A} \) and antisymmetric \( A=-\trans{A} \).
         Then \( A=-A \) and so all of \( A \)'s entries are zeroes.  
      \end{exparts}
     \end{answer}
  \item 
    Let \( W_1,W_2,W_3 \) be subspaces of a vector space.
    Prove that \( (W_1\intersection W_2)+(W_1\intersection W_3)\subseteq
              W_1\intersection (W_2+W_3) \).
    Does the inclusion reverse?
    \begin{answer}
      Assume that 
      \( \vec{v}\in (W_1\intersection W_2)+(W_1\intersection W_3) \).
      Then \( \vec{v}=\vec{w}_2+\vec{w}_3 \) where
      \( \vec{w}_2\in W_1\intersection W_2 \) and
      \( \vec{w}_3\in W_1\intersection W_3 \).
      Note that \( \vec{w}_2,\vec{w}_3\in W_1 \) and, as a subspace is closed
      under addition, \( \vec{w}_2+\vec{w}_3\in W_1 \).
      Thus \( \vec{v}=\vec{w}_2+\vec{w}_3\in W_1\intersection(W_2+W_3) \).

      This example proves that the inclusion may be strict:
      in \( \Re^2 \) take \( W_1 \) to be the \( x \)-axis, take \( W_2 \)
      to be the \( y \)-axis, and take \( W_3 \) to be the line \( y=x \).
      Then \( W_1\intersection W_2 \) and \( W_1\intersection W_3 \) are
      trivial and so their sum is trivial.
      But \( W_2+W_3 \) is all of \( \Re^2 \) so
      \( W_1\intersection(W_2+W_3) \) is the \( x \)-axis.  
     \end{answer}
  \item 
    The example of the $x$-axis and the $y$-axis in
    \( \Re^2 \) shows that \( W_1\directsum W_2=V \) does not imply that
    \( W_1\union W_2=V \).
    Can \( W_1\directsum W_2=V \) and \( W_1\union W_2=V \) happen?
    \begin{answer}
      It happens when at least one of \( W_1,W_2 \) is trivial.
      But that is the only way it can happen.

      To prove this, assume that both are non-trivial,
      select nonzero vectors \( \vec{w}_1, \vec{w}_2 \) from each,
      and consider \( \vec{w}_1+\vec{w}_2 \).
      This sum is not in \( W_1 \) because
      \( \vec{w}_1+\vec{w}_2=\vec{v}\in W_1 \) would imply that
      \( \vec{w}_2=\vec{v}-\vec{w}_1 \) is in \( W_1 \),
      which violates the assumption of the independence of the subspaces.
      Similarly, \( \vec{w}_1+\vec{w}_2 \) is not in \( W_2 \).
      Thus there is an element of \( V \) that is not in \( W_1\union W_2 \).
     \end{answer}
%   \recommended \item
%     Our model for complementary subspaces, the $x$-axis and the
%     $y$-axis in \( \Re^2 \), has one property not used here.
%     Where \( U \) is a subspace of \( \Re^n \) we define the
%     \definend{orthocomplement}\index{subspace!orthocomplement} 
%     of \( U \) to be
%     \begin{equation*}
%       U^\perp=
%       \set{\vec{v}\in\Re^n \suchthat \text{\( \vec{v}\dotprod\vec{u}=0 \)
%                                             for all \( \vec{u}\in U \)}}
%     \end{equation*}
%     (read ``\( U \) perp'').
%     \begin{exparts}
%       \partsitem Find the orthocomplement of the \( x \)-axis in \( \Re^2 \).
%       \partsitem Find the orthocomplement of the \( x \)-axis in \( \Re^3 \).
%       \partsitem Find the orthocomplement of the \( xy \)-plane in \( \Re^3 \).
%       \partsitem Show that the orthocomplement of a subspace is a subspace.
%       \partsitem Show that if \( W \) is the orthocomplement of \( U \) then
%         \( U \) is the orthocomplement of \( W \).
%       \partsitem Prove that a subspace and its orthocomplement have a trivial
%         intersection.
%       \partsitem Conclude that for any \( n \) and subspace 
%         \( U\subseteq \Re^n \) we have that \( \Re^n=U\directsum U^\perp \).
%       \partsitem Show that 
%         \( \dim(U)+\dim(U^\perp) \) equals the dimension of the
%         enclosing space.
%     \end{exparts}
%     \begin{answer}
%       \begin{exparts}
%         \partsitem The set 
%           \begin{equation*}
%             \set{\colvec{v_1 \\ v_2} 
%                \suchthat \colvec{v_1 \\ v_2}\dotprod\colvec{x \\ 0}=0 
%                               \text{\ for all $x\in\Re$}}
%           \end{equation*}
%           is easily seen to be the $y$-axis.
%         \partsitem The \( yz \)-plane.
%         \partsitem The \( z \)-axis.
%         \partsitem 
%           Assume that \( U \) is a subspace of some \( \Re^n \).
%           Because $U^\perp$ contains the zero vector, since that
%           vector is perpendicular to everything, we need only show that the
%           orthocomplement is closed under linear combinations of two elements.
%           If \( \vec{w}_1, \vec{w}_2\in U^\perp \) then
%           \( \vec{w}_1\dotprod\vec{u}=0 \) and \( \vec{w}_2\dotprod\vec{u}=0 \)
%           for all \( \vec{u}\in U \).
%           Thus \( (c_1\vec{w}_1+c_2\vec{w}_2)\dotprod\vec{u}=
%               c_1(\vec{w}_1\dotprod\vec{u})+c_2(\vec{w}_2\dotprod\vec{u})=0 \)
%           for all \( \vec{u}\in U \) and so \( U^\perp \) is closed under
%           linear combinations.
%         \partsitem The only vector orthogonal to itself is the zero vector.
%         \partsitem This is immediate.
%         \partsitem To prove that the dimensions add, 
%           it suffices by \nearbycorollary{cor:DirSumDimsAdd} and 
%           \nearbylemma{le:DirectSumTwoSp} to show that 
%           \( U\intersection U^\perp \) is the trivial subspace
%           \( \set{\zero} \).
%           But this is one of the prior items in this problem.
%       \end{exparts}  
%      \end{answer}
  \recommended \item 
    Consider \nearbycorollary{cor:DirSumDimsAdd}.
    Does it work both ways\Dash that is,
    supposing that \( V=W_1+\dots+ W_k \),
    is \( V=W_1\directsum\cdots\directsum W_k \) if and only if
    \( \dim(V)=\dim(W_1)+\dots+\dim(W_k) \)?
    \begin{answer}
      Yes.
      The left-to-right implication is
      \nearbycorollary{cor:DirSumDimsAdd}.
      For the other direction,
      assume that \( \dim(V)=\dim(W_1)+\dots+\dim(W_k) \).
      Let \( B_1,\dots, B_k \) be bases for \( W_1,\dots, W_k \).
      As $V$ is the sum of the subspaces, we can write 
      any \( \vec{v}\in V \) as
      \( \vec{v}=\vec{w}_1+\cdots+\vec{w}_k \) and expressing each
      \( \vec{w}_i \) as a combination of vectors from the associated
      basis \( B_i \) shows that the concatenation
      \( \cat{B_1}{\cat{\cdots}{B_k}}  \) spans \( V \).
      Now, that concatenation has
      \( \dim(W_1)+\dots+\dim(W_k) \) members, and so it is a spanning set of 
      size \( \dim(V) \).
      The concatenation is therefore a basis for \( V \).
      Thus \( V \) is the direct sum.
     \end{answer}
  \item 
    We know that if \( V=W_1\directsum W_2 \) then there is a basis for
    \( V \) that splits into a basis for \( W_1 \) and a basis for
    \( W_2 \).
    Can we make the stronger statement that every basis for \( V \) splits into
    a basis for \( W_1 \) and a basis for \( W_2 \)?
    \begin{answer}
      No.
      The standard basis for \( \Re^2 \) does not split into bases for
      the complementary subspaces the line \( x=y \) and the line 
      \( x=-y \).  
    \end{answer}
  \item 
     We can ask about the algebra of the `$+$' operation.
     \begin{exparts}
       \partsitem Is it commutative; is \( W_1+W_2=W_2+W_1 \)?
       \partsitem Is it associative; is \( (W_1+W_2)+W_3=W_1+(W_2+W_3) \)?
       \partsitem Let \( W \) be a subspace of some vector space.
         Show that \( W+W=W \).
       \partsitem Must there be an identity element,
         a subspace \( I \) such that \( I+W=W+I=W \) 
         for all subspaces \( W \)? 
       \partsitem Does left-cancellation hold:~if 
          \( W_1+W_2=W_1+W_3 \) then \( W_2=W_3 \)?
          Right cancellation?
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem  Yes, \( W_1+W_2=W_2+W_1 \) for all subspaces \( W_1,W_2 \)
          because each side is the span of \( W_1\union W_2=W_2\union W_1 \).
        \partsitem This one is similar to the prior one\Dash each side 
          of that equation is the span of
          \( (W_1\union W_2)\union W_3=W_1\union (W_2\union W_3) \).
        \partsitem Because this is an equality between sets, we can show that
          it holds by mutual inclusion. 
          Clearly \( W\subseteq W+W \).
          For \( W+W\subseteq W \) just recall that every subset is closed 
          under addition so any sum of the form \( \vec{w}_1+\vec{w}_2 \) is in
          \( W \).
        \partsitem In each vector space, the identity element with respect 
          to subspace addition is the trivial subspace.
        \partsitem   Neither of left or right cancellation needs to hold.
          For an example, in \( \Re^3 \) take \( W_1 \) to be the 
          \( xy \)-plane, take \( W_2 \) to be the \( x \)-axis, 
          and take \( W_3 \) to be the \( y \)-axis.  
      \end{exparts}
     \end{answer}
  % 2014-Dec-19 I commented this out to make the pages fit better; it is perfectly fine
  % \item 
  %   Consider the algebraic properties of the direct sum operation.
  %   \begin{exparts}
  %      \partsitem Does direct sum commute: does \( V=W_1\directsum W_2 \) imply
  %        that \( V=W_2\directsum W_1 \)?
  %     \partsitem Prove that direct sum is associative:
  %       \( (W_1\directsum W_2)\directsum W_3=
  %           W_1\directsum(W_2\directsum W_3) \).
  %      \partsitem Show that \( \Re^3 \) is the direct sum of the three axes
  %        (the relevance here is that by the previous item,
  %         we needn't specify which two of the three axes are combined first).
  %      \partsitem Does the direct sum operation left-cancel:~does
  %        \( W_1\directsum W_2=W_1\directsum W_3 \) imply \( W_2=W_3 \)?
  %        Does it right-cancel?
  %      \partsitem There is an identity element with respect to this operation.
  %        Find it.
  %      \partsitem Do some, or all, subspaces have inverses with respect to this
  %        operation:~is there a subspace \( W \) of some vector space such
  %        that there is a subspace \( U \) with the property that
  %        \( U\directsum W \) equals the identity element from
  %        the prior item?
  %   \end{exparts}
  %   \begin{answer}
  %     \begin{exparts}
  %        \partsitem They are equal because for each, 
  %          \( V \) is the direct sum if
  %          and only if we can write each \( \vec{v}\in V \) in a unique
  %          way as a sum \( \vec{v}=\vec{w}_1+\vec{w}_2 \) and
  %          \( \vec{v}=\vec{w}_2+\vec{w}_1 \).
  %        \partsitem They are equal because for each, 
  %          \( V \) is the direct sum if
  %          and only if we can write each \( \vec{v}\in V \) in a unique
  %          way as a sum of a vector from each
  %          $\vec{v}=(\vec{w}_1+\vec{w}_2)+\vec{w}_3$
  %          and $\vec{v}=\vec{w}_1+(\vec{w}_2+\vec{w}_3)$.
  %        \partsitem We can decompose any vector in \( \Re^3 \) uniquely into
  %          the sum of a vector from each axis.
  %        \partsitem No.
  %          For an example, in \( \Re^2 \) take \( W_1 \) to be the
  %          \( x \)-axis, take \( W_2 \) to be the \( y \)-axis, and
  %          take \( W_3 \) to be the line \( y=x \).
  %        \partsitem In any vector space the trivial subspace acts as 
  %          the identity element with respect to direct sum.
  %        \partsitem In any vector space, only the trivial subspace has
  %          a direct-sum inverse (namely, itself).
  %          One way to see this is that dimensions add, and so increase.
  %     \end{exparts}  
  %    \end{answer}
\end{exercises}
\index{direct sum|)}
