% Chapter 4, Section 2 _Linear Algebra_ Jim Hefferon
%  http://joshua.smcvt.edu/linearalgebra
%  2001-Jun-12
\section{Similarity}
\index{similarity|(}
%<*SimilarityMotiviation0>
We've defined two matrices \( H \) and \( \hat{H} \) to be
matrix equivalent if there are nonsingular \( P \) and \( Q \)
such that \( \hat{H}=PHQ \).
We were motivated by this diagram
showing $H$ and $\hat{H}$ both representing a map 
$h$, but with respect to different
pairs of bases, $B,D$ and $\hat{B},\hat{D}$.
\begin{equation*}
  \begin{CD}
    V_{\wrt{B}}                   @>h>H>        W_{\wrt{D}}       \\
    @V{\scriptstyle\identity} VV             @V{\scriptstyle\identity} VV \\
    V_{\wrt{\hat{B}}}             @>h>\hat{H}>  W_{\wrt{\hat{D}}}
  \end{CD}
\end{equation*}
%</SimilarityMotiviation0>

%<*SimilarityMotiviation1>
We now consider the special case of transformations, 
where the codomain equals the domain, and we add the requirement
that the codomain's basis equals the domain's basis.
So, we are considering representations with respect to 
$B,B$ and $D,D$.\index{arrow diagram}
\begin{equation*}
  \begin{CD}
    V_{\wrt{B}}                   @>t>T>        V_{\wrt{B}}       \\
    @V{\scriptstyle\identity} VV              @V{\scriptstyle\identity} VV \\
    V_{\wrt{D}}                   @>t>\hat{T}>        V_{\wrt{D}}
  \end{CD}
\end{equation*}
In matrix terms, 
\(\rep{t}{D,D}
  =\rep{\identity}{B,D}\;\rep{t}{B,B}\;\bigl(\rep{\identity}{B,D}\bigr)^{-1} \).
%</SimilarityMotiviation1>



\subsection{Definition and Examples}
\begin{example}
Consider the derivative transformation
$\map{d/dx}{\polyspace_2}{\polyspace_2}$,
and two bases for that space
$B=\sequence{x^2,x,1}$ and
$D=\sequence{1,1+x,1+x^2}$
We will compute the four sides of the arrow square.
\begin{equation*}
  \begin{CD}
    {\polyspace_2\,}_{\wrt{B}}                   @>d/dx>T>        {\polyspace_2\,}_{\wrt{B}}       \\
    @V{\scriptstyle\identity} VV              @V{\scriptstyle\identity} VV \\
    {\polyspace_2\,}_{\wrt{D}}                   @>d/dx>\hat{T}>        {\polyspace_2\,}_{\wrt{D}}
  \end{CD}
\end{equation*}
The top is first.
The effect of the transformation on the starting basis~$B$
\begin{equation*}
  x^2\mapsunder{d/dx} 2x
  \quad
  x\mapsunder{d/dx} 1
  \quad
  1\mapsunder{d/dx} 0
\end{equation*}
represented with respect to the ending basis (also~$B$)
\begin{equation*}
  \rep{2x}{B}=\colvec{0 \\ 2 \\ 0}
  \qquad
  \rep{1}{B}=\colvec{0 \\ 0 \\ 1}
  \qquad
  \rep{0}{B}=\colvec{0 \\ 0 \\ 0}
\end{equation*}
gives the representation 
of the map.
\begin{equation*}
  T=
  \rep{d/dx}{B,B}=
  \begin{mat}
    0 &0 &0 \\
    2 &0 &0 \\
    0 &1 &0
  \end{mat}
\end{equation*}

Next, the bottom.
The effect of the transformation on elements of~$D$
\begin{equation*}
  1\mapsunder{d/dx} 0
  \qquad
  1+x\mapsunder{d/dx} 1
  \qquad
  1+x^2\mapsunder{d/dx} 2x
\end{equation*}
represented with respect to~$D$
gives the matrix~$\hat{T}$.
\begin{equation*}
  \hat{T}=
  \rep{d/dx}{D,D}=
  \begin{mat}
    0 &1 &-2 \\
    0 &0 &2 \\
    0 &0 &0
  \end{mat}
\end{equation*}

Third, computing the matrix for the right-hand side involves
finding the effect of the identity map on the elements of~$B$.
Of course, the identity map does not transform them at all so
to find the matrix we represent $B$'s elements with respect 
to~$D$.
\begin{equation*}
  \rep{x^2}{D}=\colvec{-1 \\ 0 \\ 1}
  \quad
  \rep{x}{D}=\colvec{-1 \\ 1 \\ 0}
  \quad
  \rep{1}{D}=\colvec{1 \\ 0 \\ 0}
\end{equation*}
So the matrix for going down the right side is the concatenation of those.
\begin{equation*}
  P=\rep{id}{B,D}=
  \begin{mat}
    -1 &-1  &1 \\
     0 &1   &0 \\
     1 &0   &0   
  \end{mat}
\end{equation*}

With that, we have two options to compute the matrix for going up on 
left side.
The direct computation represents elements of~$D$ with
respect to~$B$
\begin{equation*}
  \rep{1}{B}=\colvec{0 \\ 0 \\ 1}
  \quad
  \rep{1+x}{B}=\colvec{0 \\ 1 \\ 1}
  \quad
  \rep{1+x^2}{B}=\colvec{1 \\ 0 \\ 1}
\end{equation*}
and concatenates to make the matrix.
\begin{equation*}
  \begin{mat}
    0 &0 &1 \\
    0 &1 &0 \\ 
    1 &1 &1
  \end{mat}
\end{equation*}
The other option to compute the matrix for going up on the left 
is to take the inverse of the matrix~$P$ for
going down on the right.
\begin{equation*}
  \begin{pmat}{ccc|ccc}
    -1 &-1 &1 &1 &0 &0 \\
     0 &1  &0 &0 &1 &0 \\
     1 &0  &0 &0 &0 &1
  \end{pmat}
  % \grstep{\rho_1+\rho_3}
  % \grstep{\rho_2+\rho_3}
  % \grstep{-\rho_1}
  % \grstep{\rho_3+\rho_1}
  % \grstep{-\rho_2+\rho_1}
  \grstep{}
  \cdots
  \grstep{}
  \begin{pmat}{ccc|ccc}
     1 &0  &0 &0 &0 &1 \\
     0 &1  &0 &0 &1 &0 \\
     0 &0  &1 &1 &1 &1
  \end{pmat}
\end{equation*}
\end{example}

\begin{definition} \label{df:Similar}
%<*df:Similar>
The matrices  \( T \) and $\hat{T}$ are 
\definend{similar}\index{matrix!similarity}%
\index{equivalence relation!matrix similarity}\index{similar matrices}
if there is a nonsingular \( P \) such that
$
  \hat{T}=PTP^{-1}
$.
%</df:Similar>
\end{definition}

\noindent Since nonsingular matrices are square, 
$T$ and $\hat{T}$ must
be square and of the same size.
\nearbyexercise{exer:SimIsEquivRel} checks that
similarity is an equivalence relation.

\begin{example}
The definition does not require that we consider a map.
Calculation with these two
\begin{equation*}
  P=
  \begin{mat}[r]
    2  &1  \\
    1  &1
  \end{mat}
  \qquad
  T=
  \begin{mat}[r]
    2  &-3  \\
    1  &-1
  \end{mat}
\end{equation*}
gives that $T$ is similar to this matrix.
\begin{equation*}
  \hat{T}=
  \begin{mat}[r]
    12  &-19  \\
    7  &-11
  \end{mat}
\end{equation*}
\end{example}

\begin{example}  \label{ex:OnlyZeroSimToZero}
%<*ex:OnlyZeroSimToZero>
The only matrix similar to the zero matrix is itself:~$PZP^{-1}=PZ=Z$.
The identity matrix has the same property:~$PIP^{-1}=PP^{-1}=I$.
%</ex:OnlyZeroSimToZero>
\end{example}


Matrix similarity is a special case of matrix equivalence so 
if two matrices are similar then they are matrix equivalent.
What about the converse:~if they are square, 
must any two matrix equivalent matrices be similar?
No; the matrix equivalence class
of an identity matrix consists of all nonsingular matrices of that size
while the prior example shows that the only member of the similarity class 
of an identity matrix is itself. 
Thus these two are
matrix equivalent but not similar.
\begin{equation*}
  T=
  \begin{mat}[r]
    1  &0  \\
    0  &1
  \end{mat}
  \qquad
  S=
  \begin{mat}[r]
    1  &2  \\
    0  &3
  \end{mat}
\end{equation*}
So some matrix equivalence classes
split into two or more similarity classes\Dash similarity gives a finer
partition than does matrix equivalence.
This shows some matrix equivalence classes subdivided into
similarity classes.
\begin{center}
  \includegraphics{ch5.4}
\end{center}

To understand the similarity relation we shall study the similarity classes.
We approach this question in the same way that we've studied both the
row equivalence and matrix equivalence relations, by finding
a canonical form for
representatives % \appendrefs{representatives}\spacefactor=1000 %
of the similarity classes, called Jordan form.
With this canonical form, we can decide if two matrices are similar by checking
whether they are in a class with the same representative.
We've also seen with both row equivalence and matrix equivalence that a
canonical form gives us insight into the ways in which members of
the same class are alike
(e.g., two identically-sized matrices are matrix equivalent
if and only if they have the same rank).
%(Along the way we shall see ideas that are interesting and important
%in their own right, not just as stepping stones to Jordan form.)

\begin{exercises}
  \item 
    For
    \begin{equation*}
      T=
      \begin{mat}[r]
        1  &3  \\
       -2  &-6
      \end{mat}
      \quad
      \hat{T}=
      \begin{mat}[r]
        0    &0  \\
       -11/2 &-5
      \end{mat}
      \quad
      P=
      \begin{mat}[r]
        4  &2  \\
       -3  &2
      \end{mat}
    \end{equation*}
    check that $\hat{T}=PTP^{-1}$.
    \begin{answer}
      One way to proceed is left to right.
      \begin{multline*}
        PTP^{-1}=
        \begin{mat}[r]
          4  &2  \\
         -3  &2
        \end{mat}
        \begin{mat}[r]
          1  &3  \\
         -2  &-6
        \end{mat}
        \begin{mat}[r]
          2/14  &-2/14  \\
          3/14  &4/14
        \end{mat}                                 \\
        =
        \begin{mat}[r]
          0  &0  \\
         -7  &-21
        \end{mat}  
        \begin{mat}[r]
          2/14  &-2/14  \\
          3/14  &4/14
        \end{mat}
        =
        \begin{mat}[r]
          0    &0  \\
         -11/2 &-5
        \end{mat}
      \end{multline*}
    \end{answer}
  \item
    \nearbyexample{ex:OnlyZeroSimToZero} shows that the only matrix similar
    to a zero matrix is itself and that 
    the only matrix similar to the identity
    is itself.
    \begin{exparts}
      \partsitem Show that the $\nbyn{1}$ matrix whose single entry is $2$
         is also similar only to itself.
      \partsitem Is a matrix of the form $cI$ for some scalar $c$
         similar only to itself?
     \partsitem Is a diagonal matrix similar only to itself?
     % \partsitem Is a square block partial-identity matrix similar only to 
     % itself?
    \end{exparts}
    \begin{answer}
     \begin{exparts}
      \partsitem Because the matrix $(2)$ is $\nbyn{1}$, the matrices
         $P$ and $P^{-1}$ are also $\nbyn{1}$ and so where
         $P=(p)$ the inverse is $P^{-1}=(1/p)$.  
         Thus $P(2)P^{-1}=(p)(2)(1/p)=(2)$.
      \partsitem Yes:~recall that we can bring scalar multiples out 
        of a matrix \( P(cI)P^{-1}=cPIP^{-1}=cI \).
        By the way, the zero and identity matrices are the special cases
        $c=0$ and $c=1$.
      \partsitem No, as this example shows.
        \begin{equation*}
           \begin{mat}[r]
              1  &-2  \\
             -1  &1
            \end{mat}
           \begin{mat}[r]
             -1  &0   \\
              0  &-3
           \end{mat}
           \begin{mat}[r]
              -1  &-2   \\
              -1  &-1
           \end{mat}
           =
           \begin{mat}[r]
              -5  &-4   \\
              2   &1
           \end{mat}
        \end{equation*} 
    \end{exparts}  
   \end{answer}
  \recommended \item Consider this transformation of~$\C^3$
    \begin{equation*}
      t(\colvec{x \\ y \\ z})=\colvec{x-z \\ z \\ 2y}
    \end{equation*}
    and these bases.
    \begin{equation*}
      B=\sequence{\colvec{1 \\ 2 \\ 3}, 
                  \colvec{0 \\ 1 \\ 0}, 
                  \colvec{0 \\ 0 \\ 1}}
      \qquad
      D=\sequence{\colvec{1 \\ 0 \\ 0},
                  \colvec{1 \\ 1 \\ 0},
                  \colvec{1 \\ 0 \\ 1}}
    \end{equation*}
    We will compute the parts of the arrow diagram to 
    represent the transformation using two similar matrices.
    \begin{exparts}
      \partsitem Draw the arrow diagram, specialized for this case.
      \partsitem Compute $T=\rep{t}{B,B}$.
      \partsitem Compute $\hat{T}=\rep{t}{D,D}$.
      \partsitem Compute the matrices for other the two sides of the arrow 
         square.
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem
          \begin{equation*}
            \begin{CD}
              \C^3_{\wrt{B}}                   @>t>T>        \C^3_{\wrt{B}}       \\
              @V{\scriptstyle\identity} VV              @V{\scriptstyle\identity} VV \\
              \C^3_{\wrt{D}}                   @>t>\hat{T}>        \C^3_{\wrt{D}}
            \end{CD}
          \end{equation*}
        \partsitem
          For each element of the starting basis~$B$ find the effect of 
          the transformation
          \begin{equation*}
             \colvec{1 \\ 2 \\ 3}\mapsunder{t}\colvec{-2 \\ 3 \\ 4}
             \qquad 
             \colvec{0 \\ 1 \\ 0}\mapsunder{t}\colvec{0 \\ 0 \\ 2}
             \qquad 
             \colvec{0 \\ 0 \\ 1}\mapsunder{t}\colvec{-1 \\ 1 \\ 0}
          \end{equation*}
          and represented those outputs with respect to the ending basis~$B$
          \begin{equation*}
            \rep{\colvec{-2 \\ 3 \\ 4}}{B}=\colvec{-2 \\ 7 \\ 10}
            \qquad
            \rep{\colvec{0 \\ 0 \\ 2}}{B}=\colvec{0 \\ 0 \\ 2}
            \qquad
            \rep{\colvec{-1 \\ 1 \\ 0}}{B}=\colvec{-1 \\ 3 \\ 3}
          \end{equation*}
          to get the matrix.
          \begin{equation*}
            T=\rep{t}{B,B}=
            \begin{mat}
              -2 &0 &-1 \\
               7 &0 &3  \\
              10 &2 &3 
            \end{mat}
          \end{equation*}
        \partsitem
          Find the effect of the transformation on the elements of~$D$
          \begin{equation*}
             \colvec{1 \\ 0 \\ 0}\mapsunder{t}\colvec{1 \\ 0 \\ 0}
             \qquad 
             \colvec{1 \\ 1 \\ 0}\mapsunder{t}\colvec{1 \\ 0 \\ 2}
             \qquad 
             \colvec{1 \\ 0 \\ 1}\mapsunder{t}\colvec{0 \\ 1 \\ 0}
          \end{equation*}
          and represented those with respect to the ending basis~$D$
          \begin{equation*}
            \rep{\colvec{1 \\ 0 \\ 0}}{D}=\colvec{1 \\ 0 \\ 0}
            \qquad
            \rep{\colvec{1 \\ 0 \\ 2}}{D}=\colvec{-1 \\ 0 \\ 2}
            \qquad
            \rep{\colvec{0 \\ 1 \\ 0}}{D}=\colvec{-1 \\ 1 \\ 0}
          \end{equation*}
          to get the matrix.
          \begin{equation*}
            \hat{T}=\rep{t}{D,D}=
            \begin{mat}
               1 &-1 &-1 \\
               0 &0  &1  \\
               0 &2  &0 
            \end{mat}
          \end{equation*}
        \partsitem
          To go down on the right we need 
          $\rep{\identity}{B,D}$
          so we first compute the effect of the identity map on each element 
          of~$D$,
          which is no effect, and then represent the results with respect 
          to~$B$. 
          \begin{equation*}
            \rep{\colvec{1 \\ 2 \\ 3}}{D}=\colvec{-4 \\ 2 \\ 3}
            \qquad
            \rep{\colvec{0 \\ 1 \\ 0}}{D}=\colvec{-1 \\ 1 \\ 0}
            \qquad
            \rep{\colvec{0 \\ 0 \\ 1}}{D}=\colvec{-1 \\ 0 \\ 1}
          \end{equation*}
          So this is~$P$.
          \begin{equation*}
            P=
            \begin{mat}
              -4 &-1 &-1 \\
               2 &1  &0  \\
               3 &0  &1
            \end{mat}
          \end{equation*}
          For the other matrix~$\rep{\identity}{D,B}$ we can either find 
          it directly, as we just have with~$P$, or we can do the 
          usual calculation of a matrix inverse.
          \begin{equation*}
            P^{-1}=
            \begin{mat}
               1 &1 &1 \\
               -2 &-1  &-2  \\
               -3 &-3  &-2
            \end{mat}
          \end{equation*}
      \end{exparts}
    \end{answer}
  \item 
    Consider the transformation $\map{t}{\polyspace_2}{\polyspace_2}$
    described by
    $x^2\mapsto x+1$, $x\mapsto x^2-1$, and $1\mapsto 3$.
    \begin{exparts}
      \partsitem Find $T=\rep{t}{B,B}$ where $B=\sequence{x^2,x,1}$. 
      \partsitem Find $\hat{T}=\rep{t}{D,D}$ where $D=\sequence{1,1+x,1+x+x^2}$.
      \partsitem Find the matrix $P$ such that $\hat{T}=PTP^{-1}$. 
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem Because we describe $t$ with the members of $B$,
          finding the matrix representation is easy:
          \begin{equation*}
            \rep{t(x^2)}{B}=\colvec[r]{0 \\ 1 \\ 1}_B
            \quad
            \rep{t(x)}{B}=\colvec[r]{1 \\ 0 \\ -1}_B
            \quad
            \rep{t(1)}{B}=\colvec[r]{0 \\ 0 \\ 3}_B
          \end{equation*}
          gives this.
          \begin{equation*}
            \rep{t}{B,B}
            \begin{mat}[r]
              0  &1  &0  \\
              1  &0  &0  \\
              1  &-1 &3  
            \end{mat}
          \end{equation*}
        \partsitem We will find $t(1)$, $t(1+x)$, and $t(1+x+x^2$,
          to find how each is represented with respect to $D$.
          We are given that $t(1)=3$, and the other two are easy to see:
          $t(1+x)=x^2+2$ and $t(1+x+x^2)=x^2+x+3$.
          By eye, we get the representation of each vector
          \begin{equation*}
            \rep{t(1)}{D}=\colvec[r]{3 \\ 0 \\ 0}_D
            \quad
            \rep{t(1+x)}{D}=\colvec[r]{2  \\ -1 \\  1}_D
            \quad
            \rep{t(1+x+x^2)}{D}=\colvec[r]{2 \\ 0 \\ 1}_D
          \end{equation*}
          and thus the representation of the map.
          \begin{equation*}
            \rep{t}{D,D}
            =
            \begin{mat}[r]
              3  &2  &2  \\
              0  &-1 &0  \\
              0  &1  &1
            \end{mat}
          \end{equation*}
         \partsitem The diagram
           \begin{equation*}
             \begin{CD}
               V_{\wrt{B}}                  @>t>T>  V_{\wrt{B}}       \\
               @V\scriptstyle\identity VPV      @V\scriptstyle\identity VPV \\
               V_{\wrt{D}}                  @>t>\hat{T}>  V_{\wrt{D}}
             \end{CD}
           \end{equation*}
           shows that these are $P=\rep{\identity}{B,D}$ and 
           $P^{-1}=\rep{\identity}{D,B}$.
           \begin{equation*}
             P=
             \begin{mat}[r]
               0  &-1  &1  \\ 
               -1  &1  &0  \\
               1  &0  &0 
             \end{mat}
             \qquad
             P^{-1}=
             \begin{mat}[r]
               0  &0  &1  \\ 
               0  &1  &1  \\
               1  &1  &1 
             \end{mat}
           \end{equation*}
      \end{exparts}
    \end{answer}
\item 
    Let $T$ represent $\map{t}{\C^2}{\C^2}$ with respect to $B,B$.
    \begin{equation*}
      T=
      \begin{mat}
        1 &-1 \\ 
        2 &1
      \end{mat}
      \qquad
      B=\sequence{\colvec{1 \\ 0},\colvec{1 \\ 1}},\hspace{0.7em}
      D=\sequence{\colvec{2 \\ 0},\colvec{0 \\ -2}}
    \end{equation*}
    We will convert to the matrix representing~$t$ with resepct to $D,D$.
    \begin{exparts}
      \partsitem Draw the arrow diagram.
      \partsitem Give the matrix that represents the left and right
          sides of that diagram, in the
          direction that we traverse the diagram to make the conversion.
      \partsitem Find $\rep{t}{D,D}$.
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem
          \begin{equation*}
            \begin{CD}
              \C^2_{\wrt{B}}                   @>t>T>        \C^2_{\wrt{B}}       \\
              @V{\scriptstyle\identity} VV              @V{\scriptstyle\identity} VV \\
              \C^2_{\wrt{D}}                   @>t>\hat{T}>        \C^2_{\wrt{D}}
            \end{CD}
          \end{equation*}  
        \partsitem
          For the right side we find the effect of the identity map,
          which is no effect,
          \begin{equation*}
            \colvec{1 \\ 0}\mapsunder{\identity}\colvec{1 \\ 0}
            \qquad
            \colvec{1 \\ 1}\mapsunder{\identity}\colvec{1 \\ 1}
          \end{equation*}
          and represent those with respect to~$D$
          \begin{equation*}
            \rep{\colvec{1 \\ 0}}{D}=\colvec{1/2 \\ 0}
            \qquad
            \rep{\colvec{1 \\ 1}}{D}=\colvec{1/2 \\ -1/2}
          \end{equation*}
          so we have this.
          \begin{equation*}
            P=\rep{\identity}{B,D}=
            \begin{mat}
              1/2 &1/2 \\
              0   &-1/2
            \end{mat}
          \end{equation*}

          For the matrix on the left we can either compute it directly, 
          as in the
          prior paragraph, or we can take the inverse. 
          \begin{equation*}
            P^{-1}=\rep{\identity}{D,B}=
            \frac{1}{(-1/4)}\cdot
            \begin{mat}
              -1/2 &-1/2 \\
              0   &1/2
            \end{mat}
            =
            \begin{mat}
              2 &2 \\
              0 &-2
            \end{mat}
          \end{equation*}
        \partsitem
          As with the prior item we can either compute it directly from 
          the definition
          or compute it using matrix operations.
          \begin{equation*}
            PTP^{-1}=
            \begin{mat}
              2 &2 \\
              0 &-2
            \end{mat}
            \begin{mat}
              1 &-1 \\ 
              2 &1
            \end{mat}
            \begin{mat}
              2 &2 \\
              0 &-2
            \end{mat}
            =
            \begin{mat}
              3 &3  \\
             -2 &-1
            \end{mat}
          \end{equation*}
      \end{exparts}
    \end{answer}
  \recommended \item
     Exhibit an nontrivial similarity relationship by letting
     \( \map{t}{\C^2}{\C^2} \) act in this way,
     \begin{equation*}
        \colvec[r]{1 \\ 2}\mapsto\colvec[r]{3 \\ 0}
        \qquad
        \colvec[r]{-1 \\ 1}\mapsto\colvec[r]{-1 \\ 2}
     \end{equation*}
     picking two bases~$B,D$,
     and representing \( t \) with respect to them,
     \( \hat{T}=\rep{t}{B,B} \) and \( T=\rep{t}{D,D} \).
     Then compute 
     the \( P \) and \( P^{-1} \) to change bases from \( B \) to \( D \) and
     back again.
     \begin{answer}
       One possible choice of the bases is 
       \begin{equation*}
         B=\sequence{\colvec[r]{1 \\ 2},\colvec[r]{-1 \\ 1}}
         \qquad
         D=\stdbasis_2=\sequence{\colvec[r]{1 \\ 0},\colvec[r]{0 \\ 1}}
       \end{equation*}
       (this $B$ comes from the map description).
       To find the matrix $\hat{T}=\rep{t}{B,B}$, solve the relations 
       \begin{equation*}
          c_1\colvec[r]{1 \\ 2}+c_2\colvec[r]{-1 \\ 1}=\colvec[r]{3 \\ 0}
          \qquad
         \hat{c}_1\colvec[r]{1 \\ 2}+\hat{c}_2\colvec[r]{-1 \\ 1}=\colvec[r]{-1 \\ 2}
       \end{equation*}
       to get \( c_1=1 \), \( c_2=-2 \), \( \hat{c}_1=1/3 \) and
       \( \hat{c}_2=4/3 \).
       \begin{equation*}
          \rep{t}{B,B}=
          \begin{mat}[r]
             1  &1/3 \\
            -2  &4/3
          \end{mat}
       \end{equation*}

       Finding \( \rep{t}{D,D} \) involves a bit more computation.
       We first find \( t(\vec{e}_1) \).
       The relation
       \begin{equation*}
         c_1\colvec[r]{1 \\ 2}+c_2\colvec[r]{-1 \\ 1}=\colvec[r]{1 \\ 0}
       \end{equation*}
       gives \( c_1=1/3 \) and \( c_2=-2/3 \), and so 
       \begin{equation*}
         \rep{\vec{e}_1}{B}=\colvec[r]{1/3 \\ -2/3}_B
       \end{equation*}
       making
       \begin{equation*}
          \rep{t(\vec{e}_1)}{B}=
                 \begin{mat}[r]
                    1  &1/3  \\
                   -2  &4/3
                 \end{mat}_{B,B}
                 \colvec[r]{1/3 \\ -2/3}_B
                 =
                 \colvec[r]{1/9 \\ -14/9}_B
       \end{equation*}
       and hence $t$ acts on the first basis vector $\vec{e}_1$ in this way.
       \begin{equation*}
         t(\vec{e}_1)
         =(1/9)\cdot\colvec[r]{1 \\ 2}-(14/9)\cdot\colvec[r]{-1 \\ 1}
         =\colvec[r]{5/3 \\ -4/3}      
       \end{equation*}
       The computation for \( t(\vec{e}_2) \) is similar.
       The relation
       \begin{equation*}
         c_1\colvec[r]{1 \\ 2}+c_2\colvec[r]{-1 \\ 1}=\colvec[r]{0 \\ 1}
       \end{equation*}
       gives \( c_1=1/3 \) and \( c_2=1/3 \), so
       \begin{equation*}
         \rep{\vec{e}_1}{B}=\colvec[r]{1/3 \\ 1/3}_B      
       \end{equation*}
       making
       \begin{equation*}
         \rep{t(\vec{e}_1)}{B}=
                 \begin{mat}[r]
                    1  &1/3  \\
                   -2  &4/3
                 \end{mat}_{B,B}
                 \colvec[r]{1/3 \\ 1/3}_B
                 =
                 \colvec[r]{4/9 \\ -2/9}_B
       \end{equation*}
       and hence $t$ acts on the second basis vector $\vec{e}_2$ in this way.
       \begin{equation*}
         t(\vec{e}_2)
         =(4/9)\cdot\colvec[r]{1 \\ 2}-(2/9)\cdot\colvec[r]{-1 \\ 1}
         =\colvec[r]{2/3 \\ 2/3}
       \end{equation*}
       Therefore
       \begin{equation*}
          \rep{t}{D,D}=
          \begin{mat}[r]
             5/3  &2/3  \\
            -4/3  &2/3
          \end{mat}
       \end{equation*}
       and so this matrix.
       \begin{equation*}
         P=\rep{\identity}{B,D}
         =\begin{mat}[r]
           1  &-1  \\
           2  &1 
         \end{mat}
       \end{equation*}
      and this one change the bases. 
      \begin{equation*}
         P^{-1}=\bigl(\rep{\identity}{B,D}\bigr)^{-1}
         =\begin{mat}[r]
            1  &-1 \\
            2  &1
         \end{mat}^{-1}
         =
         \begin{mat}[r]
           1/3  &1/3  \\
           -2/3 &1/3
         \end{mat}
      \end{equation*}
      The check of these computations is routine.
      \begin{equation*}
         \begin{mat}[r]
            1  &-1 \\
            2  &1
         \end{mat}
         \begin{mat}[r]
            1  &1/3 \\
           -2  &4/3
         \end{mat}
         \begin{mat}[r]
           1/3 &1/3 \\
          -2/3 &1/3
         \end{mat}
         =
         \begin{mat}[r]
           5/3 &2/3 \\
          -4/3 &2/3
         \end{mat}
      \end{equation*}  
    \end{answer}
  \recommended \item 
    Show that these matrices are not similar.
    \begin{equation*}
       \begin{mat}[r]
          1  &0  &4  \\
          1  &1  &3  \\
          2  &1  &7
       \end{mat}
       \qquad
       \begin{mat}[r]
          1  &0  &1  \\
          0  &1  &1  \\
          3  &1  &2
       \end{mat}
    \end{equation*}
    \begin{answer}
      Gauss's Method shows that
      the first matrix represents maps of rank two while the second
      matrix represents maps of rank three.
    \end{answer}
  \item 
    Explain \nearbyexample{ex:OnlyZeroSimToZero} in terms of maps.
    \begin{answer}
      The only representation of a zero map is a zero matrix,
      no matter what the pair of bases $\rep{z}{B,D}=Z$,
      and so in particular for any single basis $B$ we have $\rep{z}{B,B}=Z$.  
      The case of the identity is slightly different:~the 
      only representation of the identity map, with respect to any $B,B$, 
      is the identity $\rep{\identity}{B,B}=I$.
      (\textit{Remark:}~of course, we have seen examples where $B\neq D$ and 
      $\rep{\identity}{B,D}\neq I$\Dash in fact, we have seen that any 
      nonsingular matrix is a representation of the identity map with
      respect to some $B,D$.)
    \end{answer}
  \recommended \item 
    \cite{Halmos}
    Are there two matrices \( A \) and \( B \) that are
    similar while \( A^2 \) and \( B^2 \) are not similar?
    \begin{answer}
      No.
      If \( A=PBP^{-1} \) then \( A^2=(PBP^{-1})(PBP^{-1})=PB^2P^{-1} \).
    \end{answer}
  \recommended \item
    Prove that if two matrices are similar and one is invertible then
    so is the other.
    \begin{answer}
       Matrix similarity is a special case of matrix equivalence 
       (if matrices are similar then they are matrix equivalent)
       and matrix equivalence preserves nonsingularity.
    \end{answer}
  \recommended \item \label{exer:SimIsEquivRel}
    Show that similarity is an equivalence relation.
    (The definition given earlier already reflects this, so 
    instead start here with the definition that $\hat{T}$ is similar to
    $T$ if $\hat{T}=PTP^{-1}$.)
    \begin{answer}
       A matrix is similar to itself; take \( P \) to be the identity
       matrix:~$P=IPI^{-1}=IPI$.

       If \( \hat{T} \) is similar to \( T \) then \( \hat{T}=PTP^{-1} \)
       and so \( P^{-1}\hat{T}P=T \).
       Rewrite \( T=(P^{-1})\hat{T}(P^{-1})^{-1} \) to conclude that
       $T$ is similar to \( \hat{T} \).

       For transitivity,
       if \( T \) is similar to \( S \) and \( S \) is similar to \( U \)
       then \( T=PSP^{-1} \) and \( S=QUQ^{-1} \).
       Then \( T=PQUQ^{-1}P^{-1}=(PQ)U(PQ)^{-1} \), showing that \( T \)
       is similar to \( U \).  
     \end{answer}
  \item 
     Consider a 
     matrix representing, with respect to some $B,B$, 
     reflection across the \( x \)-axis in \( \Re^2 \).
     Consider also  
     a matrix representing, with respect to some $D,D$,
     reflection across the \( y \)-axis.
     Must they be similar?
     \begin{answer}
        Let $f_x$ and $f_y$ be the reflection maps (sometimes called `flip's).
        For any bases
        \( B \) and \( D \), the matrices \( \rep{f_x}{B,B}  \) and
        \( \rep{f_y}{D,D} \) are similar.
        First note that
        \begin{equation*}
           S=\rep{f_x}{\stdbasis_2,\stdbasis_2}=
           \begin{mat}[r]
              1  &0  \\
              0  &-1
           \end{mat}
           \qquad
           T=\rep{f_y}{\stdbasis_2,\stdbasis_2}=
           \begin{mat}[r]
             -1  &0  \\
              0  &1
           \end{mat}
        \end{equation*}
        are similar because the second matrix is the representation of $f_x$
        with respect to the basis \( A=\sequence{\vec{e}_2,\vec{e}_1} \):
        \begin{equation*}
           \begin{mat}[r]
              1  &0  \\
              0  &-1
           \end{mat}
           =    
           P
           \begin{mat}[r]
             -1  &0  \\
              0  &1
           \end{mat}
           P^{-1}
        \end{equation*}
        where $P=\rep{\identity}{A,\stdbasis_2}$.
        \begin{equation*}
          \begin{CD}
            \Re^2_{\wrt{A}}                   
               @>f_x>T>        
               V\Re^2_{\wrt{A}}       \\
            @V\scriptstyle\identity VPV                
               @V\scriptstyle\identity VPV \\
            \Re^2_{\wrt{\stdbasis_2}}         
               @>f_x>S>        
               \Re^2_{\wrt{\stdbasis_2}}
          \end{CD}
        \end{equation*}
        Now the conclusion follows from the transitivity part of
        \nearbyexercise{exer:SimIsEquivRel}.
        
        We can also finish without relying on that exercise.
        Write
        $\rep{f_x}{B,B}=QTQ^{-1}=Q\rep{f_x}{\stdbasis_2,\stdbasis_2}Q^{-1}$
        and
        $\rep{f_y}{D,D}=RSR^{-1}=R\rep{f_y}{\stdbasis_2,\stdbasis_2}R^{-1}$.
        By the equation in the first paragraph, 
        the first of these two is
        $\rep{f_x}{B,B}=QP\rep{f_y}{\stdbasis_2,\stdbasis_2}P^{-1}Q^{-1}$.
        Rewriting the second of these two as
        $R^{-1}\cdot\rep{f_y}{D,D}\cdot R=\rep{f_y}{\stdbasis_2,\stdbasis_2}$
        and substituting gives the desired relationship
        \begin{multline*}
          \rep{f_x}{B,B}
          =QP\rep{f_y}{\stdbasis_2,\stdbasis_2}P^{-1}Q^{-1}  \\
          =QPR^{-1}\cdot\rep{f_y}{D,D}\cdot RP^{-1}Q^{-1}
          =(QPR^{-1})\cdot\rep{f_y}{D,D}\cdot (QPR^{-1})^{-1}
        \end{multline*}
        Thus the matrices \( \rep{f_x}{B,B}  \) and \( \rep{f_y}{D,D} \) are
        similar. 
      \end{answer}
  \item 
     Prove that similarity preserves determinants and rank.
     Does the converse hold?
     \begin{answer}
       We must show that if two matrices are similar then they have the same
       determinant and the same rank.
       Both determinant and rank are properties of matrices that 
       are preserved by matrix equivalence. 
       They are therefore preserved by similarity (which is a 
       special case of matrix equivalence:~if two matrices
       are similar then they are matrix equivalent).

       To prove the statement without quoting the results about
       matrix equivalence, note first that
       rank is a property of the map (it is the dimension of the range space)
       and since we've shown that 
       the rank of a map is the rank of a representation,
       it must be the same for all representations.
       As for determinants,
       \( \deter{PSP^{-1}}=\deter{P}\cdot\deter{S}\cdot\deter{P^{-1}}
          =\deter{P}\cdot\deter{S}\cdot\deter{P}^{-1}=\deter{S} \).

       The converse of the statement does not hold; 
       for instance,
       there are matrices with the same determinant that are not similar.
       To check this, consider a nonzero matrix with a
       determinant of zero.
       It is not similar to the zero matrix, the zero matrix is similar
       only to itself, but they have they same determinant. 
       The argument for rank is much the same. 
     \end{answer}
  \item 
    Is there a matrix equivalence class with only one matrix similarity
    class inside?
    One with infinitely many similarity classes?
    \begin{answer}
      The matrix equivalence class containing all \( \nbyn{n} \) rank
      zero matrices contains only a single matrix, the zero matrix.
      Therefore it has as a subset only one similarity class.

      In contrast, the matrix equivalence class of \( \nbyn{1} \) matrices
      of rank one consists of those 
      $\nbyn{1}$ matrices \( (k) \) where \( k\neq 0 \).
      For any basis \( B \), the representation
      of multiplication by the scalar \( k \)
      is \( \rep{t_k}{B,B}=(k) \),
      so each such matrix is alone in its similarity class.
      So this is a case where a matrix equivalence class splits into
      infinitely many similarity classes.  
     \end{answer}
  \item 
    Can two different diagonal matrices be in the same similarity class?
    \begin{answer}
      Yes, these are similar
      \begin{equation*}
         \begin{mat}[r]
           1  &0  \\
           0  &3
         \end{mat}
         \qquad
         \begin{mat}[r]
           3  &0  \\
           0  &1
         \end{mat}
      \end{equation*}
      since, where the first matrix is $\rep{t}{B,B}$ for 
      $B=\sequence{\vec{\beta}_1,\vec{\beta}_2}$, 
      the second matrix is $\rep{t}{D,D}$ for 
      $D=\sequence{\vec{\beta}_2,\vec{\beta}_1}$.
     \end{answer}
  \recommended \item
    Prove that if two matrices are similar then their \( k \)-th powers
    are similar when \( k>0 \).
    What if \( k\leq 0 \)?
    \begin{answer}
      The \( k \)-th powers are similar because, where each matrix represents
      the map $t$, the $k$-th powers represent
      \( t^k \), the composition of $k$-many $t$'s.
      (For instance, if $T=rep{t}{B,B}$ then $T^2=\rep{\composed{t}{t}}{B,B}$.)

      Restated more computationally, if \( T=PSP^{-1} \) then
      \( T^2=(PSP^{-1})(PSP^{-1})=PS^2P^{-1} \).
      Induction extends that to all powers.

      For the $k\leq 0$ case, 
      suppose that \( S \) is invertible and that \( T=PSP^{-1} \).
      Note that \( T \) is invertible:
      \( T^{-1}=(PSP^{-1})^{-1}=PS^{-1}P^{-1} \),
      and that same equation shows that 
      \( T^{-1} \) is similar to \( S^{-1} \).
      Other negative powers are now given by the first paragraph.  
    \end{answer}
  \recommended \item
    Let \( p(x) \) be the polynomial \( c_nx^n+\cdots+c_1x+c_0 \).
    Show that if \( T \) is similar to \( S \) then
    \( p(T)=c_nT^n+\cdots+c_1T+c_0I \) is similar to
    \( p(S)=c_nS^n+\cdots+c_1S+c_0I \).
    \begin{answer}
       In conceptual terms, both represent \( p(t) \) for some
       transformation \( t \).
       In computational terms, we have this.
       \begin{align*}
          p(T)
          &=c_n(PSP^{-1})^n+\dots+c_1(PSP^{-1})+c_0I   \\
          &=c_nPS^nP^{-1}+\dots+c_1PSP^{-1}+c_0I   \\
          &=Pc_nS^nP^{-1}+\dots+Pc_1SP^{-1}+Pc_0P^{-1}   \\
          &=P(c_nS^n+\dots+c_1S+c_0)P^{-1}
       \end{align*}  
     \end{answer}
  \item 
    List all of the matrix equivalence classes of \( \nbyn{1} \) matrices.
    Also list the similarity classes, and describe which similarity classes are
    contained inside of each matrix equivalence class.
    \begin{answer}
      There are two equivalence classes, (i)~the class of rank~zero matrices, 
      of which there is one:
      $\mathscr{C}_1=\set{(0)}$,
      and (2)~the class of rank~one matrices,
      of which there are infinitely many: 
      \( \mathscr{C}_2=\set{(k)\suchthat k\neq0} \).

      Each \( \nbyn{1} \) matrix is alone in its similarity class.
      That's because any transformation of a one-dimensional space
      is multiplication by a scalar $\map{t_k}{V}{V}$ given by
      $\vec{v}\mapsto k\cdot\vec{v}$. 
      Thus, for any basis \( B=\sequence{\vec{\beta}} \),
      the matrix representing a transformation \( t_k \) 
      with respect to \( B,B \) is
      \( (\rep{t_k(\vec{\beta})}{B})=(k) \). 
      
      So, contained in the matrix equivalence class
      $\mathscr{C}_1$ is (obviously) the single 
      similarity class consisting of the matrix $(0)$.
      And, contained in the matrix equivalence class $\mathscr{C}_2$ are the
      infinitely many, one-member-each, similarity classes consisting of
      $(k)$ for $k\neq0$.  
    \end{answer}
  \item 
    Does similarity preserve sums?
    \begin{answer}
      No.
      Here is an example that has two pairs, each of two similar matrices:
      \begin{equation*}
         \begin{mat}[r]
            1  &-1  \\
            1  &2
         \end{mat}
         \begin{mat}[r]
            1  &0   \\
            0  &3
         \end{mat}
         \begin{mat}[r]
            2/3  &1/3   \\
           -1/3  &1/3
         \end{mat}
         =
         \begin{mat}[r]
            5/3  &-2/3  \\
           -4/3  &7/3
         \end{mat}
      \end{equation*}
      and
      \begin{equation*}
         \begin{mat}[r]
            1  &-2  \\
           -1  &1
         \end{mat}
         \begin{mat}[r]
           -1  &0   \\
            0  &-3
         \end{mat}
         \begin{mat}[r]
            -1  &-2   \\
            -1  &-1
         \end{mat}
         =
         \begin{mat}[r]
            -5  &-4   \\
             2  &1
         \end{mat}
      \end{equation*}
      (this example is not entirely arbitrary because
      the center matrices on the two left sides add to the zero matrix).
      Note that the sums of these similar matrices are not similar
      \begin{equation*}
         \begin{mat}[r]
            1  &0   \\
            0  &3
         \end{mat}
         +
         \begin{mat}[r]
           -1  &0   \\
            0  &-3
         \end{mat}
         =
         \begin{mat}[r]
           0  &0  \\
           0  &0
         \end{mat}
         \qquad
         \begin{mat}[r]
            5/3  &-2/3   \\
            -4/3 &7/3
         \end{mat}
         +
         \begin{mat}[r]
            -5  &-4   \\
             2  &1
         \end{mat}
         \neq
         \begin{mat}[r]
           0  &0  \\
           0  &0
         \end{mat}
      \end{equation*}
      since the zero matrix is similar only to itself.
    \end{answer}
  \item 
    Show that if \( T-\lambda I \) and \( N \) are similar matrices then
    \( T \) and \( N+\lambda I \) are also similar.
    \begin{answer}
    If \( N=P(T-\lambda I)P^{-1} \) then
    \( N=PTP^{-1}-P(\lambda I)P^{-1} \).
    The diagonal matrix \( \lambda I \) commutes with anything, so
    \( P(\lambda I)P^{-1}=PP^{-1}(\lambda I)=\lambda I \).
    Thus \( N=PTP^{-1}-\lambda I \) and
    consequently \( N+\lambda I=PTP^{-1} \).
    (So not only are they similar, in fact they are similar via 
    the same \( P \).)
    \end{answer}
\end{exercises}

















\subsection{Diagonalizability}
The prior subsection shows that
although similar matrices are necessarily matrix equivalent, the converse
does not hold.
Some matrix equivalence classes break into two or more similarity 
classes; for instance, the nonsingular $\nbyn{2}$ matrices
form one matrix equivalence class but more than one similarity class.

Thus we cannot use the canonical form for matrix equivalence, 
a block partial-identity matrix, as a canonical form 
for matrix similarity. 
The diagram below illustrates. 
The stars are similarity class representatives.
Each dashed-line similarity class subdivision has one star 
but each solid-curve matrix equivalence class division has only one 
partial identity matrix.
\begin{center}
  \includegraphics{ch5.5}
\end{center}
To develop a canonical form for representatives of
the similarity classes
we naturally build on previous work.
This means 
first that the partial identity matrices should represent the similarity
classes into which they fall. 
Beyond that, the representatives should be as simple as possible.
%(the partial identities are simple in that they consist mostly of zeros). 
The simplest extension of the partial identity form is the diagonal form.

\begin{definition} \label{df:Diagonalizable}
%<*df:Diagonalizable>
A transformation is \definend{diagonalizable}\index{diagonalizable|(}%
\index{transformation!diagonalizable}
if it has a diagonal representation
with respect to the same basis for the codomain as for the domain.
A \definend{diagonalizable matrix}\index{matrix!diagonalizable}
is one that is similar to a diagonal matrix:~\( T \) is diagonalizable
if there is a nonsingular \( P \) such that \( PTP^{-1} \) is diagonal.
%</df:Diagonalizable>
\end{definition}

\begin{example} \label{ex:DiagTwoByTwo}
The matrix 
\begin{equation*}
  \begin{mat}[r] 
     4 &-2 \\ 
     1 &1 
  \end{mat}
\end{equation*} 
is diagonalizable.
\begin{equation*}
  \begin{mat}[r]
     2  &0   \\
     0  &3
  \end{mat}
  =
  \begin{mat}[r]
    -1  &2  \\
     1  &-1
  \end{mat}
  \begin{mat}[r]
     4  &-2 \\
     1  &1
  \end{mat}
  \begin{mat}[r]
    -1  &2  \\
     1  &-1
  \end{mat}^{-1}
\end{equation*}
\end{example}

\begin{example}
%<*ex:NotDiagonalizable>
We will show that this matrix is not diagonalizable.
\begin{equation*}
  N=\begin{mat}[r]
       0  &0  \\
       1  &0
    \end{mat}
\end{equation*}
The fact that $N$ is not the zero matrix means that it cannot be similar to
the zero matrix, because the zero matrix is similar only to itself.
Thus if $N$ were to be similar to a diagonal matrix then that 
matrix would have have at least one nonzero entry on its diagonal.

The square of $N$ is the zero matrix.
This imples that for any map~$n$ represented by \( N \) 
(with respect to some $B,B$) 
the composition \( \composed{n}{n} \) is the zero map.
This in turn implies that
for any matrix representing~$n$ 
(with respect to some $\hat{B},\hat{B}$), its square is the zero matrix.
But the square of a nonzero diagonal matrix 
cannot be the zero matrix, because the square of a diagonal
matrix is the diagonal matrix whose entries are the squares of the entries 
from the starting matrix. 
Thus there is no $\hat{B},\hat{B}$ such that $n$ is represented by 
a diagonal matrix\Dash 
the matrix $N$ is not diagonalizable.
%</ex:NotDiagonalizable>
\end{example}

That example shows that a diagonal form will not suffice as a 
canonical form for similarity\Dash we cannot
find a diagonal matrix in each matrix similarity class.
However, some similarity classes contain a diagonal matrix and 
the canonical form that we are developing has the property that if
a matrix can be diagonalized then the diagonal matrix is the canonical
representative of its similarity class. 
% We now characterize which maps can be diagonalized.

\begin{lemma} \label{lm:DiagIffBasisOfEigens}
%<*lm:DiagIffBasisOfEigens>
A transformation \( t \) is diagonalizable if and only if
there is a basis
\( B=\sequence{\vec{\beta}_1,\ldots,\vec{\beta}_n } \)
and scalars \( \lambda_1,\ldots,\lambda_n \) such that
\( t(\vec{\beta}_i)=\lambda_i\vec{\beta}_i \)
for each \( i \).
%</lm:DiagIffBasisOfEigens>
\end{lemma}

\begin{proof}
%<*pf:DiagIffBasisOfEigens>
Consider a diagonal representation matrix.
\begin{equation*}
   \rep{t}{B,B}=
   \begin{pmat}{c@{\hspace*{1em}}c@{\hspace*{1em}}c}
      \vdots                    &       &\vdots                     \\
      \rep{t(\vec{\beta}_1)}{B} &\cdots &\rep{t(\vec{\beta}_n)}{B}  \\
      \vdots                    &       &\vdots
   \end{pmat}
   =
   \begin{pmat}{c@{\hspace*{1em}}c@{\hspace*{1em}}c}
      \lambda_1   &       &0         \\
      \vdots      &\ddots &\vdots    \\
      0           &       &\lambda_n
   \end{pmat}
\end{equation*}
Consider the representation of a member of this basis 
with respect to the basis $\rep{\vec{\beta}_i}{B}$.
The product of the diagonal matrix and the representation vector
\begin{equation*}
   \rep{t(\vec{\beta}_i)}{B}
   =\begin{pmat}{c@{\hspace*{1em}}c@{\hspace*{1em}}c}
      \lambda_1   &       &0         \\
      \vdots      &\ddots &\vdots    \\
      0           &       &\lambda_n
   \end{pmat}  
  \colvec{0 \\ \vdots \\ 1 \\ \vdots \\ 0}
  =\colvec{0 \\ \vdots \\ \lambda_i \\ \vdots \\ 0}
\end{equation*}
has the 
stated action.
%</pf:DiagIffBasisOfEigens>
\end{proof}

\begin{example}     \label{ex:DiagUpperTrian}
To diagonalize
\begin{equation*}
   T=\begin{mat}[r]
      3  &2  \\
      0  &1
   \end{mat}
\end{equation*}
we take $T$ as the representation of a transformation with respect to the
standard basis $\rep{t}{\stdbasis_2,\stdbasis_2}$ and look for a basis
\( B=\sequence{\vec{\beta}_1,\vec{\beta}_2} \) such that
\begin{equation*}
  \rep{t}{B,B}
  =
  \begin{mat}
    \lambda_1  &0          \\
    0          &\lambda_2
  \end{mat}
\end{equation*}
that is, such that 
$t(\vec{\beta}_1)=\lambda_1\vec{\beta}_1$ 
and $t(\vec{\beta}_2)=\lambda_2\vec{\beta}_2$.
\begin{equation*}
  \begin{mat}[r]
     3  &2  \\
     0  &1
  \end{mat}
  \vec{\beta}_1=\lambda_1\cdot\vec{\beta}_1
  \qquad
  \begin{mat}[r]
     3  &2  \\
     0  &1
  \end{mat}
  \vec{\beta}_2=\lambda_2\cdot\vec{\beta}_2
\end{equation*} 
We are looking for scalars \( x \) such that this equation
\begin{equation*}
 \begin{mat}[r]
    3  &2  \\
    0  &1
 \end{mat}
 \colvec{b_1 \\ b_2}=x\cdot\colvec{b_1 \\ b_2}
\end{equation*}
has solutions $b_1$ and $b_2$ that are not both $0$
(the zero vector is not the member of any basis).
That's a linear system.
\begin{equation*}
  \begin{linsys}{2}
     (3-x)\cdot b_1  &+  &2\cdot b_2       &=  &0  \\
                     &   &(1-x)\cdot b_2   &=  &0 
  \end{linsys}
\tag*{($*$)}\end{equation*}
Focus first on the bottom equation.
There are two cases: either
$b_2=0$ or~$x=1$. 

In the \( b_2=0 \) case 
the first equation gives that either $b_1=0$ or \( x=3 \).
Since we've disallowed the possibility that both $b_2=0$ and~$b_1=0$,
we are left with the first diagonal entry $\lambda_1=3$. 
With that, ($*$)'s first equation is $0\cdot b_1+2\cdot b_2=0$
and so associated with $\lambda_1=3$ are vectors
having a second component of zero while the first component is free. 
\begin{equation*}
     \begin{mat}[r]
        3  &2  \\
        0  &1
     \end{mat}
     \colvec{b_1 \\ 0}=3\cdot\colvec{b_1 \\ 0} 
\end{equation*}
To get a first basis vector 
choose any nonzero $b_1$.
\begin{equation*}
   \vec{\beta}_1=\colvec[r]{1 \\ 0}
\end{equation*}

The other case for the bottom equation of ($*$) is $\lambda_2=1$. 
Then ($*$)'s first equation is $2\cdot b_1+2\cdot b_2=0$ and so
associated with this case are vectors whose
second component is the negative of the first.
\begin{equation*}
     \begin{mat}[r]
        3  &2  \\
        0  &1
     \end{mat}
     \colvec{b_1 \\ -b_1}=1\cdot\colvec{b_1 \\ -b_1} 
\end{equation*}
Get the second basis vector by 
choosing a nonzero one of these.
\begin{equation*}
   \vec{\beta}_2=\colvec[r]{1 \\ -1}
\end{equation*}

Now draw the similarity diagram        
\begin{equation*}
     \begin{CD}
            \Re^2_{\wrt{\stdbasis_2}}                   
               @>t>T>        
               \Re^2_{\wrt{\stdbasis_2}}       \\
            @V{\scriptstyle\identity} VV                
               @V{\scriptstyle\identity} VV \\
            \Re^2_{\wrt{B}}         
               @>t>D>        
               \Re^2_{\wrt{B}}
      \end{CD}
\end{equation*}
and note that the matrix $\rep{\identity}{B,\stdbasis_2}$ is easy,
giving this diagonalization.
\begin{equation*}
   \begin{mat}[r]
     3  &0  \\
     0  &1
   \end{mat}
   =
   \begin{mat}[r]
     1  &1  \\
     0  &-1
   \end{mat}^{-1}
   \begin{mat}[r]
     3  &2  \\
     0  &1
   \end{mat}
   \begin{mat}[r]
     1  &1  \\
     0  &-1
   \end{mat}
\end{equation*}
\end{example}

In the next subsection we will expand on that example by considering 
more closely the property of \nearbylemma{lm:DiagIffBasisOfEigens}.
This includes seeing a 
streamlined way to find the $\lambda$'s.


\begin{exercises}
  \recommended \item 
    Repeat \nearbyexample{ex:DiagUpperTrian} for the matrix from 
    \nearbyexample{ex:DiagTwoByTwo}.
    \begin{answer}
      Because we chose the basis vectors arbitrarily, many different answers
      are possible.
      However, here is one way to go;
      to diagonalize
      \begin{equation*}
         T=\begin{mat}[r]
            4  &-2  \\
            1  &1
         \end{mat}
      \end{equation*}
      take it as the representation of a transformation with respect to the
      standard basis $T=\rep{t}{\stdbasis_2,\stdbasis_2}$ and look for
      \( B=\sequence{\vec{\beta}_1,\vec{\beta}_2} \) such that
      \begin{equation*}
        \rep{t}{B,B}
        =
        \begin{mat}
          \lambda_1  &0          \\
          0          &\lambda_2
        \end{mat}
      \end{equation*}
      that is, such that 
      $t(\vec{\beta}_1)=\lambda_1$ and $t(\vec{\beta}_2)=\lambda_2$.
      \begin{equation*}
        \begin{mat}[r]
           4  &-2  \\
           1  &1
        \end{mat}
        \vec{\beta}_1=\lambda_1\cdot\vec{\beta}_1
        \qquad
        \begin{mat}[r]
           4  &-2  \\
           1  &1
        \end{mat}
        \vec{\beta}_2=\lambda_2\cdot\vec{\beta}_2
      \end{equation*} 
      We are looking for scalars \( x \) such that this equation
      \begin{equation*}
       \begin{mat}[r]
          4  &-2  \\
          1  &1
       \end{mat}
       \colvec{b_1 \\ b_2}=x\cdot\colvec{b_1 \\ b_2}
      \end{equation*}
      has solutions $b_1$ and $b_2$, which are not both zero.
      Rewrite that as a linear system
      \begin{equation*}
        \begin{linsys}{2}
           (4-x)\cdot b_1  &+  &-2\cdot b_2       &=  &0  \\
           1\cdot b_1      &+   &(1-x)\cdot b_2   &=  &0 
        \end{linsys}
      \end{equation*}
      If $x=4$ then the first equation gives that $b_2=0$, and then
      the second equation gives that $b_1=0$.
      We have disallowed the case where both $b$'s are zero
      so we can assume that $x\neq 4$.
      \begin{equation*}
        \grstep{(-1/(4-x))\rho_1+\rho_2}
        \begin{linsys}{2}
           (4-x)\cdot b_1  &+   &-2\cdot b_2                   &=  &0  \\
                           &    &((x^2-5x+6)/(4-x))\cdot b_2   &=  &0 
        \end{linsys} 
      \end{equation*}
      Consider the bottom equation.
      If \( b_2=0 \) then the first equation gives $b_1=0$ or $x=4$.
      The $b_1=b_2=0$ case is not allowed.
      The other possibility for the bottom equation is that the numerator 
      of the fraction $x^2-5x+6=(x-2)(x-3)$ is zero.
      The $x=2$ case gives a first equation of $2b_1-2b_2=0$, and so 
      associated with $x=2$ we have
      vectors whose first and second components are equal:
      \begin{equation*}
         \vec{\beta}_1=\colvec[r]{1 \\ 1}
         \qquad\text{(so \(
           \begin{mat}[r]
              4  &-2  \\
              1  &1
           \end{mat}
           \colvec[r]{1 \\ 1}=2\cdot\colvec[r]{1 \\ 1} \), and $\lambda_1=2$).}
      \end{equation*}
      If \( x=3 \) then the first equation is
      $b_1-2b_2=0$ and so the associated vectors 
      are those whose first component is 
      twice their second:
      \begin{equation*}
         \vec{\beta}_2=\colvec[r]{2 \\ 1}
         \qquad\text{(so \(
           \begin{mat}[r]
              4  &-2  \\
              1  &1
           \end{mat}
           \colvec[r]{2 \\ 1}=3\cdot\colvec[r]{2 \\ 1} \), and so $\lambda_2=3$).}
      \end{equation*}
      This picture 
        \begin{equation*}
          \begin{CD}
            \Re^2_{\wrt{\stdbasis_2}}                   
               @>t>T>        
               \Re^2_{\wrt{\stdbasis_2}}       \\
            @V{\scriptstyle\identity} VV                
               @V{\scriptstyle\identity} VV \\
            \Re^2_{\wrt{B}}         
               @>t>D>        
               \Re^2_{\wrt{B}}
          \end{CD}
        \end{equation*}
      shows how to get the diagonalization.
      \begin{equation*}
         \begin{mat}[r]
           2  &0  \\
           0  &3
         \end{mat}
         =
         \begin{mat}[r]
           1  &2  \\
           1  &1
         \end{mat}^{-1}
         \begin{mat}[r]
           4  &-2  \\
           1  &1
         \end{mat}
         \begin{mat}[r]
           1  &2  \\
           1  &1
         \end{mat}
      \end{equation*}
      \textit{Comment}.
      This equation matches the $T=PSP^{-1}$ definition under this renaming.
      \begin{equation*}
        T=
         \begin{mat}[r]
           2  &0  \\
           0  &3
         \end{mat}
         \quad
         P=
         \begin{mat}[r]
           1  &2  \\
           1  &1
         \end{mat}^{-1}
         \quad
         P^{-1}=
         \begin{mat}[r]
           1  &2  \\
           1  &1
         \end{mat}
         \quad
         S=
         \begin{mat}[r]
           4  &-2  \\
           1  &1
         \end{mat}
      \end{equation*}
    \end{answer}
  \item 
    Diagonalize these upper triangular matrices.
    \begin{exparts*}
      \partsitem 
        $\begin{mat}[r]
          -2  &1  \\
           0  &2
        \end{mat}$
      \partsitem 
        $\begin{mat}[r]
          5  &4  \\
          0  &1
        \end{mat}$
    \end{exparts*}
    \begin{answer}
      \begin{exparts}
        \partsitem
          Setting up
          \begin{equation*}
            \begin{mat}[r]
              -2  &1  \\
               0  &2
            \end{mat}
            \colvec{b_1 \\ b_2}
            =x\cdot\colvec{b_1 \\ b_2}
            \qquad\Longrightarrow\qquad 
            \begin{linsys}{2}
               (-2-x)\cdot b_1  &+  &b_2            &=  &0  \\
                                &   &(2-x)\cdot b_2 &= &0
            \end{linsys}
          \end{equation*}
          gives the two possibilities that $b_2=0$ and $x=2$.
          Following the $b_2=0$ possibility leads to the first equation
          $(-2-x)b_1=0$ with the two cases that $b_1=0$ and that
          $x=-2$.
          Thus, under this first possibility, we find $x=-2$ and the 
          associated vectors whose second component is zero, and whose
          first component is free.  
          \begin{equation*}
            \begin{mat}[r]
              -2  &1  \\
               0  &2
            \end{mat}
            \colvec{b_1 \\ 0}
            =-2\cdot\colvec{b_1 \\ 0}
            \qquad
            \vec{\beta}_1=\colvec[r]{1 \\ 0}
          \end{equation*}
          Following the other possibility leads to a first equation of
          $-4b_1+b_2=0$ and so the vectors associated with this 
          solution have a second component that is four times their first
          component.
          \begin{equation*}
            \begin{mat}[r]
              -2  &1  \\
               0  &2
            \end{mat}
            \colvec{b_1 \\ 4b_1}
            =2\cdot\colvec{b_1 \\ 4b_1}
            \qquad
            \vec{\beta}_2=\colvec{1 \\ 4}
          \end{equation*}
          The diagonalization is this.
          \begin{equation*}
            \begin{mat}[r]
              1  &1  \\
              0  &4
            \end{mat}
            \begin{mat}[r]
              -2  &1  \\
               0  &2
            \end{mat}
            \begin{mat}[r]
              1  &1  \\
              0  &4
            \end{mat}^{-1}
            =
            \begin{mat}[r]
              -2  &0  \\
               0  &2
            \end{mat}
          \end{equation*}
      \partsitem The calculations are like those in the prior part.
          \begin{equation*}
            \begin{mat}[r]
               5  &4  \\
               0  &1
            \end{mat}
            \colvec{b_1 \\ b_2}
            =x\cdot\colvec{b_1 \\ b_2}
            \qquad\Longrightarrow\qquad 
            \begin{linsys}{2}
               (5-x)\cdot b_1  &+  &4\cdot b_2     &=  &0  \\
                               &   &(1-x)\cdot b_2 &=  &0
            \end{linsys}
          \end{equation*}
          The bottom equation
          gives the two possibilities that $b_2=0$ and $x=1$.
          Following the $b_2=0$ possibility, and discarding the
          case where both $b_2$ and $b_1$ are zero, gives
          that $x=5$, associated with vectors whose second component
          is zero and whose first component is free.
          \begin{equation*}
            \vec{\beta}_1=\colvec[r]{1 \\ 0}
          \end{equation*}
          The $x=1$ possibility gives a first equation of
          $4b_1+4b_2=0$ and so the associated vectors have a 
          second component that is the negative of their first component.
          \begin{equation*}
            \vec{\beta}_1=\colvec[r]{1 \\ -1}
          \end{equation*}
          We thus have this diagonalization.
          \begin{equation*}
            \begin{mat}[r]
              1  &1  \\
              0  &-1
            \end{mat}
            \begin{mat}[r]
               5  &4  \\
               0  &1
            \end{mat}
            \begin{mat}[r]
              1  &1  \\
              0  &-1
            \end{mat}^{-1}
            =
            \begin{mat}[r]
               5  &0  \\
               0  &1
            \end{mat}
          \end{equation*}
      \end{exparts}
    \end{answer}
  \recommended \item \label{exer:PowersOfDiags}
    What form do the powers of a diagonal matrix have?
    \begin{answer}
      For any integer \( p \), we have this.
      \begin{equation*}
        \begin{mat}
          d_1  &0      &   \\
          0    &\ddots &   \\
               &       &d_n
        \end{mat}^p=
        \begin{mat}
          d_1^p  &0      &   \\
          0      &\ddots &   \\
                 &       &d_n^p
        \end{mat}
     \end{equation*} 
    \end{answer}
  \item 
     Give two same-sized diagonal matrices that are not similar.
     Must any two different diagonal matrices come from different similarity
     classes?
     \begin{answer}
       These two are not similar 
       \begin{equation*}
          \begin{mat}[r]
             0  &0  \\
             0  &0
          \end{mat}
          \qquad
          \begin{mat}[r]
             1  &0  \\
             0  &1
          \end{mat}
       \end{equation*}
       because each is alone in its similarity class.

       For the second half, these
       \begin{equation*}
          \begin{mat}[r]
             2  &0  \\
             0  &3
          \end{mat}
          \qquad
          \begin{mat}[r]
             3  &0  \\
             0  &2
          \end{mat}
       \end{equation*}
       are similar via the matrix that changes bases from
       \( \sequence{\vec{\beta}_1,\vec{\beta}_2} \) to
       \( \sequence{\vec{\beta}_2,\vec{\beta}_1} \).
       (\textit{Question.}
        Are two diagonal matrices similar if and only if their diagonal
        entries are permutations of each others?)  
    \end{answer}
  \item 
    Give a nonsingular diagonal matrix.
    Can a diagonal matrix ever be singular?
    \begin{answer}
      Contrast these two.
      \begin{equation*}
         \begin{mat}[r]
           2  &0  \\
           0  &1
         \end{mat}
         \qquad
         \begin{mat}[r]
           2  &0  \\
           0  &0
         \end{mat}
      \end{equation*}
      The first is nonsingular, the second is singular.  
     \end{answer}
  \recommended \item
    Show that the inverse of a diagonal matrix is the diagonal of the
    the inverses, if no element on that diagonal is zero.
    What happens when a diagonal entry is zero?
    \begin{answer}  
       To check that the inverse of a diagonal matrix is the diagonal
       matrix of the inverses, just multiply.
       \begin{equation*}
          \begin{mat}
             a_{1,1}  &0                \\
             0        &a_{2,2}          \\
                      &       &\ddots    \\
                      &       &      &a_{n,n}
          \end{mat}
          \begin{mat}
            1/a_{1,1}  &0                \\
             0        &1/a_{2,2}          \\
                      &       &\ddots    \\
                      &       &      &1/a_{n,n}
          \end{mat}
      \end{equation*}
      (Showing that it is a left inverse is just as easy.)

      If a diagonal entry is zero then the diagonal matrix is
      singular; it has a zero determinant.  
    \end{answer}
  \item 
    The equation ending \nearbyexample{ex:DiagUpperTrian}
    \begin{equation*}
       \begin{mat}[r]
         1  &1  \\
         0  &-1
       \end{mat}^{-1}
       \begin{mat}[r]
         3  &2  \\
         0  &1
       \end{mat}
       \begin{mat}[r]
         1  &1  \\
         0  &-1
       \end{mat}
       =
       \begin{mat}[r]
         3  &0  \\
         0  &1
       \end{mat}
    \end{equation*}
    is a bit jarring because for $P$ we must take the first matrix,
    which is shown as an inverse, and for $P^{-1}$ we take the inverse of the
    first matrix, so that the two $-1$ powers cancel and this matrix is 
    shown without a superscript $-1$.
    \begin{exparts}
      \partsitem Check that this nicer-appearing equation holds.
        \begin{equation*}
           \begin{mat}[r]
             3  &0  \\
             0  &1
           \end{mat}
           =
           \begin{mat}[r]
             1  &1  \\
             0  &-1
           \end{mat}
           \begin{mat}[r]
             3  &2  \\
             0  &1
           \end{mat}
           \begin{mat}[r]
             1  &1  \\
             0  &-1
           \end{mat}^{-1}
        \end{equation*}
      \partsitem Is the previous item a coincidence?
        Or can we always switch the $P$ and the $P^{-1}$?
   \end{exparts}
   \begin{answer}
     \begin{exparts}
       \partsitem The check is easy.
         \begin{equation*}
           \begin{mat}[r]
             1  &1  \\
             0  &-1
           \end{mat}
           \begin{mat}[r]
             3  &2  \\
             0  &1
           \end{mat}
           =
           \begin{mat}[r]
             3  &3  \\
             0  &-1
           \end{mat}
           \qquad
           \begin{mat}[r]
             3  &3  \\
             0  &-1
           \end{mat}
           \begin{mat}[r]
             1  &1  \\
             0  &-1
           \end{mat}^{-1}
           =
           \begin{mat}[r]
             3  &0  \\
             0  &1
           \end{mat}
         \end{equation*}
        \partsitem It is a coincidence, in the sense that if $T=PSP^{-1}$
          then $T$ need not equal $P^{-1}SP$.
          Even in the case of a diagonal matrix~$D$, the condition that
          $D=PTP^{-1}$ does not imply that $D$ equals $P^{-1}TP$.
          The matrices from \nearbyexample{ex:DiagTwoByTwo} show this.
          \begin{equation*}
            \begin{mat}[r]
              1  &2  \\
              1  &1
            \end{mat}
            \begin{mat}[r]
              4  &-2  \\
              1  &1
            \end{mat}
            =
            \begin{mat}[r]
              6  &0  \\
              5  &-1
            \end{mat}
            \qquad
            \begin{mat}[r]
              6  &0  \\
              5  &-1
            \end{mat}
            \begin{mat}[r]
              1  &2  \\
              1  &1
            \end{mat}^{-1}
            =
            \begin{mat}[r]
              -6  &12  \\
              -6  &11
            \end{mat}
          \end{equation*}
     \end{exparts}
   \end{answer}
  \item 
    Show that the $P$ used to diagonalize in  
    \nearbyexample{ex:DiagUpperTrian} is not unique.
    \begin{answer}
      The columns of the matrix are the vectors associated with
      the $x$'s.
      The exact choice, and the order of the choice was
      arbitrary.
      We could, for instance, get a different matrix by swapping 
      the two columns.
    \end{answer}
  \item 
    Find a formula for the powers of this matrix
    \textit{Hint}:~see \nearbyexercise{exer:PowersOfDiags}.
    \begin{equation*}
      \begin{mat}[r]
        -3  &1  \\
        -4  &2
      \end{mat}
    \end{equation*}
    \begin{answer}
      Diagonalizing and then taking powers of the diagonal matrix shows that
      \begin{equation*}
        \begin{mat}[r]
          -3  &1  \\
          -4  &2
        \end{mat}^k
        =
        \frac{1}{3}
        \begin{mat}[r]
          -1  &1  \\
          -4  &4
        \end{mat}
        +(\frac{-2}{3})^k
        \begin{mat}[r]
           4  &-1 \\
           4  &-1
        \end{mat}.
      \end{equation*}  
     \end{answer}
  \recommended \item \label{exer:DiagThese} 
    Diagonalize these.
    \begin{exparts*}
      \partsitem \( \begin{mat}[r]
                  1  &1  \\
                  0  &0
               \end{mat} \)
      \partsitem \( \begin{mat}[r]
                  0  &1  \\
                  1  &0
               \end{mat} \)
    \end{exparts*}
    \begin{answer}
     \begin{exparts}
        \partsitem \( \begin{mat}[r]
                    1  &1  \\
                    0  &-1
                 \end{mat}^{-1}
                 \begin{mat}[r]
                    1  &1  \\
                    0  &0
                 \end{mat}
                 \begin{mat}[r]
                    1  &1  \\
                    0  &-1
                 \end{mat}
                =\begin{mat}[r]
                    1  &0  \\
                    0  &0
                 \end{mat} \)
        \partsitem \( \begin{mat}[r]
                    1  &1  \\
                    0  &-1
                 \end{mat}^{-1}
                 \begin{mat}[r]
                    0  &1  \\
                    1  &0
                 \end{mat}
                 \begin{mat}[r]
                    1  &1  \\
                    0  &-1
                 \end{mat}
                =\begin{mat}[r]
                    1  &0  \\
                    0  &-1
                 \end{mat} \)
      \end{exparts}  
     \end{answer}
  \item 
    We can ask how diagonalization interacts with the matrix operations.
    Assume that \( \map{t,s}{V}{V} \) are each diagonalizable.
    Is \( ct \) diagonalizable for all scalars \( c \)?
    What about \( t+s \)?
    \( \composed{t}{s} \)?
    \begin{answer}
      Yes, \( ct \) is diagonalizable by the final theorem of this
      subsection.

      No, \( t+s \) need not be diagonalizable.
      Intuitively, the problem arises when the two maps diagonalize with
      respect to different bases (that is, when they are not
      \definend{simultaneously diagonalizable}).
      Specifically, these two are diagonalizable but their sum is not:
      \begin{equation*}
         \begin{mat}[r]
            1  &1  \\
            0  &0
         \end{mat}
         \qquad
         \begin{mat}[r]
           -1  &0  \\
            0  &0
         \end{mat}
      \end{equation*}
      (the second is already diagonal; for the first, see 
      \nearbyexercise{exer:DiagThese}).
      The sum is not diagonalizable because its square is the zero matrix. 

     The same intuition suggests that \( \composed{t}{s} \) is not
     be diagonalizable.
     These two are diagonalizable but their product is not:
     \begin{equation*}
        \begin{mat}[r]
           1  &0  \\
           0  &0
        \end{mat}
        \qquad
        \begin{mat}[r]
           0  &1  \\
           1  &0
        \end{mat}
     \end{equation*}
     (for the second, see \nearbyexercise{exer:DiagThese}).   
    \end{answer}
  \recommended \item 
    Show that matrices of this form are not diagonalizable.
    \begin{equation*}
       \begin{mat}[r]
          1  &c  \\
          0  &1
       \end{mat}
       \qquad c\neq 0
    \end{equation*}
    \begin{answer}
      If
      \begin{equation*}
         P
         \begin{mat}
            1  &c  \\
            0  &1
         \end{mat}
         P^{-1}
         =
         \begin{mat}
            a  &0  \\
            0  &b
         \end{mat}
      \end{equation*}
      then
      \begin{equation*}
         P
         \begin{mat}
            1  &c  \\
            0  &1
         \end{mat}
         =
         \begin{mat}
            a  &0  \\
            0  &b
         \end{mat}
         P
      \end{equation*}
      so
      \begin{align*}
         \begin{mat}
            p  &q  \\
            r  &s
         \end{mat}
         \begin{mat}
            1  &c  \\
            0  &1
         \end{mat}
         &=
         \begin{mat}
            a  &0  \\
            0  &b
         \end{mat}
         \begin{mat}
            p  &q  \\
            r  &s
         \end{mat}        \\
         \begin{mat}
            p  &cp+q  \\
            r  &cr+s
         \end{mat}
         &=
         \begin{mat}
            ap  &aq  \\
            br  &bs
         \end{mat}
      \end{align*}
      The \( 1,1 \) entries show that \( a=1 \) and the \( 1,2 \) entries
      then show that \( pc=0 \).
      Since \( c\neq 0 \) this means that \( p=0 \).
      The \( 2,1 \) entries show that 
      \( b=1 \) and the \( 2,2 \) entries then show that
      \( rc=0 \).
      Since \( c\neq 0 \) this means that \( r=0 \).
      But if both \( p \) and \( r \) are \( 0 \) then \( P \) is not
      invertible.  
     \end{answer}
  \item 
    Show that each of these is diagonalizable.
    \begin{exparts*}
      \partsitem
       \( \begin{mat}[r]
             1  &2  \\
             2  &1
          \end{mat}  \)
      \partsitem
       \( \begin{mat}
             x  &y  \\
             y  &z
          \end{mat}
          \qquad \text{$x,y,z$ scalars}  \)
    \end{exparts*}
    \begin{answer}
      \begin{exparts}
      \partsitem Using the formula for the inverse of a $\nbyn{2}$
        matrix gives this.
        \begin{multline*}
           \begin{mat}
              a  &b  \\
              c  &d
           \end{mat}
           \begin{mat}[r]
              1  &2  \\
              2  &1
           \end{mat}
           \cdot\frac{1}{ad-bc}\cdot
           \begin{mat}
              d  &-b \\
             -c  &a
           \end{mat}                                  \\
           =\frac{1}{ad-bc}
           \begin{mat}
              ad+2bd-2ac-bc    &-ab-2b^2+2a^2+ab \\
              cd+2d^2-2c^2-cd  &-bc-2bd+2ac+ad
           \end{mat}
        \end{multline*}
        Now pick scalars \( a,\ldots,d \) so that
        \( ad-bc\neq 0 \) and \( 2d^2-2c^2=0 \) and \( 2a^2-2b^2=0 \).
        For example, these will do.
        \begin{equation*}
          \begin{mat}[r]
            1  &1  \\
            1  &-1
          \end{mat}
          \begin{mat}[r]
            1  &2  \\
            2  &1
          \end{mat}
          \cdot\frac{1}{-2}\cdot
          \begin{mat}[r]
            -1  &-1  \\
            -1  &1
          \end{mat}
          =
          \frac{1}{-2}
          \begin{mat}[r]
            -6  &0   \\
             0  &2
          \end{mat}
        \end{equation*}
      \partsitem As above,
        \begin{multline*}
           \begin{mat}
              a  &b  \\
              c  &d
           \end{mat}
           \begin{mat}
              x  &y  \\
              y  &z
           \end{mat}
           \cdot\frac{1}{ad-bc}\cdot
           \begin{mat}
              d  &-b \\
             -c  &a
           \end{mat}                               \\
           =\frac{1}{ad-bc}
           \begin{mat}
              adx+bdy-acy-bcz    &-abx-b^2y+a^2y+abz \\
              cdx+d^2y-c^2y-cdz  &-bcx-bdy+acy+adz
           \end{mat}
        \end{multline*}
        we are looking for scalars \( a,\ldots,d \) so that
        \( ad-bc\neq 0 \) and
        \( -abx-b^2y+a^2y+abz=0 \)
        and \( cdx+d^2y-c^2y-cdz=0 \), no matter what values
        \( x \), \( y \), and \( z \) have.

        For starters, we assume that \( y\neq 0 \), else the given matrix is
        already diagonal.
        We shall use that assumption because if we (arbitrarily) let
        \( a=1 \) then we get
        \begin{align*}
           -bx-b^2y+y+bz
           &=0              \\
           (-y)b^2+(z-x)b+y
           &=0
        \end{align*}
        and the quadratic formula gives
        \begin{equation*}
           b=\frac{-(z-x)\pm\sqrt{(z-x)^2-4(-y)(y)} }{-2y}
           \qquad
           y\neq 0
        \end{equation*}
        (note that if \( x \), \( y \), and \( z \) are real then these two
         \( b \)'s are real as the discriminant is positive).
        By the same token, if we (arbitrarily) let \( c=1 \) then
        \begin{align*}
           dx+d^2y-y-dz
           &=0              \\
           (y)d^2+(x-z)d-y
           &=0
        \end{align*}
        and we get here
        \begin{equation*}
           d=\frac{-(x-z)\pm\sqrt{(x-z)^2-4(y)(-y)} }{2y}
           \qquad
           y\neq 0
        \end{equation*}
        (as above, if \( x,y,z\in\Re \) then this discriminant is positive
        so a symmetric, real, \( \nbyn{2} \) matrix is similar to a real
        diagonal matrix).

        For a check we try \( x=1 \), \( y=2 \), \( z=1 \).
        \begin{equation*}
           b=\frac{0\pm\sqrt{0+16} }{-4}=\mp 1
           \qquad
           d=\frac{0\pm\sqrt{0+16} }{4}=\pm 1
        \end{equation*}
        Note that not all four choices \( (b,d)=(+1,+1),\dots,(-1,-1) \)
        satisfy \( ad-bc\neq 0 \).
      \end{exparts} 
    \end{answer}
\index{diagonalizable|)}
\end{exercises}































\subsection{Eigenvalues and Eigenvectors}
We will next focus on the
property of \nearbylemma{lm:DiagIffBasisOfEigens}.

\begin{definition} \label{def:Eigen}
%<*df:Eigen>
A transformation \( \map{t}{V}{V} \) has a scalar
\definend{eigenvalue}\index{eigenvalue, eigenvector!of a transformation}%
\index{transformation!eigenvalue, eigenvector}
\( \lambda \)
if there is a nonzero \definend{eigenvector} \( \vec{\zeta}\in V \)
such that
$
  t(\vec{\zeta})=\lambda\cdot\vec{\zeta}
$.
%</df:Eigen>
\end{definition}

\noindent (``Eigen'' is German for ``characteristic of'' or ``peculiar to.'' 
Some authors call these \definend{characteristic}%
\index{characteristic!vectors, values} values and vectors.
No authors call them ``peculiar.'')

\begin{example}
The projection map
\begin{equation*}
  \colvec{x \\ y \\ z}
     \mapsunder{\pi}
  \colvec{x \\ y \\ 0}
   \qquad x,y,z\in\C
\end{equation*}
has an eigenvalue of \( 1 \) associated with any eigenvector
\begin{equation*}
   \colvec{x \\ y \\ 0}
\end{equation*}
where \( x \) and \( y \) are scalars that are not both zero.

In contrast, a number that is not an eigenvalue of of this map is \( 2 \),
since assuming that $\pi$ doubles a vector leads to 
the equations $x=2x$, $y=2y$, and $0=2z$, and thus 
no non-$\zero$ vector is doubled.
\end{example}

Note that
the definition requires that the eigenvector be non-$\zero$.
Some authors allow $\zero$ as
an eigenvector for $\lambda$ as long as there are also
non-$\zero$ vectors associated with $\lambda$.
The key point is 
to disallow the trivial case where $\lambda$ is such that
$t(\vec{v})=\lambda\vec{v}$ for only the single vector $\vec{v}=\zero$.

Also, note that the eigenvalue $\lambda$ could be~$0$.
The issue is whether $\vec{\zeta}$ equals $\zero$.  

\begin{example}  \label{ex:NoEigenOnTrivSp}
The only transformation on the trivial space \( \set{\zero} \) is
%\begin{equation*}
$\zero\mapsto\zero$.
%\end{equation*}
This map has no eigenvalues because there are no non-\( \zero \) vectors
$\vec{v}$ mapped to a scalar multiple $\lambda\cdot\vec{v}$ of themselves.
\end{example}        

\begin{example} \label{ex:TransPolyOne}
Consider the homomorphism \( \map{t}{\polyspace_1}{\polyspace_1} \)
given by \( c_0+c_1x\mapsto(c_0+c_1)+(c_0+c_1)x \).
While the codomain $\polyspace_1$ of $t$ is two-dimensional, its 
range is one-dimensional $\rangespace{t}=\set{c+cx\suchthat c\in\C}$.
Application of
\( t \) to a vector in that range will simply rescale the vector
\( c+cx\mapsto (2c)+(2c)x \).
That is, \( t \) has an eigenvalue of \( 2 \) associated with eigenvectors of
the form \( c+cx \) where \( c\neq 0 \).

This map also has an eigenvalue of \( 0 \) associated with eigenvectors of
the form \( c-cx \) where \( c\neq 0 \).
\end{example}

The definition above is for maps. 
We can give a matrix version.

\begin{definition}  \label{df:EigenOfMatrix}
%<*df:EigenOfMatrix>
A square matrix \( T \) has a scalar
\definend{eigenvalue}\index{eigenvalue, eigenvector!of a matrix}
\( \lambda \) associated with the nonzero
\definend{eigenvector} \( \vec{\zeta} \) if
\( T\vec{\zeta}=\lambda\cdot\vec{\zeta} \).
%</df:EigenOfMatrix>
\end{definition}

This extension of the definition for maps to a definition
for matrices is natural but there is a point on which we must take care.
The eigenvalues of a map are also the eigenvalues of matrices r
epresenting
that map, and so similar matrices have the same eigenvalues.
However, the eigenvectors can 
differ\Dash similar matrices need not have the 
same eigenvectors.
The next example explains.

\begin{example}
These matrices are similar
\begin{equation*}
  T=
  \begin{mat}
    2  &0  \\
    0  &0
  \end{mat}
  \quad
  \hat{T}
  =
  \begin{mat}[r]
    4  &-2  \\
    4  &-2
  \end{mat}
\end{equation*}
since $\hat{T}=PTP^{-1}$ for this $P$.
\begin{equation*}
  P=
  \begin{mat}
    1  &1  \\
    1  &2
  \end{mat}
  \quad
  P^{-1}=
  \begin{mat}[r]
    2   &-1  \\
    -1  &1
  \end{mat}
\end{equation*}
The matrix $T$
has two eigenvalues, $\lambda_1=2$ and~$\lambda_2=0$.
The first one is associated with this eigenvector.
\begin{equation*}
  T\vec{e}_1=
  \begin{mat}
    2  &0  \\
    0  &0
  \end{mat}
  \colvec{1 \\ 0}
  =\colvec{2 \\ 0}
  =2\vec{e}_1
\end{equation*}
Suppose that $T$ represents a transformation $\map{t}{\C^2}{\C^2}$
with respect to the standard basis.
Then the action of this transformation $t$ is simple.
\begin{equation*}
  \colvec{x \\ y}\mapsunder{t}\colvec{2x \\ 0}
\end{equation*}
Of course, $\hat{T}$ represents the same transformation but with 
respect to a different basis~$B$.
We can easily find this basis.
The arrow diagram
\begin{equation*}
  \begin{CD}
    V_{\wrt{\stdbasis_3}}            @>t>T>        V_{\wrt{\stdbasis_3}}       \\
    @V{\scriptstyle\identity} VV              @V{\scriptstyle\identity} VV \\
    V_{\wrt{B}}                   @>t>\hat{T}>        V_{\wrt{B}}       
  \end{CD}
\end{equation*}
shows that $P^{-1}=\rep{\identity}{B,\stdbasis_3}$.
By the definition of the matrix representation of a map, its first column is 
$\rep{\identity(\vec{\beta}_1)}{\stdbasis_3}=\rep{\vec{\beta}_1}{\stdbasis_3}$.
With respect to the standard basis any vector is represented by itself, 
so the first basis element $\vec{\beta}_1$ is the first column of $P^{-1}$.
The same goes for the other one.
\begin{equation*}
  B=\sequence{\colvec[r]{2 \\ -1},
              \colvec[r]{-1 \\ 1}
              }
\end{equation*}
Since the matrices $T$ and~$\hat{T}$ both represent the transformation~$t$,
both reflect the action $t(\vec{e}_1)=2\vec{e}_1$.
\begin{align*}
  &\rep{t}{\stdbasis_2,\stdbasis_2}\cdot\rep{\vec{e}_1}{\stdbasis_2}
    =T\cdot\rep{\vec{e}_1}{\stdbasis_2}                
    =2\cdot\rep{\vec{e}_1}{\stdbasis_2}                  \\               
    &\rep{t}{B,B}\cdot\rep{\vec{e}_1}{B}
    =\hat{T}\cdot\rep{\vec{e}_1}{B}                                 
    =2\cdot\rep{\vec{e}_1}{B}  
\end{align*}
But while in those two equations the eigenvalue $2$'s are the same, the
vector representations differ.
\begin{align*}
    T\cdot\rep{\vec{e}_1}{\stdbasis_2}
    =T\colvec{1 \\ 0}
    &=2\cdot\colvec{1 \\ 0}                \\
    \hat{T}\cdot\rep{\vec{e}_1}{B}  
    =\hat{T}\cdot\colvec{1 \\ 1}
    &=2\cdot\colvec{1 \\ 1}
\end{align*}
That is, when the matrix representing the transformation is
\( T=\rep{t}{\stdbasis_2,\stdbasis_2} \) then it ``assumes'' that 
column vectors are 
representations with respect to \( \stdbasis_2 \).
However \( \hat{T}=\rep{t}{B,B} \) ``assumes'' that column vectors 
are representations with respect to \( B \), and
so the column vectors that get doubled 
are different.
\end{example}


% \begin{example}
% Consider again the transformation \( \map{t}{\polyspace_1}{\polyspace_1} \) 
% % from \nearbyexample{ex:TransPolyOne} 
% given by
% \( c_0+c_1x\mapsto (c_0+c_1)+(c_0+c_1)x \).
% One of its eigenvalues is \( 2 \), associated with the eigenvectors
% \( c+cx \) where \( c\neq 0 \).
% If we represent \( t \) with respect to \( B=\sequence{1+1x,1-1x} \)
% \begin{equation*}
%    T=\rep{t}{B,B}=
%    \begin{mat}[r]
%       2  &0  \\
%       0  &0
%    \end{mat}
% \end{equation*}
% then \( 2 \) is an eigenvalue of the matrix \( T \), 
% associated with these eigenvectors.
% \begin{equation*}
%    \set{\colvec{c_0 \\ c_1}\suchthat \begin{mat}[r]
%                                          2  &0  \\
%                                          0  &0
%                                       \end{mat}\colvec{c_0 \\ c_1}
%                                       =\colvec{2c_0 \\ 2c_1}  }
%   =\set{\colvec{c_0 \\ 0}\suchthat c_0\in\C,\, c_0\neq 0 }
% \end{equation*}
% On the other hand, if we represent $t$ with respect to
% \( D=\sequence{2+1x,1+0x} \)  
% \begin{equation*}
%    S=\rep{t}{D,D}=
%    \begin{mat}[r]
%       3  &1  \\
%      -3  &-1
%    \end{mat}
% \end{equation*}
% then the eigenvectors associated with the eigenvalue \( 2 \) are
% these.
% \begin{equation*}
%    \set{\colvec{c_0 \\ c_1}\suchthat \begin{mat}[r]
%                                          3  &1  \\
%                                         -3  &-1
%                                       \end{mat}\colvec{c_0 \\ c_1}
%                                       =\colvec{2c_0 \\ 2c_1}  }
%   =\set{\colvec{0 \\ c_1}\suchthat c_1\in\C,\, c_1\neq 0 }
% \end{equation*}
% \end{example}


We next see the basic tool for
finding eigenvectors and eigenvalues.

\begin{example}  \label{ex:IntroCharEqn}
If
\begin{equation*}
  T=
  \begin{mat}[r]
     1    &2    &1    \\
     2    &0    &-2   \\
    -1    &2    &3
  \end{mat}
\end{equation*}
then to find the scalars \( x \) such that
\( T\vec{\zeta}=x\vec{\zeta} \) for nonzero eigenvectors
\( \vec{\zeta} \), bring everything to the left-hand side
\begin{equation*}
  \begin{mat}[r]
     1    &2    &1    \\
     2    &0    &-2   \\
    -1    &2    &3
  \end{mat}
  \colvec{z_1 \\ z_2 \\ z_3}
  -x\colvec{z_1 \\ z_2 \\ z_3}
  =\zero
\end{equation*}
and factor
\( (T-x I)\vec{\zeta}=\zero \).
(Note that it says $T-xI$. 
The expression \( T-x \) doesn't make sense 
because \( T \) is a matrix while \( x \) is a scalar.)
This homogeneous linear system
\begin{equation*}
  \begin{mat}
   1-x           &2            &1            \\
     2           &0-x          &-2           \\
    -1           &2            &3-x
  \end{mat}
  \colvec{z_1 \\ z_2 \\ z_3}
  =
  \colvec[r]{0 \\ 0 \\ 0}
\end{equation*}
has a nonzero solution $\vec{z}$ if and only if the
matrix is singular.
We can determine when that happens.
\begin{align*}
  0
  &=\deter{T-x I}                                               \\
  &=\begin{vmatrix}
     1-x          &2            &1            \\
     2           &0-x          &-2           \\
    -1           &2            &3-x
   \end{vmatrix}                                       \\
  &=x^3-4x^2+4x  \\
  &=x(x-2)^2
\end{align*}
The eigenvalues are \( \lambda_1=0 \) and \( \lambda_2=2 \).
To find the associated eigenvectors plug in each eigenvalue.
Plugging in $\lambda_1=0$ gives
\begin{equation*}
  \begin{mat}
     1-0         &2            &1            \\
     2           &0-0          &-2           \\
    -1           &2            &3-0
  \end{mat}
  \colvec{z_1 \\ z_2 \\ z_3}
  =
  \colvec[r]{0 \\ 0 \\ 0}
  \quad\Longrightarrow\quad
  \colvec{z_1 \\ z_2 \\ z_3}
  =
  \colvec[r]{a \\ -a \\ a}
\end{equation*}
for \( a\neq 0 \)
(\( a \) must be non-$0$ because eigenvectors are defined to
be non-\( \zero \)).
Plugging in $\lambda_2=2$ gives 
\begin{equation*}
  \begin{mat}
     1-2         &2            &1            \\
     2           &0-2          &-2           \\
    -1           &2            &3-2
  \end{mat}
  \colvec{z_1 \\ z_2 \\ z_3}
  =
  \colvec[r]{0 \\ 0 \\ 0}
  \quad\Longrightarrow\quad
  \colvec{z_1 \\ z_2 \\ z_3}
  =
  \colvec{b \\ 0 \\ b}
\end{equation*}
with \( b\neq 0 \).
\end{example}

\begin{example} \label{ex:AnotherCharPoly}
If
\begin{equation*}
  S=
  \begin{mat}[r]
    \pi      &1      \\
    0        &3
  \end{mat}
\end{equation*}
(here \( \pi \) is not a projection map, it is the number
\( 3.14\ldots \)) then
\begin{equation*}
    \begin{vmat}
      \pi-x &1         \\
      0     &3-x
    \end{vmat} 
  =
  (x-\pi)(x-3)
\end{equation*}
so \( S \) has eigenvalues of \( \lambda_1=\pi \) and \( \lambda_2=3 \).
To find associated eigenvectors, first plug in $\lambda_1$ for $x$
\begin{equation*}
  \begin{mat}
    \pi-\pi     &1         \\
    0           &3-\pi
  \end{mat}
  \colvec{z_1 \\ z_2}
  =
  \colvec[r]{0 \\ 0}
  \qquad\Longrightarrow\qquad
  \colvec{z_1 \\ z_2}
  =
  \colvec{a \\ 0}
\end{equation*}
for a scalar \( a\neq 0 \).
Then plug in $\lambda_2$
\begin{equation*}
  \begin{mat}
    \pi-3       &1         \\
    0           &3-3
  \end{mat}
  \colvec{z_1 \\ z_2}
  =
  \colvec[r]{0 \\ 0}
  \qquad\Longrightarrow\qquad
  \colvec{z_1 \\ z_2}
  =
  \colvec{-b/(\pi-3) \\ b}
\end{equation*}
where \( b\neq 0 \).
\end{example}

\begin{definition} \label{df:CharacteristicPoly}
%<*df:CharacteristicPoly>
The \definend{characteristic polynomial of a square matrix}\index{characteristic!polynomial}%
\index{matrix!characteristic polynomial}
\( T \) is the
determinant \( \deter{T-x I} \) where \( x \) is a variable.
The \definend{characteristic equation}\index{characteristic!equation}%
\index{matrix!characteristic polynomial}
is $\deter{T-xI}=0$.
The \definend{characteristic polynomial of a transformation}
\( t \) is the characteristic polynomial
of any matrix representation
\( \rep{t}{B,B} \).\index{transformation!characteristic polynomial}
%</df:CharacteristicPoly>
\end{definition}

\noindent 
%<*df:CharacteristicPolyExer>
\nearbyexercise{exer:CharPolyTransWellDefed} checks that the 
characteristic polynomial of a transformation is 
well-defined, that is, that the characteristic polynomial is the same
no matter which basis we use for the representation.
%</df:CharacteristicPolyExer>

\begin{lemma} \label{le:MapNonTrivSpHasEigen}
%<*lm:MapNonTrivSpHasEigen>
A linear transformation on a nontrivial vector space has at least one
eigenvalue.
%</lm:MapNonTrivSpHasEigen>
\end{lemma}

\begin{proof}
%<*pf:MapNonTrivSpHasEigen>
Any root of the characteristic polynomial is an eigenvalue.
Over the complex numbers, any polynomial of degree one or greater
has a root.
%</pf:MapNonTrivSpHasEigen>
\end{proof}

\begin{remark}
That result is the reason that in this chapter 
we use scalars that are complex numbers.
\end{remark}

% Notice the familiar form of the sets of eigenvectors in the above examples.

\begin{definition} \label{df:Eigenspace}
%<*df:Eigenspace>
The \definend{eigenspace of a transformation~$t$ associated 
with the 
eigenvalue~$\lambda$}\index{eigenspace}\index{transformation!eigenspace}
is
$
  V_{\lambda}=\set{\vec{\zeta}\suchthat t(\vec{\zeta}\,)=\lambda\vec{\zeta}\,}
              % \union\set{\zero\,}
$.
The eigenspace of a matrix is analogous.
%</df:Eigenspace>
\end{definition}

% \begin{remark}
% In \nearbydefinition{def:Eigen} we specified that eigenvectors must be
% non-$\zero$.
% So, strictly speaking, the eigenspace for $\lambda$ contains the set of
% associated eigenvectors along with $\zero$.
% We need the zero vector for the next result. 
% \end{remark}

\begin{lemma}  \label{le:EigSpaceIsSubSp}
%<*lm:EigSpaceIsSubSp>
An eigenspace is a subspace.
%</lm:EigSpaceIsSubSp>
\end{lemma}

\begin{proof}
%<*pf:EigSpaceIsSubSp>
Fix an eigenvalue~$\lambda$.
Notice first that
$V_{\lambda}$ contains the zero
vector since $t(\zero)=\zero$, which equals
$\lambda\zero$.
So the eigenspace is a nonempty subset of the space. 
What remains is to check closure of this set under linear combinations.
Take \( \vec{\zeta}_1,\ldots,\vec{\zeta}_n\in V_{\lambda} \) and then verify
\begin{align*}
  t(\lincombo{c}{\vec{\zeta}})
  &=c_1t(\vec{\zeta}_1)+\dots+c_nt(\vec{\zeta}_n)               \\
  &=c_1\lambda\vec{\zeta}_1+\dots+c_n\lambda\vec{\zeta}_n          \\
  &=\lambda(c_1\vec{\zeta}_1+\dots+c_n\vec{\zeta}_n)
\end{align*}
that the combination is also an element of  $V_{\lambda}$.
% (despite that the zero vector isn't an eigenvector, 
% the second equality holds even if some \( \vec{\zeta}_i \) is \( \zero \) since
% \( t(\zero)=\lambda\cdot\zero=\zero \)).
%</pf:EigSpaceIsSubSp>
\end{proof}

\begin{example}
In \nearbyexample{ex:IntroCharEqn}
these are the eigenspaces associated with the eigenvalues \( 0 \) 
and \( 2 \).
\begin{equation*}
  V_0=\set{\colvec[r]{a \\ -a \\ a}\suchthat a\in\C},
  \qquad
  V_2=\set{\colvec{b \\ 0 \\ b}\suchthat b\in\C}.
\end{equation*}
\end{example}

\begin{example}
In \nearbyexample{ex:AnotherCharPoly} these are the eigenspaces 
associated with the eigenvalues \( \pi \) 
and~\( 3 \).
\begin{equation*}
  V_{\pi}=\set{\colvec{a \\ 0}\suchthat a\in\C}
  \qquad
  V_3=\set{\colvec{-b/(\pi-3) \\ b}\suchthat b\in\C}
\end{equation*}
\end{example}

The characteristic equation 
in \nearbyexample{ex:IntroCharEqn}
is \( 0=x(x-2)^2 \) so in some sense
\( 2 \) is an eigenvalue twice.
However there are not twice as many eigenvectors in that the dimension
of the associated eigenspace $V_2$ is one, not two.
The next example is a case where a number is a double root of
the characteristic equation and the dimension of the associated eigenspace
is two.

\begin{example}
With respect to the standard bases, this matrix
\begin{equation*}
  \begin{mat}[r]
     1  &0  &0  \\
     0  &1  &0  \\
     0  &0  &0
  \end{mat}
\end{equation*}
represents projection.
\begin{equation*}
  \colvec{x \\ y \\ z}
     \mapsunder{\pi}
  \colvec{x \\ y \\ 0}
   \qquad x,y,z\in\C
\end{equation*}
Its characteristic equation 
\begin{align*}
  0
  &=\deter{T-x I}                                     \\
  &=\begin{vmatrix}
     1-x          &0            &0            \\
     0           &1-x          &0           \\
     0           &0            &0-x
   \end{vmatrix}                                       \\
  &=(1-x)^2(0-x)
\end{align*}
has the double root~$x=1$ along with the single root~$x=0$.
Its eigenspace associated with the eigenvalue \( 0 \) and
its eigenspace associated with the eigenvalue \( 1 \)
are easy to find.
\begin{equation*}
   V_0=\set{\colvec{0 \\ 0 \\ c_3}\suchthat c_3\in\C}
   \qquad
   V_1=\set{\colvec{c_1 \\ c_2 \\ 0}\suchthat c_1,c_2\in\C}
\end{equation*}
Note that $V_1$ has dimension two.
\end{example}

By \nearbylemma{le:EigSpaceIsSubSp} 
if two eigenvectors $\vec{v}_1$ and $\vec{v}_2$ are 
associated with the same eigenvalue then a linear combination of those
two is also an eigenvector, associated with the same eigenvalue.
As an illustration, referring to the prior example, 
this sum of two members of $V_1$
\begin{equation*}
  \colvec[r]{1 \\ 0 \\ 0}+\colvec[r]{0 \\ 1 \\ 0}
\end{equation*}
yields another member of $V_1$.

The next result speaks to the situation where the vectors come from
different eigenspaces.

\begin{theorem}  \label{th:DistEValueGivesLIEvecs}
%<*th:DistEValueGivesLIEvecs>
For any set of distinct eigenvalues of a map or matrix, a set of associated
eigenvectors, one per eigenvalue, is linearly independent.
%</th:DistEValueGivesLIEvecs>
\end{theorem}

\begin{proof}
%<*pf:DistEValueGivesLIEvecs0>
We will use induction on the number of eigenvalues.
The base step is that there are zero eigenvalues.
Then the set of associated vectors is empty
and so is linearly independent.
% If there is 
% only one eigenvalue then the set of associated eigenvectors is
% a singleton set with a non-$\zero$ member, 
% and so is linearly independent.
%</pf:DistEValueGivesLIEvecs0>

%<*pf:DistEValueGivesLIEvecs1>
For the inductive step assume that the statement is true for any set 
of \( k\geq 0 \) distinct eigenvalues. 
Consider distinct eigenvalues 
\( \lambda_1,\dots,\lambda_{k+1} \)
and let \( \vec{v}_1,\dots,\vec{v}_{k+1} \)
be associated eigenvectors.
Suppose that
$\zero=c_1\vec{v}_1+\dots+c_k\vec{v}_k+c_{k+1}\vec{v}_{k+1}$. 
Derive two equations from that, the first by multiplying by \( \lambda_{k+1} \) 
on both sides 
$\zero=c_1\lambda_{k+1}\vec{v}_1+\dots+c_{k+1}\lambda_{k+1}\vec{v}_{k+1}$
and the second by applying the map to both sides
$\zero=c_1t(\vec{v}_1)+\dots+c_{k+1}t(\vec{v}_{k+1})
  =c_1\lambda_1\vec{v}_1+\dots+c_{k+1}\lambda_{k+1}\vec{v}_{k+1}$
(applying the matrix gives the same result). 
Subtract the second from the first.
\begin{equation*}
  \zero=
  c_1(\lambda_{k+1}-\lambda_1)\vec{v}_1+\dots
  +c_k(\lambda_{k+1}-\lambda_k)\vec{v}_k
      +c_{k+1}(\lambda_{k+1}-\lambda_{k+1})\vec{v}_{k+1}
\end{equation*}
The $\vec{v}_{k+1}$ term vanishes.
Then the induction hypothesis gives that
\( c_1(\lambda_{k+1}-\lambda_1)=0 \), \ldots, \( c_k(\lambda_{k+1}-\lambda_k)=0 \).
The eigenvalues are distinct so 
the coefficients \( c_1,\,\dots,\,c_k \) are all~\( 0 \).
With that
we are left with the equation \( \zero=c_{k+1}\vec{v}_{k+1} \)
so \( c_{k+1} \) is also~\( 0 \).
%</pf:DistEValueGivesLIEvecs1>
\end{proof}

\begin{example}
The eigenvalues of
\begin{equation*}
     \begin{mat}[r]
        2   &-2   &2   \\
        0   &1    &1   \\
       -4   &8    &3
     \end{mat}
\end{equation*}
are distinct: \( \lambda_1=1 \), \( \lambda_2=2 \), and~\( \lambda_3=3 \).
A set of associated eigenvectors 
\begin{equation*}
  \set{
       \colvec[r]{2 \\ 1 \\ 0},
       \colvec[r]{9 \\ 4 \\ 4},
       \colvec[r]{2 \\ 1 \\ 2}  }
\end{equation*}
is linearly independent.
\end{example}

\begin{corollary} \label{co:DistinctEivenvaluesImpliesDiagonal}
%<*co:DistinctEivenvaluesImpliesDiagonal>
An \( \nbyn{n} \) matrix with \( n \) distinct eigenvalues is diagonalizable.
%</co:DistinctEivenvaluesImpliesDiagonal>
\end{corollary}

\begin{proof}
%<*pf:DistinctEivenvaluesImpliesDiagonal>
Form a basis of eigenvectors.
Apply \nearbylemma{lm:DiagIffBasisOfEigens}.
%</pf:DistinctEivenvaluesImpliesDiagonal>
\end{proof}

This section observes that some
matrices are similar to a diagonal matrix. 
The idea of eigenvalues arose as the entries of that diagonal matrix,
although the definition applies more broadly than just to 
diagonalizable matrices.  
To find eigenvalues we defined the characteristic equation and
that led to the final result, a 
criteria for diagonalizability.
(While it is useful for the theory, note that 
in applications finding eigenvalues this way
is typically impractical; for one thing the matrix may be large and
finding roots of large-degree polynomials is hard.) 

In the next section we study matrices that cannot be diagonalized. 


\begin{exercises}
  \item 
    For each, find the characteristic polynomial and the eigenvalues.
    \begin{exparts*}
      \partsitem \( \begin{mat}[r]
                 10  &-9 \\
                  4  &-2
            \end{mat}  \)
      \partsitem $\begin{mat}[r]
                    1  &2  \\
                    4  &3  
                 \end{mat}$
      \partsitem \( \begin{mat}[r]
                  0  &3  \\
                  7 &0
                 \end{mat} \)
      \partsitem \( \begin{mat}[r]  
                  0  &0  \\
                  0  &0
            \end{mat}  \)
      \partsitem \( \begin{mat}[r]
                  1  &0  \\
                  0  &1
            \end{mat}  \)
    \end{exparts*}
    \begin{answer}
      \begin{exparts}
         \partsitem This
           \begin{equation*}
             0=
             \begin{vmatrix}
               10-x  &-9  \\
               4     &-2-x
             \end{vmatrix}
             =(10-x)(-2-x)-(-36)
           \end{equation*}
           simplifies to the characteristic equation \( x^2-8x+16=0 \). 
           Because the equation factors into $(x-4)^2$ there is
           only one eigenvalue \( \lambda_1=4 \).
         \partsitem $0=(1-x)(3-x)-8=x^2-4x-5$; $\lambda_1=5$, $\lambda_2=-1$
         \partsitem \( x^2-21=0 \); 
           \( \lambda_1=\sqrt{21} \), $\lambda_2=-\sqrt{21}$
         \partsitem \( x^2=0 \); \( \lambda_1=0 \)
         \partsitem \( x^2-2x+1=0 \); \( \lambda_1=1 \)
       \end{exparts}  
     \end{answer}
  \recommended \item
    For each matrix, find the characteristic equation, and the
    eigenvalues and associated eigenvectors.
    \begin{exparts}
      \partsitem \( \begin{mat}[r]
                  3  &0  \\
                  8  &-1
            \end{mat}  \)
      \partsitem \( \begin{mat}[r]
                  3  &2  \\
                 -1  &0
            \end{mat}  \)
    \end{exparts}
    \begin{answer}
       \begin{exparts}
         \partsitem The characteristic equation is \( (3-x)(-1-x)=0 \).
           Its roots, the eigenvalues, are \( \lambda_1=3 \) and 
           \( \lambda_2=-1 \).
           For the eigenvectors we consider this equation.
           \begin{equation*}
             \begin{mat}
               3-x  &0    \\
               8    &-1-x
             \end{mat}
             \colvec{b_1  \\  b_2}
             =\colvec[r]{0  \\  0}
           \end{equation*}
           For the eigenvector associated with $\lambda_1=3$,
           we consider the resulting linear system.
           \begin{equation*}
             \begin{linsys}{2}
               0\cdot b_1  &+  &0\cdot b_2  &=  &0  \\
               8\cdot b_1  &+  &-4\cdot b_2 &=  &0
             \end{linsys}
           \end{equation*}
           The eigenspace is the set of vectors whose second component is 
           twice the first component.
           \begin{equation*}
             \set{\colvec{b_2/2 \\ b_2}\suchthat b_2\in\C}
             \qquad
             \begin{mat}[r]
               3  &0  \\
               8  &-1
             \end{mat}
             \colvec{b_2/2  \\ b_2}
             =3\cdot\colvec{b_2/2 \\ b_2}
           \end{equation*}
           (Here, the parameter is $b_2$ only because that is the variable that
           is free in the above system.)
           Hence, this is an eigenvector associated with the eigenvalue $3$.
           \begin{equation*}
             \colvec[r]{1 \\ 2}
           \end{equation*}

           Finding an eigenvector associated with $\lambda_2=-1$ is similar.
           This system
           \begin{equation*}
             \begin{linsys}{2}
               4\cdot b_1  &+  &0\cdot b_2  &=  &0  \\
               8\cdot b_1  &+  &0\cdot b_2 &=  &0
             \end{linsys}
           \end{equation*}
           leads to the set of vectors whose first component is 
           zero.
           \begin{equation*}
             \set{\colvec{0 \\ b_2}\suchthat b_2\in\C}
             \qquad
             \begin{mat}[r]
               3  &0  \\
               8  &-1
             \end{mat}
             \colvec{0  \\ b_2}
             =-1\cdot\colvec{0 \\ b_2}
           \end{equation*}
           And so this is an eigenvector associated with $\lambda_2$.
           \begin{equation*}
             \colvec[r]{0 \\ 1}
           \end{equation*}
         \partsitem The characteristic equation is
           \begin{equation*}
             0=
             \begin{vmatrix}
               3-x  &2  \\
               -1   &-x               
             \end{vmatrix}
             =x^2-3x+2=(x-2)(x-1)
           \end{equation*}
           and so the eigenvalues are $\lambda_1=2$ and $\lambda_2=1$.
           To find eigenvectors, consider this system.
           \begin{equation*}
             \begin{linsys}{2}
               (3-x)\cdot b_1  &+  &2\cdot b_2  &=  &0  \\
               -1\cdot b_1     &-  &x\cdot b_2  &=  &0
             \end{linsys}
           \end{equation*}
           For $\lambda_1=2$ we get 
           \begin{equation*}
             \begin{linsys}{2}
                1\cdot b_1  &+  &2\cdot b_2  &=  &0  \\
               -1\cdot b_1  &-  &2\cdot b_2  &=  &0
             \end{linsys}
           \end{equation*}
           leading to this eigenspace and eigenvector.
           \begin{equation*}
             \set{\colvec{-2b_2 \\ b_2}
                   \suchthat b_2\in\C}
             \qquad
             \colvec[r]{-2 \\ 1}
           \end{equation*}
           For $\lambda_2=1$ the system is 
           \begin{equation*}
             \begin{linsys}{2}
                2\cdot b_1  &+  &2\cdot b_2  &=  &0  \\
               -1\cdot b_1  &-  &1\cdot b_2  &=  &0
             \end{linsys}
           \end{equation*}
           leading to this.
           \begin{equation*}
             \set{\colvec{-b_2 \\ b_2}
                   \suchthat b_2\in\C}
             \qquad
             \colvec[r]{-1 \\ 1}
           \end{equation*}
       \end{exparts}  
     \end{answer}
  \item
    Find the characteristic equation, and the
    eigenvalues and associated eigenvectors for this matrix.
    \textit{Hint.}
      The eigenvalues are complex.
    \begin{equation*}
      \begin{mat}[r]
         -2  &-1 \\
          5  &2
      \end{mat}  
    \end{equation*}
    \begin{answer}
         The characteristic equation 
           \begin{equation*}
             0=
             \begin{vmatrix}
               -2-x  &-1  \\
               5     &2-x               
             \end{vmatrix}
             =x^2+1
           \end{equation*}
           has the complex roots $\lambda_1=i$ and $\lambda_2=-i$.
           This system 
           \begin{equation*}
             \begin{linsys}{2}
               (-2-x)\cdot b_1  &-  &1\cdot b_2      &=  &0  \\
               5\cdot b_1       &   &(2-x)\cdot b_2  &=  &0
             \end{linsys}
           \end{equation*}
           For $\lambda_1=i$ Gauss's Method gives this reduction.
           \begin{equation*}
             \begin{linsys}{2}
                (-2-i)\cdot b_1  &-  &1\cdot b_2      &=  &0  \\
                5\cdot b_1       &-  &(2-i)\cdot b_2  &=  &0
             \end{linsys}
             \grstep{(-5/(-2-i))\rho_1+\rho_2}
             \begin{linsys}{2}
                (-2-i)\cdot b_1  &-  &1\cdot b_2      &=  &0  \\
                                 &   &0               &=  &0
             \end{linsys}
           \end{equation*}
           (For the calculation in the lower right get a common
           denominator
           \begin{equation*}
             \frac{5}{-2-i}-(2-i)
             =
             \frac{5}{-2-i}-\frac{-2-i}{-2-i}\cdot (2-i)
             =
             \frac{5-(-5)}{-2-i}
           \end{equation*}
           to see that it gives a $0=0$ equation.)
           These are the resulting eigenspace and  eigenvector.
           \begin{equation*}
             \set{\colvec{(1/(-2-i))b_2 \\ b_2}
                   \suchthat b_2\in\C}
             \qquad
             \colvec{1/(-2-i) \\ 1}
           \end{equation*}
           For $\lambda_2=-i$ the system 
           \begin{equation*}
             \begin{linsys}{2}
                (-2+i)\cdot b_1  &-  &1\cdot b_2      &=  &0  \\
                5\cdot b_1       &-  &(2+i)\cdot b_2  &=  &0
             \end{linsys}
             \grstep{(-5/(-2+i))\rho_1+\rho_2}
             \begin{linsys}{2}
                (-2+i)\cdot b_1  &-  &1\cdot b_2      &=  &0  \\
                                 &   &0               &=  &0
             \end{linsys}
           \end{equation*}
           leads to this.
           \begin{equation*}
             \set{\colvec{(1/(-2+i))b_2 \\ b_2}
                   \suchthat b_2\in\C}
             \qquad
             \colvec{1/(-2+i) \\ 1}
           \end{equation*}
    \end{answer}
  \item  
    Find the characteristic polynomial, the eigenvalues, and the associated
    eigenvectors of this matrix.
    \begin{equation*}
      \begin{mat}[r]
        1  &1  &1  \\
        0  &0  &1  \\
        0  &0  &1
      \end{mat}
    \end{equation*}
    \begin{answer}
      The characteristic equation is
      \begin{equation*}
        0=
        \begin{vmatrix}
          1-x  &1   &1   \\
          0    &-x  &1   \\
          0    &0   &1-x
        \end{vmatrix}
        =(1-x)^2(-x)
      \end{equation*}
      and so the eigenvalues are $\lambda_1=1$ (this is a repeated root
      of the equation) and $\lambda_2=0$.
      For the rest, consider this system.
      \begin{equation*}
        \begin{linsys}{3}
          (1-x)\cdot b_1  &+  &b_2         &+  &b_3            &=  &0  \\
                          &   &-x\cdot b_2 &+  &b_3            &=  &0  \\
                          &   &            &   &(1-x)\cdot b_3 &= &0  
        \end{linsys}
      \end{equation*}
      When $x=\lambda_1=1$ then the solution set is this eigenspace.
      \begin{equation*}
        \set{\colvec{b_1 \\ 0 \\ 0}\suchthat b_1\in\C}
      \end{equation*}
      When $x=\lambda_2=0$ then the solution set is this eigenspace.
      \begin{equation*}
        \set{\colvec{-b_2 \\ b_2 \\ 0}\suchthat b_2\in\C}
      \end{equation*}
      So these are eigenvectors associated with $\lambda_1=1$ and 
      $\lambda_2=0$.
      \begin{equation*}
        \colvec[r]{1 \\ 0 \\ 0}
        \qquad
        \colvec[r]{-1 \\ 1 \\ 0}  
      \end{equation*}
    \end{answer}
  \recommended \item
    For each matrix, find the characteristic equation, and the
    eigenvalues and associated eigenvectors.
    \begin{exparts*}
      \partsitem \( \begin{mat}[r]
              3  &-2 &0  \\
             -2  &3  &0  \\
              0  &0  &5
            \end{mat}  \)
      \partsitem \( \begin{mat}[r]
              0  &1   &0  \\
              0  &0   &1  \\
              4  &-17 &8
            \end{mat}  \)
    \end{exparts*}
    \begin{answer}
      \begin{exparts}
         \partsitem The characteristic equation is
           \begin{equation*}
             0=
             \begin{vmatrix}
              3-x  &-2   &0  \\
             -2    &3-x  &0  \\
              0    &0    &5-x
             \end{vmatrix}
             =x^3-11x^2+35x-25=(x-1)(x-5)^2
           \end{equation*}
           and so the eigenvalues are $\lambda_1=1$ and also the
           repeated eigenvalue $\lambda_2=5$.
           To find eigenvectors, consider this system.
           \begin{equation*}
             \begin{linsys}{3}
               (3-x)\cdot b_1  &-  &2\cdot b_2      &   &   &=  &0  \\
               -2\cdot b_1     &+  &(3-x)\cdot b_2  &   &   &=  &0  \\
                               &   &                &   &(5-x)\cdot b_3 &= &0 
             \end{linsys}
           \end{equation*}
           For $\lambda_1=1$ we get 
           \begin{equation*}
             \begin{linsys}{3}
                2\cdot b_1     &-  &2\cdot b_2   &   &   &=  &0  \\
               -2\cdot b_1     &+  &2\cdot b_2   &   &   &=  &0  \\
                               &   &             &   &4\cdot b_3 &= &0 
             \end{linsys}
           \end{equation*}
           leading to this eigenspace and eigenvector.
           \begin{equation*}
             \set{\colvec{b_2 \\ b_2 \\ 0}
                   \suchthat b_2\in\C}
             \qquad
             \colvec[r]{1 \\ 1 \\ 0}
           \end{equation*}
           For $\lambda_2=5$ the system is 
           \begin{equation*}
             \begin{linsys}{3}
                -2\cdot b_1  &-  &2\cdot b_2   &   &   &=  &0  \\
               -2\cdot b_1   &-  &2\cdot b_2   &   &   &=  &0  \\
                             &   &             &   &0\cdot b_3 &= &0 
             \end{linsys}
           \end{equation*}
           leading to this.
           \begin{equation*}
             \set{\colvec{-b_2 \\ b_2 \\ 0}+\colvec{0 \\ 0 \\ b_3}
                   \suchthat b_2,b_3\in\C}
             \qquad
             \colvec[r]{-1 \\ 1 \\ 0},\,\colvec[r]{0 \\ 0 \\ 1}
           \end{equation*}
         \partsitem The characteristic equation is
           \begin{equation*}
             0=
             \begin{vmatrix}
              -x   &1    &0  \\
              0    &-x   &1  \\
              4    &-17  &8-x
             \end{vmatrix}
             =-x^3+8x^2-17x+4=-1\cdot(x-4)(x^2-4x+1)
           \end{equation*}
           and the eigenvalues are $\lambda_1=4$ and (by using the
           quadratic equation) $\lambda_2=2+\sqrt{3}$ and 
           $\lambda_3=2-\sqrt{3}$.
           To find eigenvectors, consider this system.
           \begin{equation*}
             \begin{linsys}{3}
               -x\cdot b_1  &+  &b_2          &   &               &=  &0  \\
                            &   &-x\cdot b_2  &+  &b_3            &=  &0  \\
               4\cdot b_1   &-  &17\cdot b_2  &+  &(8-x)\cdot b_3 &= &0 
             \end{linsys}
           \end{equation*}
           Substituting $x=\lambda_1=4$ gives the system 
           \begin{align*}
             \begin{linsys}{3}
               -4\cdot b_1  &+  &b_2          &   &           &= &0  \\
                            &   &-4\cdot b_2  &+  &b_3        &= &0  \\
               4\cdot b_1   &-  &17\cdot b_2  &+  &4\cdot b_3 &= &0 
             \end{linsys}                                              
             &\grstep{\rho_1+\rho_3}
             \begin{linsys}{3}
               -4\cdot b_1  &+  &b_2          &   &           &= &0  \\
                            &   &-4\cdot b_2  &+  &b_3        &= &0  \\
                            &   &-16\cdot b_2  &+ &4\cdot b_3 &= &0 
             \end{linsys}                                                \\
             &\grstep{-4\rho_2+\rho_3}
             \begin{linsys}{3}
               -4\cdot b_1  &+  &b_2          &   &           &= &0  \\
                            &   &-4\cdot b_2  &+  &b_3        &= &0  \\
                            &   &              &  &0          &= &0 
             \end{linsys}
           \end{align*}
           leading to this eigenspace and eigenvector.
           \begin{equation*}
             V_4=\set{\colvec{(1/16)\cdot b_3 \\ (1/4)\cdot b_3 \\ b_3}
                   \suchthat b_2\in\C}
             \qquad
             \colvec[r]{1 \\ 4 \\ 16}
           \end{equation*}

           Substituting $x=\lambda_2=2+\sqrt{3}$ gives the system 
           \begin{multline*}
             \begin{linsys}{3}
               (-2-\sqrt{3})\cdot b_1  
                      &+  &b_2          
                          &   &           &= &0  \\
                      &   &(-2-\sqrt{3})\cdot b_2  
                          &+  &b_3        &= &0  \\
               4\cdot b_1   
                      &-  &17\cdot b_2  
                          &+  &(6-\sqrt{3})\cdot b_3 &= &0 
             \end{linsys}                                   \\
             \grstep{(-4/(-2-\sqrt{3}))\rho_1+\rho_3}
             \begin{linsys}{3}
               (-2-\sqrt{3})\cdot b_1  
                      &+  &b_2          
                          &   &           &= &0  \\
                      &   &(-2-\sqrt{3})\cdot b_2  
                          &+  &b_3        &= &0  \\
                      &+  &(-9-4\sqrt{3})\cdot b_2  
                          &+  &(6-\sqrt{3})\cdot b_3 &= &0 
             \end{linsys}
           \end{multline*}
           (the middle coefficient in the third equation equals
           the number $(-4/(-2-\sqrt{3}))-17$; find a common denominator
           of $-2-\sqrt{3}$ and then rationalize the denominator by
           multiplying the top and bottom of the fraction by $-2+\sqrt{3}$)
           \begin{equation*}
             \grstep{((9+4\sqrt{3})/(-2-\sqrt{3}))\rho_2+\rho_3}
             \begin{linsys}{3}
               (-2-\sqrt{3})\cdot b_1  
                      &+  &b_2          
                          &   &           &= &0  \\
                      &   &(-2-\sqrt{3})\cdot b_2  
                          &+  &b_3        &= &0  \\
                      &   &                         
                          &   &0                     &= &0 
             \end{linsys}
           \end{equation*}
           which leads to this eigenspace and eigenvector.
           \begin{equation*}
             V_{2+\sqrt{3}}
             =\set{\colvec{(1/(2+\sqrt{3})^2)\cdot b_3  \\ 
                           (1/(2+\sqrt{3}))\cdot b_3    \\ 
                           b_3}
                    \suchthat b_3\in\C}
             \qquad
             \colvec{(1/(2+\sqrt{3})^2)  \\ 
                           (1/(2+\sqrt{3}))  \\ 
                           1}
           \end{equation*}

           Finally, substituting $x=\lambda_3=2-\sqrt{3}$ gives the system 
           \begin{multline*}
             \begin{linsys}{3}
               (-2+\sqrt{3})\cdot b_1  
                      &+  &b_2          
                          &   &           &= &0  \\
                      &   &(-2+\sqrt{3})\cdot b_2  
                          &+  &b_3        &= &0  \\
               4\cdot b_1   
                      &-  &17\cdot b_2  
                          &+  &(6+\sqrt{3})\cdot b_3 &= &0 
             \end{linsys}                                       \\
             \begin{aligned}
               &\grstep{(-4/(-2+\sqrt{3}))\rho_1+\rho_3}
               \begin{linsys}{3}
                 (-2+\sqrt{3})\cdot b_1  
                        &+  &b_2          
                            &   &           &= &0  \\
                        &   &(-2+\sqrt{3})\cdot b_2  
                            &+  &b_3        &= &0  \\
                        &   &(-9+4\sqrt{3})\cdot b_2  
                            &+  &(6+\sqrt{3})\cdot b_3 &= &0 
               \end{linsys}                                       \\
               &\grstep{((9-4\sqrt{3})/(-2+\sqrt{3}))\rho_2+\rho_3}
               \begin{linsys}{3}
                 (-2+\sqrt{3})\cdot b_1  
                        &+  &b_2          
                            &   &           &= &0  \\
                        &   &(-2+\sqrt{3})\cdot b_2  
                            &+  &b_3        &= &0  \\
                        &   &                         
                            &   &0                     &= &0 
               \end{linsys}
             \end{aligned}
           \end{multline*}
           which gives this eigenspace and eigenvector.
           \begin{equation*}
             V_{2-\sqrt{3}}
             =\set{\colvec{(1/(2+\sqrt{3})^2)\cdot b_3  \\ 
                           (1/(2-\sqrt{3}))\cdot b_3    \\ 
                           b_3}
                    \suchthat b_3\in\C}
             \qquad
             \colvec{(1/(-2+\sqrt{3})^2)  \\ 
                           (1/(-2+\sqrt{3}))  \\ 
                           1}
           \end{equation*}
      \end{exparts}
    \end{answer}
   \recommended \item
     Let \( \map{t}{\polyspace_2}{\polyspace_2} \) be
     \begin{equation*}
      a_0+a_1x+a_2x^2\mapsto
      (5a_0+6a_1+2a_2)-(a_1+8a_2)x+(a_0-2a_2)x^2.
     \end{equation*}
    Find its eigenvalues and the associated eigenvectors.
    \begin{answer}
      With respect to the natural basis $B=\sequence{1,x,x^2}$ 
      the matrix representation is this.
      \begin{equation*}
        \rep{t}{B,B}
        =
        \begin{mat}[r]
          5  &6  &2  \\
          0  &-1 &-8 \\
          1  &0  &-2 
        \end{mat}
      \end{equation*}
      Thus the characteristic equation 
      \begin{equation*}
        0
        =
        \begin{mat}
          5-x  &6    &2  \\
          0    &-1-x &-8 \\
          1    &0    &-2-x 
        \end{mat}
        =(5-x)(-1-x)(-2-x)-48-2\cdot(-1-x)
      \end{equation*}
      is $0=-x^3+2x^2+15x-36=-1\cdot (x+4)(x-3)^2$.
      To find the associated eigenvectors, consider this system.
      \begin{equation*}
        \begin{linsys}{3}
          (5-x)\cdot b_1 &+ &6b_2      &+ &2b_3      &= &0 \\
                         &  &(-1-x)\cdot b_2 &- &8b_3      &= &0 \\
          b_1            &  &                &+ &(-2-x)\cdot b_3 &= &0 
        \end{linsys}
      \end{equation*}
      Plugging in $\lambda_1=-4$ for $x$ gives
      \begin{equation*}
        \begin{linsys}{3}
                    9b_1 &+ &6b_2      &+ &2b_3      &= &0 \\
                         &  &3b_2      &- &8b_3      &= &0 \\
                     b_1 &  &          &+ &2b_3     &= &0 
        \end{linsys}
        \grstep{-(1/9)\rho_1+\rho_3}\quad
        \grstep{(2/9)\rho_2+\rho_3}\quad
        \begin{linsys}{3}
                     9b_1 &+ &6b_2      &+ &2b_3      &= &0 \\
                         &  &3b_2       &- &8b_3      &= &0  
        \end{linsys}
      \end{equation*}
      Here is the eigenspace and an eigenvector.
           \begin{equation*}
             V_{-4}
             =\set{\colvec{2\cdot b_3  \\ 
                           (8/3)\cdot b_3    \\ 
                           b_3}
                    \suchthat b_3\in\C}
             \qquad
             \colvec{2  \\ 
                     8/3  \\ 
                      1}
           \end{equation*}

      Similarly, plugging in $x=\lambda_2=3$ gives
      \begin{equation*}
        \begin{linsys}{3}
                    2b_1 &+ &6\cdot b_2      &+ &2\cdot b_3      &= &0 \\
                         &  &-4\cdot b_2      &- &8\cdot b_3      &= &0 \\
                     b_1 &  &                &- & 5\cdot b_3     &= &0 
        \end{linsys}
        \grstep{-(1/2)\rho_1+\rho_3}
        \repeatedgrstep{-(3/4)\rho_2+\rho_3}
        \begin{linsys}{3}
                     2b_1 &+ &6\cdot b_2      &+ &2\cdot b_3      &= &0 \\
                         &  &-4\cdot b_2       &- &8\cdot b_3      &= &0  
        \end{linsys}
      \end{equation*}
      with this eigenspace and eigenvector.
           \begin{equation*}
             V_{3}
             =\set{\colvec{5\cdot b_3  \\ 
                           -2\cdot b_3    \\ 
                           b_3}
                    \suchthat b_3\in\C}
             \qquad
             \colvec[r]{5  \\ 
                     -2  \\ 
                       1}
           \end{equation*}
    \end{answer}
   \item 
     Find the eigenvalues and eigenvectors of this
     map \( \map{t}{\matspace_2}{\matspace_2} \).
     \begin{equation*}
         \begin{mat}
            a  &b  \\
            c  &d
         \end{mat}
       \mapsto
         \begin{mat}
           2c    &a+c  \\
           b-2c  &d
         \end{mat}
     \end{equation*}
     \begin{answer}
        $\lambda=1,
          \begin{mat}
                   0  &0  \\
                   0  &1
          \end{mat} \text{ and }
          \begin{mat}[r]
                   2  &3  \\
                   1  &0
          \end{mat}$,           
          $\lambda=-2,
          \begin{mat}[r]
                  -1  &0  \\
                   1  &0
          \end{mat}$,
          $\lambda=-1,
          \begin{mat}[r]
                  -2  &1  \\
                   1  &0
          \end{mat}$  
       \end{answer}
   \recommended \item 
     Find the eigenvalues and associated eigenvectors of the
     differentiation operator
     \( \map{d/dx}{\polyspace_3}{\polyspace_3} \).
     \begin{answer}
       Fix the natural basis $B=\sequence{1,x,x^2,x^3}$. 
       The map's action is $1\mapsto 0$, $x\mapsto 1$, $x^2\mapsto 2x$,
       and $x^3\mapsto 3x^2$ and its representation is easy to compute.
       \begin{equation*}
         T=\rep{d/dx}{B,B}=
         \begin{mat}[r]
           0  &1  &0  &0  \\
           0  &0  &2  &0  \\
           0  &0  &0  &3  \\
           0  &0  &0  &0
         \end{mat}_{B,B}
       \end{equation*}
       We find the eigenvalues with this computation.
       \begin{equation*}
         0=\deter{T-xI}=
         \begin{vmatrix}
           -x &1  &0  &0  \\
           0  &-x &2  &0  \\
           0  &0  &-x &3  \\
           0  &0  &0  &-x          
         \end{vmatrix}
         =x^4
       \end{equation*}
       Thus the map has the single eigenvalue $\lambda=0$.
       To find the associated eigenvectors, we solve
       \begin{equation*}
         \begin{mat}[r]
           0  &1  &0  &0  \\
           0  &0  &2  &0  \\
           0  &0  &0  &3  \\
           0  &0  &0  &0
         \end{mat}_{B,B}
         \colvec{b_1 \\ b_2 \\ b_3 \\ b_4}_B
         =0\cdot\colvec{b_1 \\ b_2 \\ b_3 \\ b_4}_B
         \qquad\Longrightarrow\qquad
         \text{$b_2=0$, $b_3=0$, $b_4=0$}          
       \end{equation*}
       to get this eigenspace.
       \begin{equation*}
         \set{\colvec{b_1 \\ 0 \\ 0 \\ 0}_B
               \suchthat b_1\in\C}
         =\set{b_1+0\cdot x+0\cdot x^2+0\cdot x^3
               \suchthat b_1\in\C}
         =\set{b_1
               \suchthat b_1\in\C}
       \end{equation*}
      \end{answer}
   \item Prove that 
     the eigenvalues of a triangular matrix  (upper or lower triangular)
     are the entries on the diagonal.
     \begin{answer}
       The determinant of the triangular matrix $T-xI$ is the product 
       down the diagonal, and so it factors into the product of 
       the terms $t_{i,i}-x$.
     \end{answer}
  \recommended \item 
    Find the formula for the characteristic polynomial of a $\nbyn{2}$
    matrix.
    \begin{answer}
      Just expand the determinant of $T-xI$.
      \begin{equation*}
        \begin{vmatrix}
          a-x  &c  \\
          b    &d-x
        \end{vmatrix}
        =(a-x)(d-x)-bc
        =x^2+(-a-d)\cdot x +(ad-bc)
      \end{equation*}
    \end{answer}
  \item \label{exer:CharPolyTransWellDefed}
    Prove that 
    the characteristic polynomial of a transformation is well-defined.
    \begin{answer}
      Any two representations of that transformation are similar, and
      similar matrices have the same characteristic polynomial.  
    \end{answer}
  \item Prove or disprove: if all the eigenvalues of a matrix are $0$ 
    then it must be the zero matrix.
    \begin{answer}
      It is not true.
      All of the eigenvalues of this matrix are $0$.
      \begin{equation*}
        \begin{mat}[r]
          0  &1  \\
          0  &0
        \end{mat}
      \end{equation*}
    \end{answer}
  \recommended \item 
    \begin{exparts}
      \partsitem Show that any non-\( \zero \) vector in any nontrivial 
        vector space can be a eigenvector.
        That is, given a \( \vec{v}\neq\zero \) from a nontrivial \( V \),
        show that there is a transformation \( \map{t}{V}{V} \) having a scalar
        eigenvalue \( \lambda\in\Re \) such that \( \vec{v}\in V_{\lambda} \).
      \partsitem What if we are given a scalar \( \lambda \)?
        Can any non-\( \zero \) member of any
        nontrivial vector space be an eigenvector associated with \( \lambda \)?
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem Use \( \lambda=1 \) and the identity map.
        \partsitem Yes, use the transformation that multiplies all 
          vectors by the scalar \( \lambda \).
      \end{exparts}  
     \end{answer}
  \recommended \item 
    Suppose that \( \map{t}{V}{V} \) and \( T=\rep{t}{B,B} \).
    Prove that the eigenvectors of \( T \) associated with \( \lambda \) are
    the non-\( \zero \) vectors in the kernel of the map represented
    (with respect to the same bases) by \( T-\lambda I \).
    \begin{answer}
      If $t(\vec{v})=\lambda\cdot\vec{v}$ then 
      $\vec{v}\mapsto\zero$ under the map $t-\lambda\cdot\identity$.
    \end{answer}
  \item 
    Prove that if $a,\ldots,\,d$ are all integers and \( a+b=c+d \) then
    \begin{equation*}
      \begin{mat}
         a  &b  \\
         c  &d
      \end{mat}
    \end{equation*}
    has integral eigenvalues, namely \( a+b \) and \( a-c \).
    \begin{answer}
      The characteristic equation 
      \begin{equation*}
        0=
        \begin{vmatrix}
          a-x  &b  \\
          c    &d-x 
        \end{vmatrix}
        =(a-x)(d-x)-bc
      \end{equation*}
      simplifies to $x^2+(-a-d)\cdot x + (ad-bc)$.
      Checking that the values $x=a+b$ and $x=a-c$ satisfy the equation 
      (under the $a+b=c+d$ condition) is routine. 
    \end{answer}
  \recommended \item
    Prove that if \( T \) is nonsingular and has eigenvalues
    \( \lambda_1,\dots,\lambda_n \) then \( T^{-1} \) has eigenvalues
    \( 1/\lambda_1,\dots,1/\lambda_n \).
    Is the converse true?
    \begin{answer}
      Consider an eigenspace $V_{\lambda}$.
      Any $\vec{w}\in V_{\lambda}$ is the image
      $\vec{w}=\lambda\cdot\vec{v}$ of some $\vec{v}\in V_{\lambda}$ 
      (namely, $\vec{v}=(1/\lambda)\cdot\vec{w}$).
      Thus, on $V_{\lambda}$ (which is a nontrivial subspace) 
      the action of $t^{-1}$ is 
      $t^{-1}(\vec{w})=\vec{v}=(1/\lambda)\cdot\vec{w}$,
      and so $1/\lambda$ is an eigenvalue of $t^{-1}$.
    \end{answer}
  \recommended \item
    Suppose that \( T \) is \( \nbyn{n} \) and \( c,d \) are scalars.
    \begin{exparts}
      \partsitem Prove that if \( T \) has the eigenvalue 
        \( \lambda \) with an associated
        eigenvector \( \vec{v} \) then \( \vec{v} \) is an eigenvector of
        \( cT+dI \) associated with eigenvalue \( c\lambda+d \).
      \partsitem Prove that if \( T \) is diagonalizable then so is
        \( cT+dI \).
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem We have 
          $(cT+dI)\vec{v}=cT\vec{v}+dI\vec{v}=c\lambda\vec{v}+d\vec{v}
               =(c\lambda+d)\cdot \vec{v}$.
        \partsitem Suppose that $S=PTP^{-1}$ is diagonal.
          Then $P(cT+dI)P^{-1}=P(cT)P^{-1}+P(dI)P^{-1}
                 =cPTP^{-1}+dI=cS+dI$ is also diagonal.
      \end{exparts}
    \end{answer}
  \recommended \item
    Show that \( \lambda \) is an eigenvalue of \( T \) if and only if the map
    represented by \( T-\lambda I \) is not an isomorphism.
    \begin{answer}
      The scalar $\lambda$ is an eigenvalue if and only if the transformation
      $t-\lambda \identity$ is singular.
      A transformation is singular if and only if it is not an isomorphism
      (that is, a transformation is an isomorphism if and only if it is
      nonsingular).
    \end{answer}
  \item \cite{Strang} 
    \begin{exparts}
      \partsitem Show that if \( \lambda \) is an eigenvalue of \( A \)
         then \( \lambda^k \) is an eigenvalue of \( A^k \).
      \partsitem What is wrong with this proof generalizing that?
         ``If \( \lambda \) is an eigenvalue of \( A \) and \( \mu \) is
         an eigenvalue for \( B \), then \( \lambda\mu \) is an eigenvalue
         for \( AB \), for, if \( A\vec{x}=\lambda\vec{x} \) and
         \( B\vec{x}=\mu\vec{x} \) then
         \( AB\vec{x}=A\mu\vec{x}=\mu A\vec{x}=\mu\lambda\vec{x} \)''?
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem Where the eigenvalue $\lambda$ is associated with the
          eigenvector $\vec{x}$ then
          $A^k\vec{x}=A\cdots A\vec{x}=A^{k-1}\lambda\vec{x}
            =\lambda A^{k-1}\vec{x}=\cdots=\lambda^k\vec{x}$.
          (The full details require induction on $k$.)
        \partsitem The eigenvector associated with $\lambda$
          might not be an eigenvector associated with $\mu$.
      \end{exparts}
    \end{answer}
  \item 
    Do matrix equivalent matrices have the same eigenvalues?
    \begin{answer}
      No.
      These are two same-sized, equal rank, matrices
      with different eigenvalues.
      \begin{equation*}
        \begin{mat}[r]
          1  &0  \\
          0  &1
        \end{mat}
        \qquad
        \begin{mat}[r]
          1  &0  \\
          0  &2
        \end{mat}
      \end{equation*}
    \end{answer}
  \item 
    Show that a square matrix with real entries and an odd number of rows
    has at least one real eigenvalue.
    \begin{answer}
      The characteristic polynomial has an odd power and so 
      has at least one real root.  
    \end{answer}
  \item 
    Diagonalize.
    \begin{equation*}
       \begin{mat}[r]
         -1  &2  &2  \\
          2  &2  &2  \\
         -3  &-6 &-6
       \end{mat}
    \end{equation*}
    \begin{answer}
      The characteristic polynomial $x^3+5x^2+6x$ has distinct roots
      \( \lambda_1=0 \), \( \lambda_2=-2 \), and \( \lambda_3=-3 \).
      Thus the matrix can be diagonalized into this form.
      \begin{equation*}
         \begin{mat}[r]
            0  &0  &0  \\
            0  &-2 &0  \\
            0  &0  &-3
         \end{mat}
      \end{equation*}    
    \end{answer}
  \item 
    Suppose that \( P \) is a nonsingular \( \nbyn{n} \) matrix.
    Show that 
    the \definend{similarity transformation}\index{similarity transformation}
    map \( \map{t_P}{\matspace_{\nbyn{n}}}{\matspace_{\nbyn{n}}} \)
    sending \( T\mapsto PTP^{-1} \)
    is an isomorphism.
    \begin{answer}
      We must show that it is one-to-one and onto, and that it respects the
      operations of matrix addition and scalar multiplication.

      To show that it is one-to-one, suppose that $t_P(T)=t_P(S)$,
      that is, suppose that $PTP^{-1}=PSP^{-1}$, and note that multiplying
      both sides on the left by $P^{-1}$ and on the right by $P$ gives that
      $T=S$.
      To show that it is onto, consider $S\in\matspace_{\nbyn{n}}$ and observe
      that $S=t_P(P^{-1}SP)$.

      The map $t_P$ preserves matrix addition since
      $t_P(T+S)=P(T+S)P^{-1}=(PT+PS)P^{-1}=PTP^{-1}+PSP^{-1}=t_P(T+S)$
      follows from properties of matrix multiplication and addition that 
      we have seen.
      Scalar multiplication is 
      similar:~$t_P(cT)=P(c\cdot T)P^{-1}=c\cdot (PTP^{-1})=c\cdot t_P(T)$.
    \end{answer}
  \puzzle \item 
    \cite{MathMag67p232}
    Show that if \( A \) is an \( n \) square matrix and each row (column)
    sums to \( c \) then \( c \) is a characteristic root of \( A \).
    (``Characteristic root'' is a synonym for eigenvalue.)\index{characteristic!root}\index{root!characteristic}
    \begin{answer}
      \answerasgiven %
      If the argument of the characteristic function of \( A \) is set equal to
      \( c \), adding the first \( (n-1) \) rows (columns) to the
      \( n \)th row (column) yields a determinant whose \( n \)th row
      (column) is zero.
      Thus \( c \) is a characteristic root of \( A \).  
    \end{answer}
\index{similarity|)}
\end{exercises}
